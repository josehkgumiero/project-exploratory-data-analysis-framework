{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331296d8-3667-45ca-a9e9-de528034e812",
   "metadata": {},
   "source": [
    "# Requirements Gathering\n",
    "* two group of people\n",
    "    * domain expert\n",
    "    * product owner\n",
    "* use case to resolve\n",
    "* Requirements\n",
    "    * Story and Sprints\n",
    "        * agile methodology\n",
    "            * development of software lifecycle \n",
    "* Data discussion\n",
    "    * Databases\n",
    "    * Cloud\n",
    "    * third party APIs\n",
    "* Big Data Engieenring\n",
    "* Start the lifecycle of data science project\n",
    "# Data Analysis\n",
    "* Definition\n",
    "    * Inspecting, cleaning & transforming data\n",
    "    * Insights, trends and relatioship\n",
    "    * Statical e mathematical techniques\n",
    "* Used for\n",
    "    * Solve problem\n",
    "    * Improve\n",
    "    * Maintain status quo\n",
    "* Lifecycle\n",
    "    * understand the background\n",
    "    * analyzethe situation/opportunity\n",
    "    * solve problem / identify improvement opportunities\n",
    "    * story telling\n",
    "* Four types\n",
    "    * descriptive analysis(what happend?) example: what is my weight gain?\n",
    "    * diagnostic anaysis (why it happened?) example: why my did i gain weight suddenly last week?\n",
    "    * predictive analysis (what will happen?) example: based on my weight what will be my weight in 6 months?\n",
    "    * prescriptive analysis (what solution?) example: wat changes sould i make maintain or reduce my weight?\n",
    "* Statical\n",
    "    * definition: statics is the science of collecting, organizing nd analyzing data.\n",
    "    * data definition: facts or pieces of information\n",
    "    * statical analysis\n",
    "\n",
    "* Types of statistics\n",
    "    * descriptive statistics\n",
    "        * involves meths for summarizing and organizing ataa to make it understandable. This type of statistics helps to describe the basic features of the data in a study.\n",
    "            * Measure of central tendency: ean or averge, median, mode\n",
    "            * Measure of dispersion: range, variance standard deviation, interquartile range (IQR)\n",
    "            * (Box plot)\n",
    "            * histogram and skewness\n",
    "            * covariance\n",
    "            * correlation\n",
    "    * Inferential statistics\n",
    "        * invoves methods for making predictios or inferences about a population based n a sample of data. It allow or hypothesis testing estiation, and drawina conclusions\n",
    "        * Hypothesis: testing mechanism\n",
    "            * Null hipothesis\n",
    "            * alternate hypothesis\n",
    "            * experiments -> statistical analysis\n",
    "            * Accept the null hyptesis or reject the null hypothesis\n",
    "            * P-Value\n",
    "            * \n",
    "        * hypothesis testing (parametric)\n",
    "            * test for means\n",
    "                * paired t test\n",
    "                * independent t-test\n",
    "                * one way anova\n",
    "                * to way anova\n",
    "                * repeated measures anova\n",
    "                * t-test\n",
    "            * test for association\n",
    "                * pearson correlation\n",
    "                * linea regression\n",
    "            * test for distribution\n",
    "                * chi-square\n",
    "                * z-test\n",
    "        * hyothesis testing (non-parametric)\n",
    "            * teste for median\n",
    "                * wilcoxon signed rank test\n",
    "                * mann-whitney u test\n",
    "                * kruskal wallis test\n",
    "                * mood's median test\n",
    "                * friendman's test\n",
    "            * test for association\n",
    "                * spearman's rank correlation\n",
    "                * kendall's tau\n",
    "                * chi-square test\n",
    "            * test for distribution comparison\n",
    "                * kolmogorov-smirnov test\n",
    "                * sign test\n",
    "        * Baye's theorem\n",
    "* Tests\n",
    "    * Parametric tests\n",
    "        * Require assuptions about the distribution of the data\n",
    "        * typically require inerval or ratio data\n",
    "        * Often effective with larger sample sizes, providing more pwoerful and precise estimates\n",
    "        * t-tests, anova, linear regression\n",
    "        * sensitive to the means of the data distributions\n",
    "        * less robut to outliers and non-normality\n",
    "        * best suited for testing hypotheses about means or variances\n",
    "        * focus on estimating parameters of the distribution\n",
    "    * Non-parametric tests\n",
    "        * fewer assumptions about he distribution of the data\n",
    "        * can be used with ordinal, nominal, or interval/ratio data\n",
    "        * can be used with smaller sample sizes\n",
    "        * chi-squar test, mann-whitneyu test, kruskal-wallis test, spearman's rank correltion\n",
    "        * sensitive to the medians or ranks of he data distribution\n",
    "        * more robust to outlier and non-normal distributions\n",
    "        * useful for ordinal data or when the assumptions for parametric ests are not met\n",
    "        * focus on the median or rank-based methods of inferance\n",
    "* Probability Distribution Functions (random variables)\n",
    "    * Probability mass functions (discrete variable)\n",
    "    * probability density functions (continuous variable)\n",
    "    * Types\n",
    "        * bernouli distribution\n",
    "        * binominal distribution\n",
    "        * normal/gaussian distribution\n",
    "        * poisson distribution\n",
    "        * standard normal distribution\n",
    "        * uniform distribution\n",
    "        * log normal distribution\n",
    "        * power-law distribution\n",
    "        * pareto distribution\n",
    "        * central lmit theorem\n",
    "        * estimates\n",
    "* Population\n",
    "    * a population is the entire set of individuals or objects of interest in a particular study. It includes all members of a defined group that we are studying or collecing information on.\n",
    "* Sample data\n",
    "    * A sample is a subset of the population that is used t represent the entire group. Sampling involves selecting a group of individuas or obsrvsaions from the population to draw conclusions aboou the whole population\n",
    "* Probability sampling\n",
    "    * simple random sampling\n",
    "    * sywtematic sampling\n",
    "    * stratified sampling\n",
    "    * cluster sampling\n",
    "    * multi stage sampling\n",
    "* Non Probabiity Sampling\n",
    "    * convenient sampling\n",
    "    * judgental sampling\n",
    "    * snowball sampling\n",
    "    * quota sampling\n",
    "* Scales of measurement of data\n",
    "    * nominal scale: do not have order\n",
    "    * ordinal scale: have ranked or order\n",
    "    * interval scale\n",
    "    * ratio\n",
    "# Exploratory Data Analysis\n",
    "* Derive maximum value out of data\n",
    "*  Concept of dependent vs independent variables\n",
    "    * dependent: weight\n",
    "    * independent: caories, genetics medicatios, stress, poor sleep, hormonal imbalance\n",
    "* Data (values)\n",
    "    * Qualitative\n",
    "        * Discrete\n",
    "            * number of products prodcued\n",
    "        * Continuous\n",
    "            * height, length, area\n",
    "    * Quantitative\n",
    "        * Nominal\n",
    "        * Ordinal\n",
    "        * Binary\n",
    "* Transformation\n",
    "    * Feature Engineering\n",
    "        * mixed conveted data type\n",
    "        * null values (missing data)\n",
    "            * Missing Completely at Random\n",
    "            * Missing at Random\n",
    "            * Missing data not at random\n",
    "            * imput mean value\n",
    "            * input median value for outliers\n",
    "            * imput mode for categorical value\n",
    "        * Encoding values\n",
    "            * Nominal/OHE Encoding\n",
    "            * Label Enconding\n",
    "            * Ordinal Encoding\n",
    "            * Target Guided Ordinal Encoding\n",
    "        * outliers\n",
    "        * multi collinearity\n",
    "        * imbalaced dataset\n",
    "            * SMOTE\n",
    "            * Other techniques\n",
    "        * data scaling\n",
    "        * duplicate values\n",
    "#  Machine Learning\n",
    "## Model\n",
    "- regression\n",
    "    - linear\n",
    "    - polynomial\n",
    "    - OLS > use one variabale independent to fiit model > find slope\n",
    "    - Logistic\n",
    "- time series forecasting\n",
    "    - ARIMA\n",
    "- Unsupervised\n",
    "    - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02295982-2cc0-48f9-9282-8b1c85e3691e",
   "metadata": {},
   "source": [
    "# Project\n",
    "- ##### Core Dependencies\n",
    "- ##### DataFrame ingestions\n",
    "    - ##### Dataset Inventory Engine\n",
    "    - ##### Dataset Inventory Validation\n",
    "    - ##### Test Execution – Dataset Inventory\n",
    "    - ##### Dataset Inventory Execution\n",
    "    - ##### Dataset Loading Engine\n",
    "    - ##### Dataset Loading Validation\n",
    "    - ##### Test Execution - Dataset Loading Engine\n",
    "    - ##### Dataset Loading Execution\n",
    "    - ##### Data Preview Engine\n",
    "    - ##### Data Preview Engine Validation\n",
    "    - ##### Test Execution - Data Preview Engine\n",
    "    - ##### Data Preview Engine Execution\n",
    "    - ##### Dataset Registry Engine\n",
    "    - ##### Dataset Registry Engine Validation\n",
    "    - ##### Test Execution - Dataset Registry Engine\n",
    "    - ##### Dataset Registry Engine Execution\n",
    "    - ##### Schema Profiling Engine\n",
    "    - ##### Schema Profiling Engine Validation\n",
    "    - ##### Test Execution - Schema Profiling Engine\n",
    "    - ##### Schema Profiling Engine Execution\n",
    "    - ##### Column-Level Description for Each DataFrame\n",
    "    - ##### Dataset Shape Inspection Engine\n",
    "    - ##### Dataset Shape Inspection Engine Validation\n",
    "    - ##### Test Execution - Dataset Shape Inspection Engine\n",
    "    - ##### Dataset Shape Inspection Engine Execution\n",
    "- ##### DataFrame Transformations\n",
    "    - ##### Mixed Data Type\n",
    "        - ##### Date Column Profiling Engine\n",
    "        - ##### Date Column Profiling Engine Validation\n",
    "        - ##### Test Execution - Date Column Profiling Engine\n",
    "        - ##### Date Column Profiling Engine Execution\n",
    "        - ##### Date Type Converter Engine\n",
    "        - ##### Date Type Converter Engine Validation\n",
    "        - ##### Test Execution - Date Type Converter Engine\n",
    "        - ##### Date Type Converter Engine Execution\n",
    "        - ##### Cost Column Profiling Engine\n",
    "        - ##### Cost Column Profiling Engine Validation\n",
    "        - ##### Test Execution - Cost Column Profiling Engine\n",
    "        - ##### Cost Column Profiling Engine Execution\n",
    "        - ##### Cost Type Converter Engine\n",
    "        - ##### Cost Type Converter Engine Validation\n",
    "        - ##### Test Execution - Cost Type Converter Engine\n",
    "    - ##### Duplicate Data\n",
    "        - ##### Cost Type Converter Engine Execution\n",
    "        - ##### Duplicate Removal Engine\n",
    "        - ##### Duplicate Removal Engine Validation\n",
    "        - ##### Test Execution - Duplicate Removal Engine\n",
    "        - ##### Duplicate Removal Engine Execution\n",
    "    - ##### Missing Data\n",
    "        - #####\n",
    "        - #####\n",
    "        - #####\n",
    "        - #####\n",
    "        - #####\n",
    "        - #####\n",
    "        - #####\n",
    "        - #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafec932-e7f4-4032-b847-f0396c7c9b2d",
   "metadata": {},
   "source": [
    "# Core Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bb23ad-59c2-416c-a1b5-8052440f5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef36fafd-953e-4c37-859d-dc6826cb04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e25d7-0d04-4458-a3e7-994879b3d546",
   "metadata": {},
   "source": [
    "# DataFrame Ingestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21d2c3-7fc5-4547-b22b-3a96eabd8f2d",
   "metadata": {},
   "source": [
    "# Dataset Inventory Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5e6b60-2e58-4fef-8173-67305b3332ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetInventory:\n",
    "    \"\"\"\n",
    "    DatasetInventory\n",
    "\n",
    "    Responsible for:\n",
    "    - Locating project root\n",
    "    - Scanning raw data directory\n",
    "    - Building dataset metadata inventory\n",
    "    - Exporting results to CSV inside data/inventory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder: str = \"data/raw\"):\n",
    "        \"\"\"\n",
    "        Initialize inventory builder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_folder : str\n",
    "            Relative path to the raw data directory.\n",
    "        \"\"\"\n",
    "        self.logger = self._configure_logger()\n",
    "        self.base_dir = self._find_project_root()\n",
    "        self.data_directory = self.base_dir / data_folder\n",
    "\n",
    "        self._validate_directory()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        \"\"\"Configure logging for the class.\"\"\"\n",
    "        logger = logging.getLogger(\"DatasetInventory\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _find_project_root(self) -> Path:\n",
    "        \"\"\"\n",
    "        Automatically locate project root by searching for 'data' folder.\n",
    "        Works in notebooks and scripts.\n",
    "        \"\"\"\n",
    "        current_path = Path().resolve()\n",
    "\n",
    "        while not (current_path / \"data\").exists() and current_path != current_path.parent:\n",
    "            current_path = current_path.parent\n",
    "\n",
    "        self.logger.info(f\"Project root located at: {current_path}\")\n",
    "        return current_path\n",
    "\n",
    "    def _validate_directory(self):\n",
    "        \"\"\"Ensure raw data directory exists.\"\"\"\n",
    "        if not self.data_directory.exists():\n",
    "            self.logger.error(f\"Data directory not found: {self.data_directory}\")\n",
    "            raise FileNotFoundError(\n",
    "                f\"Directory not found: {self.data_directory}\"\n",
    "            )\n",
    "\n",
    "        self.logger.info(f\"Data directory validated: {self.data_directory}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def build_inventory(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scan folder and build dataset inventory.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing metadata of datasets.\n",
    "        \"\"\"\n",
    "        allowed_extensions = [\".csv\", \".xlsx\"]\n",
    "\n",
    "        files = [\n",
    "            file for file in self.data_directory.iterdir()\n",
    "            if file.suffix.lower() in allowed_extensions\n",
    "        ]\n",
    "\n",
    "        self.logger.info(f\"{len(files)} dataset files found.\")\n",
    "\n",
    "        records = []\n",
    "\n",
    "        for file in files:\n",
    "            clean_name = file.stem.split(\".\", 1)[-1]\n",
    "\n",
    "            records.append({\n",
    "                \"dataset_name\": clean_name,\n",
    "                \"file_name\": file.name,\n",
    "                \"extension\": file.suffix.lower(),\n",
    "                \"full_path\": str(file),\n",
    "                \"file_size_kb\": round(file.stat().st_size / 1024, 2)\n",
    "            })\n",
    "\n",
    "        df_inventory = pd.DataFrame(records)\n",
    "\n",
    "        self.logger.info(\"Dataset inventory successfully built.\")\n",
    "        return df_inventory\n",
    "\n",
    "    def export_to_csv(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        output_filename: str = \"dataset_inventory.csv\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Export inventory DataFrame to CSV inside:\n",
    "\n",
    "        project_root/data/inventory/\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Inventory DataFrame.\n",
    "        output_filename : str\n",
    "            Name of the CSV file.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define export folder\n",
    "        export_folder = self.base_dir / \"data\" / \"files\" / \"inventory\"\n",
    "\n",
    "        # Create folder automatically if it does not exist\n",
    "        export_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        export_path = export_folder / output_filename\n",
    "\n",
    "        df.to_csv(export_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"Inventory exported to: {export_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227efbb-7a78-4c0b-a4b1-f31745a8038a",
   "metadata": {},
   "source": [
    "# Dataset Inventory Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6875db-caa1-49b0-9f1f-e9f2ef9e10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inventory_creation():\n",
    "    inventory = DatasetInventory()\n",
    "    df = inventory.build_inventory()\n",
    "\n",
    "    assert isinstance(df, pd.DataFrame), \"Output is not a DataFrame\"\n",
    "    assert not df.empty, \"Inventory DataFrame is empty\"\n",
    "    assert \"dataset_name\" in df.columns, \"Missing expected column\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149af583-4edd-43e3-9b94-202e4bdaa388",
   "metadata": {},
   "source": [
    "# Test Execution – Dataset Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bead7c-e786-4bf4-bd91-971385e9b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:33,837 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:00:33,843 | INFO | Data directory validated: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\raw\n",
      "2026-02-25 15:00:33,845 | INFO | 7 dataset files found.\n",
      "2026-02-25 15:00:33,848 | INFO | Dataset inventory successfully built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_inventory_creation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c449e8-6341-48aa-8708-2a584055641e",
   "metadata": {},
   "source": [
    "# Dataset Inventory Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b259a2-c6d9-47cf-88f2-67145f10f175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:33,865 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:00:33,867 | INFO | Data directory validated: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\raw\n",
      "2026-02-25 15:00:33,870 | INFO | 7 dataset files found.\n",
      "2026-02-25 15:00:33,873 | INFO | Dataset inventory successfully built.\n",
      "2026-02-25 15:00:33,880 | INFO | Inventory exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\inventory\\dataset_inventory.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>file_name</th>\n",
       "      <th>extension</th>\n",
       "      <th>full_path</th>\n",
       "      <th>file_size_kb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_dataset</td>\n",
       "      <td>01.sample_dataset.csv</td>\n",
       "      <td>.csv</td>\n",
       "      <td>C:\\Users\\User\\Desktop\\project-exploratory-data...</td>\n",
       "      <td>97.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sales_dataset</td>\n",
       "      <td>02.sales_dataset.xlsx</td>\n",
       "      <td>.xlsx</td>\n",
       "      <td>C:\\Users\\User\\Desktop\\project-exploratory-data...</td>\n",
       "      <td>84.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>extra_variable_dataset</td>\n",
       "      <td>03.extra_variable_dataset.xlsx</td>\n",
       "      <td>.xlsx</td>\n",
       "      <td>C:\\Users\\User\\Desktop\\project-exploratory-data...</td>\n",
       "      <td>32.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extra_data_dataset</td>\n",
       "      <td>04.extra_data_dataset.xlsx</td>\n",
       "      <td>.xlsx</td>\n",
       "      <td>C:\\Users\\User\\Desktop\\project-exploratory-data...</td>\n",
       "      <td>10.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fact_sales_dataset</td>\n",
       "      <td>05.fact_sales_dataset.csv</td>\n",
       "      <td>.csv</td>\n",
       "      <td>C:\\Users\\User\\Desktop\\project-exploratory-data...</td>\n",
       "      <td>3505.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dim_customers_dataset</td>\n",
       "      <td>06.dim_customers_dataset.csv</td>\n",
       "      <td>.csv</td>\n",
       "      <td>C:\\Users\\User\\Desktop\\project-exploratory-data...</td>\n",
       "      <td>1511.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dim_products_dataset</td>\n",
       "      <td>07.dim_products_dataset.csv</td>\n",
       "      <td>.csv</td>\n",
       "      <td>C:\\Users\\User\\Desktop\\project-exploratory-data...</td>\n",
       "      <td>27.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataset_name                       file_name extension  \\\n",
       "0          sample_dataset           01.sample_dataset.csv      .csv   \n",
       "1           sales_dataset           02.sales_dataset.xlsx     .xlsx   \n",
       "2  extra_variable_dataset  03.extra_variable_dataset.xlsx     .xlsx   \n",
       "3      extra_data_dataset      04.extra_data_dataset.xlsx     .xlsx   \n",
       "4      fact_sales_dataset       05.fact_sales_dataset.csv      .csv   \n",
       "5   dim_customers_dataset    06.dim_customers_dataset.csv      .csv   \n",
       "6    dim_products_dataset     07.dim_products_dataset.csv      .csv   \n",
       "\n",
       "                                           full_path  file_size_kb  \n",
       "0  C:\\Users\\User\\Desktop\\project-exploratory-data...         97.62  \n",
       "1  C:\\Users\\User\\Desktop\\project-exploratory-data...         84.81  \n",
       "2  C:\\Users\\User\\Desktop\\project-exploratory-data...         32.95  \n",
       "3  C:\\Users\\User\\Desktop\\project-exploratory-data...         10.46  \n",
       "4  C:\\Users\\User\\Desktop\\project-exploratory-data...       3505.06  \n",
       "5  C:\\Users\\User\\Desktop\\project-exploratory-data...       1511.35  \n",
       "6  C:\\Users\\User\\Desktop\\project-exploratory-data...         27.10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inventory = DatasetInventory()\n",
    "df_inventory = inventory.build_inventory()\n",
    "inventory.export_to_csv(df_inventory)\n",
    "display(df_inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbd324-b2e3-4631-aea6-a695997a67bb",
   "metadata": {},
   "source": [
    "#  Dataset Loading Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80044806-8c2f-4ee5-9384-238ce3bb54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    \"\"\"\n",
    "    DatasetLoader\n",
    "\n",
    "    Responsible for:\n",
    "    - Loading datasets dynamically from an inventory DataFrame\n",
    "    - Supporting CSV and XLSX formats\n",
    "    - Logging loading process\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inventory_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize DatasetLoader.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inventory_df : pd.DataFrame\n",
    "            DataFrame containing dataset metadata with columns:\n",
    "            - dataset_name\n",
    "            - full_path\n",
    "            - extension\n",
    "        \"\"\"\n",
    "        self.logger = self._configure_logger()\n",
    "        self.inventory_df = inventory_df\n",
    "\n",
    "        self._validate_inventory()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        \"\"\"Configure structured logger.\"\"\"\n",
    "        logger = logging.getLogger(\"DatasetLoader\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _validate_inventory(self):\n",
    "        \"\"\"Validate required inventory columns.\"\"\"\n",
    "        required_columns = {\"dataset_name\", \"full_path\", \"extension\"}\n",
    "\n",
    "        if not required_columns.issubset(self.inventory_df.columns):\n",
    "            missing = required_columns - set(self.inventory_df.columns)\n",
    "            raise ValueError(f\"Missing required inventory columns: {missing}\")\n",
    "\n",
    "        self.logger.info(\"Inventory validation successful.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def load_datasets(self) -> dict:\n",
    "        \"\"\"\n",
    "        Load all datasets from inventory.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with dataset_name as key\n",
    "            and pandas DataFrame as value.\n",
    "        \"\"\"\n",
    "        dfs = {}\n",
    "\n",
    "        for _, row in self.inventory_df.iterrows():\n",
    "            dataset_name = row[\"dataset_name\"]\n",
    "            file_path = row[\"full_path\"]\n",
    "            extension = row[\"extension\"]\n",
    "\n",
    "            try:\n",
    "                if extension == \".csv\":\n",
    "                    dfs[dataset_name] = pd.read_csv(file_path)\n",
    "\n",
    "                elif extension == \".xlsx\":\n",
    "                    dfs[dataset_name] = pd.read_excel(\n",
    "                        file_path, engine=\"openpyxl\"\n",
    "                    )\n",
    "\n",
    "                self.logger.info(f\"Loaded dataset: {dataset_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(\n",
    "                    f\"Error loading {dataset_name}: {e}\"\n",
    "                )\n",
    "\n",
    "        self.logger.info(\"All datasets processed.\")\n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e396bcd-10c2-4591-ae92-f6ba7eface4b",
   "metadata": {},
   "source": [
    "# Dataset Loading Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f17770-0915-4d31-9ffb-d043c982f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_loader():\n",
    "    loader = DatasetLoader(df_inventory)\n",
    "    dfs = loader.load_datasets()\n",
    "\n",
    "    assert isinstance(dfs, dict), \"Output is not a dictionary\"\n",
    "    assert len(dfs) > 0, \"No datasets were loaded\"\n",
    "\n",
    "    for name, df in dfs.items():\n",
    "        assert isinstance(df, pd.DataFrame), f\"{name} is not a DataFrame\"\n",
    "\n",
    "    print(\"DatasetLoader test passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a621f2a8-c45e-46c4-b7ad-1d0f3683aa8b",
   "metadata": {},
   "source": [
    "# Test Execution - Dataset Loading Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "197a983f-1068-413e-abd4-d8f893e29e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:33,954 | INFO | Inventory validation successful.\n",
      "2026-02-25 15:00:33,967 | INFO | Loaded dataset: sample_dataset\n",
      "2026-02-25 15:00:34,601 | INFO | Loaded dataset: sales_dataset\n",
      "2026-02-25 15:00:34,683 | INFO | Loaded dataset: extra_variable_dataset\n",
      "2026-02-25 15:00:34,696 | INFO | Loaded dataset: extra_data_dataset\n",
      "2026-02-25 15:00:34,820 | INFO | Loaded dataset: fact_sales_dataset\n",
      "2026-02-25 15:00:34,882 | INFO | Loaded dataset: dim_customers_dataset\n",
      "2026-02-25 15:00:34,888 | INFO | Loaded dataset: dim_products_dataset\n",
      "2026-02-25 15:00:34,890 | INFO | All datasets processed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetLoader test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_dataset_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515bdd6-5c1e-4d6f-b47c-637339044c8e",
   "metadata": {},
   "source": [
    "# Dataset Loading Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d27b20d4-a479-43e1-8cd3-646aeebcb660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:34,914 | INFO | Inventory validation successful.\n",
      "2026-02-25 15:00:34,924 | INFO | Loaded dataset: sample_dataset\n",
      "2026-02-25 15:00:35,269 | INFO | Loaded dataset: sales_dataset\n",
      "2026-02-25 15:00:35,361 | INFO | Loaded dataset: extra_variable_dataset\n",
      "2026-02-25 15:00:35,380 | INFO | Loaded dataset: extra_data_dataset\n",
      "2026-02-25 15:00:35,491 | INFO | Loaded dataset: fact_sales_dataset\n",
      "2026-02-25 15:00:35,555 | INFO | Loaded dataset: dim_customers_dataset\n",
      "2026-02-25 15:00:35,568 | INFO | Loaded dataset: dim_products_dataset\n",
      "2026-02-25 15:00:35,569 | INFO | All datasets processed.\n"
     ]
    }
   ],
   "source": [
    "loader = DatasetLoader(df_inventory)\n",
    "loader_dfs = loader.load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9186f9e-e1d8-4fd6-84c9-005f005a4539",
   "metadata": {},
   "source": [
    "# Data Preview Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a64e0992-21e3-4124-b58e-e3e006b58b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetPreviewer:\n",
    "    \"\"\"\n",
    "    DatasetPreviewer\n",
    "\n",
    "    Responsible for:\n",
    "    - Displaying preview rows from loaded datasets\n",
    "    - Logging preview operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        datasets : dict\n",
    "            Dictionary containing dataset_name -> DataFrame\n",
    "        \"\"\"\n",
    "        self.logger = self._configure_logger()\n",
    "        self.datasets = datasets\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetPreviewer\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def preview_first_row(self):\n",
    "        \"\"\"\n",
    "        Display the first row of each dataset.\n",
    "        \"\"\"\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                self.logger.warning(f\"{dataset_name} is not a DataFrame.\")\n",
    "                continue\n",
    "\n",
    "            self.logger.info(f\"Previewing dataset: {dataset_name}\")\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(f\"First row of dataset: {dataset_name}\")\n",
    "            display(df.head(1))\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3009a5e-c919-464f-bdaf-8d823a3e768b",
   "metadata": {},
   "source": [
    "# Data Preview Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da909ed0-de22-47a4-9e6f-5912ac77e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_previewer():\n",
    "    previewer = DatasetPreviewer(loader_dfs)\n",
    "    \n",
    "    assert isinstance(previewer.datasets, dict), \"Datasets is not a dictionary\"\n",
    "    assert len(previewer.datasets) > 0, \"No datasets available\"\n",
    "\n",
    "    print(\"DatasetPreviewer test passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9824dd9-82c5-4508-840f-0e9b7d4b322e",
   "metadata": {},
   "source": [
    "# Test Execution - Data Preview Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43db2984-f329-4802-a92a-7535186b8dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPreviewer test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_dataset_previewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e49d03-adab-4a91-ae96-0e9e2f3af4b5",
   "metadata": {},
   "source": [
    "# Data Preview Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1234f86-e4d9-4f0f-a8c1-4caca5b7761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:35,636 | INFO | Previewing dataset: sample_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "First row of dataset: sample_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          NaN         10.38           122.8     1001.0           0.1184   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0            0.2776          0.3001               0.1471         0.2419   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33              NaN      2019.0   \n",
       "\n",
       "  worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0           0.1622             0.6656           0.7119                0.2654   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                   0.1189       0  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:35,665 | INFO | Previewing dataset: sales_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "First row of dataset: sales_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>order_value_EUR</th>\n",
       "      <th>cost</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>sales_manager</th>\n",
       "      <th>sales_rep</th>\n",
       "      <th>device_type</th>\n",
       "      <th>order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>98320.37</td>\n",
       "      <td>77722.25</td>\n",
       "      <td>8/23/2020</td>\n",
       "      <td>Games</td>\n",
       "      <td>Konopelski LLC</td>\n",
       "      <td>Maxie Marrow</td>\n",
       "      <td>Tarrah Castelletti</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>70-0511466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  order_value_EUR      cost       date category   customer_name  \\\n",
       "0  Sweden         98320.37  77722.25  8/23/2020    Games  Konopelski LLC   \n",
       "\n",
       "  sales_manager           sales_rep device_type    order_id  \n",
       "0  Maxie Marrow  Tarrah Castelletti      Tablet  70-0511466  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:35,683 | INFO | Previewing dataset: extra_variable_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "First row of dataset: extra_variable_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>refund</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-0511466</td>\n",
       "      <td>43621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     order_id  refund\n",
       "0  70-0511466   43621"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:35,691 | INFO | Previewing dataset: extra_data_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "First row of dataset: extra_data_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>order_value_EUR</th>\n",
       "      <th>cost</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>sales_manager</th>\n",
       "      <th>sales_rep</th>\n",
       "      <th>device_type</th>\n",
       "      <th>order_id</th>\n",
       "      <th>refund</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>17524.02</td>\n",
       "      <td>14122.61</td>\n",
       "      <td>2020-12-02 00:00:00</td>\n",
       "      <td>Books</td>\n",
       "      <td>Goldner-Dibbert</td>\n",
       "      <td>Maxie Marrow</td>\n",
       "      <td>Madelon Bront</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>70-0511466</td>\n",
       "      <td>7964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  order_value_EUR      cost                 date category  \\\n",
       "0  Sweden         17524.02  14122.61  2020-12-02 00:00:00    Books   \n",
       "\n",
       "     customer_name sales_manager      sales_rep device_type    order_id  \\\n",
       "0  Goldner-Dibbert  Maxie Marrow  Madelon Bront      Mobile  70-0511466   \n",
       "\n",
       "   refund  \n",
       "0    7964  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:35,706 | INFO | Previewing dataset: fact_sales_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "First row of dataset: fact_sales_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_number</th>\n",
       "      <th>product_key</th>\n",
       "      <th>customer_key</th>\n",
       "      <th>order_date</th>\n",
       "      <th>shipping_date</th>\n",
       "      <th>due_date</th>\n",
       "      <th>sales_amount</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SO54496</td>\n",
       "      <td>282</td>\n",
       "      <td>5400</td>\n",
       "      <td>2013-03-16</td>\n",
       "      <td>2013-03-23</td>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  order_number  product_key  customer_key  order_date shipping_date  \\\n",
       "0      SO54496          282          5400  2013-03-16    2013-03-23   \n",
       "\n",
       "     due_date  sales_amount  quantity  price  \n",
       "0  2013-03-28            25         1     25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:35,718 | INFO | Previewing dataset: dim_customers_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "First row of dataset: dim_customers_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_number</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>country</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>create_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11000</td>\n",
       "      <td>AW00011000</td>\n",
       "      <td>Jon</td>\n",
       "      <td>Yang</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Married</td>\n",
       "      <td>Male</td>\n",
       "      <td>1971-10-06</td>\n",
       "      <td>2025-10-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id customer_number first_name last_name    country  \\\n",
       "0             1        11000      AW00011000        Jon      Yang  Australia   \n",
       "\n",
       "  marital_status gender   birthdate create_date  \n",
       "0        Married   Male  1971-10-06  2025-10-06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:35,730 | INFO | Previewing dataset: dim_products_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "First row of dataset: dim_products_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_number</th>\n",
       "      <th>product_name</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>maintenance</th>\n",
       "      <th>cost</th>\n",
       "      <th>product_line</th>\n",
       "      <th>start_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "      <td>FR-R92B-58</td>\n",
       "      <td>HL Road Frame - Black- 58</td>\n",
       "      <td>CO_RF</td>\n",
       "      <td>Components</td>\n",
       "      <td>Road Frames</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Road</td>\n",
       "      <td>2003-07-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  product_id product_number               product_name  \\\n",
       "0            1         210     FR-R92B-58  HL Road Frame - Black- 58   \n",
       "\n",
       "  category_id    category  subcategory maintenance  cost product_line  \\\n",
       "0       CO_RF  Components  Road Frames         Yes     0         Road   \n",
       "\n",
       "   start_date  \n",
       "0  2003-07-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previewer = DatasetPreviewer(loader_dfs)\n",
    "previewer.preview_first_row()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a2581-f6b9-434f-a0db-314afc4c4979",
   "metadata": {},
   "source": [
    "# Dataset Registry Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db120451-4a9e-430d-a4cc-1311f663c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRegistry:\n",
    "    \"\"\"\n",
    "    DatasetRegistry\n",
    "\n",
    "    Responsible for:\n",
    "    - Registering loaded DataFrames\n",
    "    - Creating a catalog DataFrame of dataset names\n",
    "    - Exporting registry to CSV\n",
    "    - Logging registry operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict):\n",
    "        \"\"\"\n",
    "        Initialize DatasetRegistry.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datasets : dict\n",
    "            Dictionary containing dataset_name -> DataFrame\n",
    "        \"\"\"\n",
    "        self.logger = self._configure_logger()\n",
    "        self.datasets = datasets\n",
    "        self.base_dir = self._find_project_root()\n",
    "\n",
    "        self._validate_input()\n",
    "\n",
    "        # 🔥 Caminho de exportação\n",
    "        self.export_folder = self.base_dir / \"data\" / \"files\" / \"dataframes\"\n",
    "        self.export_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.logger.info(f\"Registry export folder set to: {self.export_folder}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetRegistry\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _find_project_root(self) -> Path:\n",
    "        current_path = Path().resolve()\n",
    "\n",
    "        while not (current_path / \"data\").exists() and current_path != current_path.parent:\n",
    "            current_path = current_path.parent\n",
    "\n",
    "        self.logger.info(f\"Project root located at: {current_path}\")\n",
    "        return current_path\n",
    "\n",
    "    def _validate_input(self):\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Datasets validated successfully.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def build_registry(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a registry DataFrame listing loaded DataFrames.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing dataset names.\n",
    "        \"\"\"\n",
    "        registry_df = pd.DataFrame({\n",
    "            \"dataframe_name\": list(self.datasets.keys())\n",
    "        })\n",
    "\n",
    "        self.logger.info(\"Dataset registry successfully created.\")\n",
    "        return registry_df\n",
    "\n",
    "    def export_to_csv(self, filename: str = \"dataset_registry.csv\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Export registry DataFrame to CSV.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            Name of the CSV file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Exported registry DataFrame.\n",
    "        \"\"\"\n",
    "        registry_df = self.build_registry()\n",
    "\n",
    "        export_path = self.export_folder / filename\n",
    "        registry_df.to_csv(export_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"Dataset registry exported to: {export_path}\")\n",
    "\n",
    "        return registry_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c561a84-edc5-4416-ba88-5e4affe18650",
   "metadata": {},
   "source": [
    "# Dataset Registry Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f469e34f-3457-45f1-9062-c79c7c2e1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_registry():\n",
    "    \"\"\"\n",
    "    Unit test for DatasetRegistry.\n",
    "\n",
    "    Validates:\n",
    "    - Correct instantiation\n",
    "    - Output type\n",
    "    - Expected columns\n",
    "    - Registry size consistency\n",
    "    - CSV file creation\n",
    "    \"\"\"\n",
    "\n",
    "    # Arrange (mock datasets)\n",
    "    mock_datasets = {\n",
    "        \"dataset_a\": pd.DataFrame({\"col1\": [1, 2]}),\n",
    "        \"dataset_b\": pd.DataFrame({\"col2\": [3, 4]})\n",
    "    }\n",
    "\n",
    "    registry = DatasetRegistry(mock_datasets)\n",
    "\n",
    "    # Act\n",
    "    registry_df = registry.export_to_csv(\"test_registry.csv\")\n",
    "\n",
    "    # Assert - tipo de retorno\n",
    "    assert isinstance(registry_df, pd.DataFrame), \"Output is not a DataFrame\"\n",
    "\n",
    "    # Assert - coluna esperada\n",
    "    assert \"dataframe_name\" in registry_df.columns, \"Missing dataframe_name column\"\n",
    "\n",
    "    # Assert - tamanho consistente\n",
    "    assert len(registry_df) == len(mock_datasets), \"Registry size mismatch\"\n",
    "\n",
    "    # Assert - arquivo criado\n",
    "    expected_file = registry.export_folder / \"test_registry.csv\"\n",
    "    assert expected_file.exists(), \"CSV file was not created\"\n",
    "\n",
    "    print(\"DatasetRegistry test passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2720cd1c-3cbf-428c-9ef0-3df67908875d",
   "metadata": {},
   "source": [
    "# Test Execution - Dataset Registry Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdb2dc6c-7e41-4fbb-a0f4-48da5b14d0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:35,807 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:00:35,808 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:00:35,810 | INFO | Registry export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\dataframes\n",
      "2026-02-25 15:00:35,812 | INFO | Dataset registry successfully created.\n",
      "2026-02-25 15:00:35,815 | INFO | Dataset registry exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\dataframes\\test_registry.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetRegistry test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_dataset_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ca305-8c6c-4cdc-9078-19345fde26e6",
   "metadata": {},
   "source": [
    "# Dataset Registry Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5678709-7619-48ab-b071-332141088871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:00:35,836 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:00:35,838 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:00:35,840 | INFO | Registry export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\dataframes\n",
      "2026-02-25 15:00:35,842 | INFO | Dataset registry successfully created.\n",
      "2026-02-25 15:00:35,846 | INFO | Dataset registry exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\dataframes\\dataset_registry.csv\n"
     ]
    }
   ],
   "source": [
    "registry = DatasetRegistry(loader_dfs)\n",
    "registry_df = registry.export_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7455e36-a406-49d4-a854-5757184af183",
   "metadata": {},
   "source": [
    "# Schema Profiling Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0627dd17-cfc9-4404-ae47-9b10edf4a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSchemaProfiler:\n",
    "    \"\"\"\n",
    "    DatasetSchemaProfiler\n",
    "\n",
    "    Responsible for:\n",
    "    - Extracting schema (dataset_name + column_name + dtype)\n",
    "    - Saving schema files inside:\n",
    "      project_root/data/files/schemas\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        datasets : dict\n",
    "            Dictionary with dataset_name -> DataFrame\n",
    "        \"\"\"\n",
    "        self.logger = self._configure_logger()\n",
    "        self.datasets = datasets\n",
    "        self.base_dir = self._find_project_root()\n",
    "\n",
    "        # Export folder\n",
    "        self.export_folder = self.base_dir / \"data\" / \"files\" / \"schemas\"\n",
    "        self.export_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.logger.info(f\"Export folder set to: {self.export_folder}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetSchemaProfiler\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _find_project_root(self) -> Path:\n",
    "        current_path = Path().resolve()\n",
    "\n",
    "        while not (current_path / \"data\").exists() and current_path != current_path.parent:\n",
    "            current_path = current_path.parent\n",
    "\n",
    "        self.logger.info(f\"Project root located at: {current_path}\")\n",
    "        return current_path\n",
    "\n",
    "    def _build_schema(self, dataset_name: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame({\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"column_name\": df.columns,\n",
    "            \"data_type\": df.dtypes.astype(str).values\n",
    "        })\n",
    "\n",
    "    def _export_schema(self, dataset_name: str, df: pd.DataFrame, suffix: str):\n",
    "        \"\"\"\n",
    "        Internal export method that prevents overwriting.\n",
    "        \"\"\"\n",
    "\n",
    "        output_path = self.export_folder / f\"{dataset_name}_{suffix}.csv\"\n",
    "\n",
    "        # 🔒 Do not overwrite existing file\n",
    "        if output_path.exists():\n",
    "            self.logger.warning(\n",
    "                f\"File already exists and will NOT be overwritten: {output_path}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        schema_df = self._build_schema(dataset_name, df)\n",
    "        schema_df.to_csv(output_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"Schema exported to: {output_path}\")\n",
    "        return schema_df\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def export_before_transformation(self) -> dict:\n",
    "        \"\"\"\n",
    "        Export schemas with suffix 'before_transformation'\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "            schema = self._export_schema(\n",
    "                dataset_name,\n",
    "                df,\n",
    "                suffix=\"before_transformation\"\n",
    "            )\n",
    "            results[dataset_name] = schema\n",
    "\n",
    "        return results\n",
    "\n",
    "    def export_after_transformation(self) -> dict:\n",
    "        \"\"\"\n",
    "        Export schemas with suffix 'after_transformation'\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "            schema = self._export_schema(\n",
    "                dataset_name,\n",
    "                df,\n",
    "                suffix=\"after_transformation\"\n",
    "            )\n",
    "            results[dataset_name] = schema\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7e9c8-b07b-4264-af70-4fff36252ed5",
   "metadata": {},
   "source": [
    "# Schema Profiling Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8aea0c1-053d-48b3-a678-b5beefcd70bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def test_dataset_schema_profiler():\n",
    "    print(\"🔎 Iniciando teste do DatasetSchemaProfiler...\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 1️⃣ Criar DataFrame de teste\n",
    "    # ==========================================================\n",
    "    df_test = pd.DataFrame({\n",
    "        \"id\": [1, 2, 3],\n",
    "        \"name\": [\"A\", \"B\", \"C\"],\n",
    "        \"value\": [10.5, 20.3, 30.7]\n",
    "    })\n",
    "\n",
    "    datasets = {\"test_dataset\": df_test}\n",
    "\n",
    "    profiler = DatasetSchemaProfiler(datasets)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 2️⃣ Limpar possíveis arquivos anteriores (ambiente controlado)\n",
    "    # ==========================================================\n",
    "    before_path = profiler.export_folder / \"test_dataset_before_transformation.csv\"\n",
    "    after_path = profiler.export_folder / \"test_dataset_after_transformation.csv\"\n",
    "\n",
    "    if before_path.exists():\n",
    "        before_path.unlink()\n",
    "\n",
    "    if after_path.exists():\n",
    "        after_path.unlink()\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3️⃣ Testar export_before_transformation\n",
    "    # ==========================================================\n",
    "    result_before = profiler.export_before_transformation()\n",
    "\n",
    "    assert before_path.exists(), \"❌ Arquivo before_transformation não foi criado.\"\n",
    "    assert isinstance(result_before[\"test_dataset\"], pd.DataFrame), \"❌ Retorno inválido.\"\n",
    "\n",
    "    print(\"✅ Export before_transformation funcionando.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 4️⃣ Testar bloqueio de sobrescrita\n",
    "    # ==========================================================\n",
    "    result_before_again = profiler.export_before_transformation()\n",
    "\n",
    "    assert result_before_again[\"test_dataset\"] is None, \\\n",
    "        \"❌ Arquivo foi sobrescrito quando não deveria.\"\n",
    "\n",
    "    print(\"✅ Bloqueio de sobrescrita funcionando.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 5️⃣ Testar export_after_transformation\n",
    "    # ==========================================================\n",
    "    result_after = profiler.export_after_transformation()\n",
    "\n",
    "    assert after_path.exists(), \"❌ Arquivo after_transformation não foi criado.\"\n",
    "    assert isinstance(result_after[\"test_dataset\"], pd.DataFrame), \"❌ Retorno inválido.\"\n",
    "\n",
    "    print(\"✅ Export after_transformation funcionando.\")\n",
    "\n",
    "    print(\"\\n🎯 TODOS OS TESTES PASSARAM COM SUCESSO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9705da9-34e0-494b-8f9d-c06d997b12b8",
   "metadata": {},
   "source": [
    "# Test Execution - Schema Profiling Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20c3bdd0-5b27-4461-bf46-63bfd7c841cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:01:58,232 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:01:58,234 | INFO | Export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\n",
      "2026-02-25 15:01:58,239 | INFO | Schema exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\test_dataset_before_transformation.csv\n",
      "2026-02-25 15:01:58,241 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\test_dataset_before_transformation.csv\n",
      "2026-02-25 15:01:58,246 | INFO | Schema exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\test_dataset_after_transformation.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Iniciando teste do DatasetSchemaProfiler...\n",
      "✅ Export before_transformation funcionando.\n",
      "✅ Bloqueio de sobrescrita funcionando.\n",
      "✅ Export after_transformation funcionando.\n",
      "\n",
      "🎯 TODOS OS TESTES PASSARAM COM SUCESSO!\n"
     ]
    }
   ],
   "source": [
    "test_dataset_schema_profiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bbef3-107d-427e-8725-df19ea4bb5ad",
   "metadata": {},
   "source": [
    "# Schema Profiling Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96e55e33-d011-4cb8-8703-a4df41a70c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:02:00,040 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:02:00,042 | INFO | Export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\n",
      "2026-02-25 15:02:00,045 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\sample_dataset_before_transformation.csv\n",
      "2026-02-25 15:02:00,047 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\sales_dataset_before_transformation.csv\n",
      "2026-02-25 15:02:00,048 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\extra_variable_dataset_before_transformation.csv\n",
      "2026-02-25 15:02:00,050 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\extra_data_dataset_before_transformation.csv\n",
      "2026-02-25 15:02:00,052 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\fact_sales_dataset_before_transformation.csv\n",
      "2026-02-25 15:02:00,054 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\dim_customers_dataset_before_transformation.csv\n",
      "2026-02-25 15:02:00,055 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\schemas\\dim_products_dataset_before_transformation.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sample_dataset': None,\n",
       " 'sales_dataset': None,\n",
       " 'extra_variable_dataset': None,\n",
       " 'extra_data_dataset': None,\n",
       " 'fact_sales_dataset': None,\n",
       " 'dim_customers_dataset': None,\n",
       " 'dim_products_dataset': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler = DatasetSchemaProfiler(loader_dfs)\n",
    "profiler.export_before_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b936d26-9f85-4641-b78f-adca18626e21",
   "metadata": {},
   "source": [
    "# Column-Level Description for Each DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431fb786-d142-4762-b689-8499035099d2",
   "metadata": {},
   "source": [
    "# Dataset: sample\n",
    "## Medidas mean (média)\n",
    "\n",
    "- mean radius: Tamanho médio do núcleo.\n",
    "- mean texture: Variação da cor (irregularidade visual).\n",
    "- mean perimeter: Comprimento médio da borda.\n",
    "- mean area: Área média do núcleo.\n",
    "- mean smoothness: Regularidade da superfície.\n",
    "- mean compactness: Grau de irregularidade da forma.\n",
    "- mean concavity: Nível de “entradas” na borda.\n",
    "- mean concave points: Quantidade de pontos côncavos.\n",
    "- mean symmetry: Nível de simetria.\n",
    "- mean fractal dimension: Complexidade da borda.\n",
    "## Medidas error (variação)\n",
    "- Mostram o quanto cada medida varia entre as células.\n",
    "- radius error: Variação do tamanho.\n",
    "- texture error: Variação da textura.\n",
    "- perimeter error: Variação do perímetro.\n",
    "- area error: Variação da área.\n",
    "- smoothness error: Variação da suavidade.\n",
    "- compactness error: Variação da compactação.\n",
    "- concavity error: Variação da concavidade.\n",
    "- concave points error: Variação dos pontos côncavos.\n",
    "- symmetry error: Variação da simetria.\n",
    "- fractal dimension error: Variação da complexidade.\n",
    "## Medidas worst (pior valor observado)\n",
    "- Maior valor encontrado em cada característica.\n",
    "- worst radius: Maior tamanho.\n",
    "- worst texture: Maior irregularidade.\n",
    "- worst perimeter: Maior perímetro.\n",
    "- worst area: Maior área.\n",
    "- worst smoothness: Maior irregularidade de superfície.\n",
    "- worst compactness: Maior irregularidade de forma.\n",
    "- worst concavity: Maior concavidade.\n",
    "- worst concave points: Maior número de pontos côncavos.\n",
    "- worst symmetry: Maior assimetria.\n",
    "- worst fractal dimension: Maior complexidade da borda.\n",
    "\n",
    "## target\n",
    "- 0: Benigno\n",
    "- 1: Maligno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64cf438-f6ec-4c2e-ac55-7bc80c6b6023",
   "metadata": {},
   "source": [
    "# Dataset: sales\n",
    "\n",
    "## Dataset transacional de vendas.\n",
    "\n",
    "- Coluna: Significado\n",
    "- country: País do cliente\n",
    "- order_value_EUR: Valor total do pedido em euros\n",
    "- cost: Custo da venda (⚠️ está como object, deveria ser float)\n",
    "- date: Data da venda\n",
    "- category: Categoria do produto\n",
    "- customer_name: Nome do cliente\n",
    "- sales_manager: Gerente responsável\n",
    "- sales_rep: Representante comercial\n",
    "- device_type: Dispositivo usado na compra\n",
    "- order_id: dentificador único do pedido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2717db3-1bfe-4e42-8770-2c477f11b3f2",
   "metadata": {},
   "source": [
    "# Dataset: extra_variable\r\n",
    "- Coluna:\tSignificado\r\n",
    "- order_id:\tChave de ligação\r\n",
    "- refund:\tValor reembolsado ao cliente\r\n",
    "###### Complementa o dataset sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd0a86-030e-494f-9544-700c0673e051",
   "metadata": {},
   "source": [
    "# Dataset: extra_data\n",
    "\n",
    "Este dataset já é uma versão enriquecida do sales.\n",
    "\n",
    "- Coluna: Significado\n",
    "- country: País\n",
    "- order_value_EUR: Valor do pedido\n",
    "- cost: Custo da venda\n",
    "- date: Data\n",
    "- category: Categoria\n",
    "- customer_name: Cliente\n",
    "- sales_manager: Gerente\n",
    "- sales_rep: Representante\n",
    "- device_type: Dispositivo\n",
    "- order_id\tID do pedido\n",
    "- refund:\tReembolso\n",
    "\n",
    "###### É praticamente o sales já consolidado com extra_variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15447542-7427-4822-a443-4e933d32d3f8",
   "metadata": {},
   "source": [
    "# Dataset: fact_sales\r\n",
    "\r\n",
    "###### Tabela Fato (Modelo Star Schema – Data Warehouse)\r\n",
    "\r\n",
    "Representa fatos de vendas.\r\n",
    "\r\n",
    "- Coluna: Significado\r\n",
    "- order_number: Número do pedido\r\n",
    "- product_key: Chave estrangeira do produto\r\n",
    "- customer_key: Chave estrangeira do cliente\r\n",
    "- order_date: Data do pedido\r\n",
    "- shipping_date: Data de envio\r\n",
    "- due_date: Data prevista de entrega\r\n",
    "- sales_amount: Valor total da venda\r\n",
    "- quantity: Quantidade vendida\r\n",
    "- price: Preço unitário\r\n",
    "\r\n",
    "###### Esta é a tabela central do modelo dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95561f-c1f2-4e7d-bff5-19afbe92ea9a",
   "metadata": {},
   "source": [
    "# Dataset: dim_customers\r\n",
    "\r\n",
    "###### Dimensão Cliente\r\n",
    "\r\n",
    "- Coluna: Significado\r\n",
    "- customer_key: Chave substituta (Primary Key)\r\n",
    "- customer_id: ID original do sistema\r\n",
    "- customer_number: Código do cliente\r\n",
    "- first_name: Primeiro nome\r\n",
    "- last_name: Sobrenome\r\n",
    "- country: País\r\n",
    "- marital_status: Estado civil\r\n",
    "- gender: Gênero\r\n",
    "- birthdate: Data de nascimento\r\n",
    "- create_date: Data de cadastro\r\n",
    "\r\n",
    "###### Usado para análises demográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de88c3a-9996-4db9-addd-22c94e212c6f",
   "metadata": {},
   "source": [
    "# Dataset: dim_products\n",
    "\n",
    "###### Dimensão Produto\n",
    "\n",
    "- Coluna: Significado\n",
    "- product_key: Chave substituta\n",
    "- product_id: ID original\n",
    "- product_number: Código do produto\n",
    "- product_name: Nome do produto\n",
    "- category_id: ID da categoria\n",
    "- category: Categoria do produto\n",
    "- subcategory: Subcategoria\n",
    "- maintenance: Tipo de manutenção\n",
    "- cost: Custo do produto\n",
    "- product_line: Linha de produto\n",
    "- start_date: Data de início de comercialização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87247a4b-fdd6-436e-917d-462743ce0f76",
   "metadata": {},
   "source": [
    "# Dataset Shape Inspection Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c11f7f34-0432-461b-94ed-4c39a4d620d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetShapeInspector:\n",
    "    \"\"\"\n",
    "    DatasetShapeInspector\n",
    "\n",
    "    Responsible for:\n",
    "    - Inspecting dataset shapes\n",
    "    - Logging inspection operations\n",
    "    - Exporting shape report to CSV\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict):\n",
    "        self.logger = self._configure_logger()\n",
    "        self.datasets = datasets\n",
    "        self.base_dir = self._find_project_root()\n",
    "        self._validate_input()\n",
    "\n",
    "        # Export path\n",
    "        self.export_folder = self.base_dir / \"data\" / \"files\" / \"shape\"\n",
    "        self.export_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.logger.info(f\"Shape export folder set to: {self.export_folder}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetShapeInspector\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _find_project_root(self) -> Path:\n",
    "        current_path = Path().resolve()\n",
    "\n",
    "        while not (current_path / \"data\").exists() and current_path != current_path.parent:\n",
    "            current_path = current_path.parent\n",
    "\n",
    "        self.logger.info(f\"Project root located at: {current_path}\")\n",
    "        return current_path\n",
    "\n",
    "    def _validate_input(self):\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Datasets validated successfully.\")\n",
    "\n",
    "    def _build_shape_dataframe(self) -> pd.DataFrame:\n",
    "        results = []\n",
    "\n",
    "        for name, df in self.datasets.items():\n",
    "\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                self.logger.warning(f\"{name} is not a DataFrame. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            rows, cols = df.shape\n",
    "\n",
    "            self.logger.info(f\"Dataset: {name} | Rows: {rows} | Columns: {cols}\")\n",
    "\n",
    "            results.append({\n",
    "                \"dataset_name\": name,\n",
    "                \"rows\": rows,\n",
    "                \"columns\": cols\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def _export_shape(self, suffix: str):\n",
    "        \"\"\"\n",
    "        Internal export method that prevents overwriting.\n",
    "        \"\"\"\n",
    "\n",
    "        output_path = self.export_folder / f\"datasets_shape_report_{suffix}.csv\"\n",
    "\n",
    "        # 🔒 Prevent overwrite\n",
    "        if output_path.exists():\n",
    "            self.logger.warning(\n",
    "                f\"File already exists and will NOT be overwritten: {output_path}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        shape_df = self._build_shape_dataframe()\n",
    "        shape_df.to_csv(output_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"Shape report exported to: {output_path}\")\n",
    "        return shape_df\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def export_before_transformation(self) -> dict:\n",
    "        \"\"\"\n",
    "        Export shape report with suffix 'before_transformation'\n",
    "        \"\"\"\n",
    "        return self._export_shape(\"before_transformation\")\n",
    "\n",
    "    def export_after_transformation(self) -> dict:\n",
    "        \"\"\"\n",
    "        Export shape report with suffix 'after_transformation'\n",
    "        \"\"\"\n",
    "        return self._export_shape(\"after_transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cc00c-69c4-490f-bed3-8c541700b213",
   "metadata": {},
   "source": [
    "# Dataset Shape Inspection Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "400cd196-e6d2-4f27-9ec4-f99bbcc04c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_shape_inspector():\n",
    "    print(\"🔎 Iniciando teste do DatasetShapeInspector...\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 1️⃣ Criar dataset de teste\n",
    "    # ==========================================================\n",
    "    df_test = pd.DataFrame({\n",
    "        \"col1\": [1, 2, 3],\n",
    "        \"col2\": [\"A\", \"B\", \"C\"]\n",
    "    })\n",
    "\n",
    "    datasets = {\"test_dataset\": df_test}\n",
    "    inspector = DatasetShapeInspector(datasets)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 2️⃣ Preparar caminhos esperados (com sufixo _test)\n",
    "    # ==========================================================\n",
    "    before_path = (\n",
    "        inspector.export_folder /\n",
    "        \"datasets_shape_report_before_transformation_test.csv\"\n",
    "    )\n",
    "\n",
    "    after_path = (\n",
    "        inspector.export_folder /\n",
    "        \"datasets_shape_report_after_transformation_test.csv\"\n",
    "    )\n",
    "\n",
    "    # Limpar arquivos anteriores\n",
    "    if before_path.exists():\n",
    "        before_path.unlink()\n",
    "\n",
    "    if after_path.exists():\n",
    "        after_path.unlink()\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3️⃣ Testar export BEFORE (com sufixo _test)\n",
    "    # ==========================================================\n",
    "    result_before = inspector._export_shape(\"before_transformation_test\")\n",
    "\n",
    "    assert before_path.exists(), \"❌ Arquivo before_transformation_test não foi criado.\"\n",
    "    assert isinstance(result_before, pd.DataFrame), \"❌ Retorno não é DataFrame.\"\n",
    "    assert result_before.loc[0, \"rows\"] == 3, \"❌ Número de linhas incorreto.\"\n",
    "    assert result_before.loc[0, \"columns\"] == 2, \"❌ Número de colunas incorreto.\"\n",
    "\n",
    "    print(\"✅ Export before_transformation_test funcionando.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 4️⃣ Testar bloqueio de sobrescrita\n",
    "    # ==========================================================\n",
    "    result_before_again = inspector._export_shape(\"before_transformation_test\")\n",
    "\n",
    "    assert result_before_again is None, \\\n",
    "        \"❌ Arquivo foi sobrescrito quando não deveria.\"\n",
    "\n",
    "    print(\"✅ Bloqueio de sobrescrita funcionando.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 5️⃣ Testar export AFTER (com sufixo _test)\n",
    "    # ==========================================================\n",
    "    result_after = inspector._export_shape(\"after_transformation_test\")\n",
    "\n",
    "    assert after_path.exists(), \"❌ Arquivo after_transformation_test não foi criado.\"\n",
    "    assert isinstance(result_after, pd.DataFrame), \"❌ Retorno não é DataFrame.\"\n",
    "\n",
    "    print(\"✅ Export after_transformation_test funcionando.\")\n",
    "\n",
    "    print(\"\\n🎯 TODOS OS TESTES PASSARAM COM SUCESSO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0914612-b37a-41ce-998e-5ead9522ef6a",
   "metadata": {},
   "source": [
    "# Test Execution - Dataset Shape Inspection Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d94418c-780c-4d93-afed-741b6b8bfc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:02:07,876 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:02:07,877 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:02:07,880 | INFO | Shape export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\shape\n",
      "2026-02-25 15:02:07,883 | INFO | Dataset: test_dataset | Rows: 3 | Columns: 2\n",
      "2026-02-25 15:02:07,887 | INFO | Shape report exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\shape\\datasets_shape_report_before_transformation_test.csv\n",
      "2026-02-25 15:02:07,889 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\shape\\datasets_shape_report_before_transformation_test.csv\n",
      "2026-02-25 15:02:07,891 | INFO | Dataset: test_dataset | Rows: 3 | Columns: 2\n",
      "2026-02-25 15:02:07,896 | INFO | Shape report exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\shape\\datasets_shape_report_after_transformation_test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Iniciando teste do DatasetShapeInspector...\n",
      "✅ Export before_transformation_test funcionando.\n",
      "✅ Bloqueio de sobrescrita funcionando.\n",
      "✅ Export after_transformation_test funcionando.\n",
      "\n",
      "🎯 TODOS OS TESTES PASSARAM COM SUCESSO!\n"
     ]
    }
   ],
   "source": [
    "test_dataset_shape_inspector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef59e3-ca8f-4865-8cf0-945728569c63",
   "metadata": {},
   "source": [
    "# Dataset Shape Inspection Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7caed67-7441-4f21-ad8f-8568f4582893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:02:10,586 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:02:10,587 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:02:10,589 | INFO | Shape export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\shape\n",
      "2026-02-25 15:02:10,591 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\shape\\datasets_shape_report_before_transformation.csv\n"
     ]
    }
   ],
   "source": [
    "inspector = DatasetShapeInspector(loader_dfs)\n",
    "inspector.export_before_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00f360-71dd-4e29-8f6e-58ab0de7b509",
   "metadata": {},
   "source": [
    "# Dataset Info profiler Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53874531-f64f-485e-9f36-29a3e73fceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetInfoProfiler:\n",
    "    \"\"\"\n",
    "    DatasetInfoProfiler\n",
    "\n",
    "    Responsible for:\n",
    "    - Extracting structured df.info() metadata\n",
    "    - Exporting dataset structural reports\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict[str, pd.DataFrame]):\n",
    "        self.logger = self._configure_logger()\n",
    "        self.datasets = datasets\n",
    "        self.base_dir = self._find_project_root()\n",
    "\n",
    "        self.export_folder = self.base_dir / \"data\" / \"files\" / \"info\"\n",
    "        self.export_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self._validate_input()\n",
    "\n",
    "        self.logger.info(f\"Info export folder set to: {self.export_folder}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetInfoProfiler\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _find_project_root(self) -> Path:\n",
    "        current_path = Path().resolve()\n",
    "\n",
    "        while not (current_path / \"data\").exists() and current_path != current_path.parent:\n",
    "            current_path = current_path.parent\n",
    "\n",
    "        self.logger.info(f\"Project root located at: {current_path}\")\n",
    "        return current_path\n",
    "\n",
    "    def _validate_input(self):\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Datasets validated successfully.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # BUILD METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def build_column_info(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build column-level structure DataFrame.\n",
    "        \"\"\"\n",
    "        return pd.DataFrame({\n",
    "            \"#\": range(len(df.columns)),\n",
    "            \"Column\": df.columns,\n",
    "            \"Non-Null Count\": df.notnull().sum().values,\n",
    "            \"Dtype\": df.dtypes.astype(str).values\n",
    "        })\n",
    "\n",
    "    def build_dataset_metadata(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build dataset-level metadata DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        range_index = (\n",
    "            f\"{df.index.start} to {df.index.stop - 1}\"\n",
    "            if isinstance(df.index, pd.RangeIndex)\n",
    "            else \"Custom Index\"\n",
    "        )\n",
    "\n",
    "        return pd.DataFrame([{\n",
    "            \"RangeIndex\": range_index,\n",
    "            \"Total Rows\": df.shape[0],\n",
    "            \"Total Columns\": df.shape[1],\n",
    "            \"Dtype Summary\": str(df.dtypes.value_counts().to_dict()),\n",
    "            \"Memory Usage (MB)\": round(df.memory_usage(deep=True).sum() / (1024 ** 2), 4)\n",
    "        }])\n",
    "\n",
    "    # ==========================================================\n",
    "    # EXPORT METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def export_all_infos(self) -> dict:\n",
    "        \"\"\"\n",
    "        Export default CSV files:\n",
    "        - *_columns_info.csv\n",
    "        - *_metadata_info.csv\n",
    "        \"\"\"\n",
    "        return self._export_with_suffix(suffix=\"\")\n",
    "\n",
    "    def export_all_infos_with_suffix(self, suffix: str) -> dict:\n",
    "        \"\"\"\n",
    "        Export CSV files adding a suffix at the end of the filename.\n",
    "\n",
    "        Example:\n",
    "        dataset_columns_info_v2.csv\n",
    "        dataset_metadata_info_v2.csv\n",
    "        \"\"\"\n",
    "        if not suffix:\n",
    "            raise ValueError(\"Suffix cannot be empty.\")\n",
    "\n",
    "        return self._export_with_suffix(suffix=f\"_{suffix}\")\n",
    "\n",
    "    def _export_with_suffix(self, suffix: str) -> dict:\n",
    "        \"\"\"\n",
    "        Internal reusable export logic.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                self.logger.warning(f\"{dataset_name} is not a DataFrame. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            column_info_df = self.build_column_info(df)\n",
    "            metadata_df = self.build_dataset_metadata(df)\n",
    "\n",
    "            column_path = self.export_folder / \\\n",
    "                f\"{dataset_name}_columns_info{suffix}.csv\"\n",
    "\n",
    "            metadata_path = self.export_folder / \\\n",
    "                f\"{dataset_name}_metadata_info{suffix}.csv\"\n",
    "\n",
    "            column_info_df.to_csv(column_path, index=False)\n",
    "            metadata_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "            self.logger.info(f\"Column info exported: {column_path}\")\n",
    "            self.logger.info(f\"Metadata info exported: {metadata_path}\")\n",
    "\n",
    "            results[dataset_name] = {\n",
    "                \"columns_info\": column_info_df,\n",
    "                \"metadata_info\": metadata_df\n",
    "            }\n",
    "\n",
    "        self.logger.info(\"Dataset info reports exported successfully.\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47298d51-4319-4d24-90b4-5c908c84cb0e",
   "metadata": {},
   "source": [
    "# Dataset Info Profiler Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f06fcd6-6b85-4215-937e-39fa7fd5a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_info_profiler():\n",
    "    \"\"\"\n",
    "    Simple unit test for DatasetInfoProfiler.\n",
    "\n",
    "    Validates:\n",
    "    - Folder creation\n",
    "    - CSV export (default)\n",
    "    - CSV export with suffix\n",
    "    - Structure of returned DataFrames\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Criando DataFrame de teste\n",
    "    df = pd.DataFrame({\n",
    "        \"A\": [1, 2, None],\n",
    "        \"B\": [\"x\", \"y\", \"z\"]\n",
    "    })\n",
    "\n",
    "    datasets = {\"test_dataset\": df}\n",
    "\n",
    "    # Ambiente isolado\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "\n",
    "        temp_path = Path(temp_dir)\n",
    "\n",
    "        # Criando estrutura mínima para find_project_root funcionar\n",
    "        (temp_path / \"data\").mkdir()\n",
    "\n",
    "        profiler = DatasetInfoProfiler(datasets)\n",
    "\n",
    "        # Sobrescrevendo base_dir para ambiente temporário\n",
    "        profiler.base_dir = temp_path\n",
    "        profiler.export_folder = temp_path / \"data\" / \"files\" / \"info\"\n",
    "        profiler.export_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Teste export padrão\n",
    "        results = profiler.export_all_infos()\n",
    "\n",
    "        column_file = profiler.export_folder / \"test_dataset_columns_info.csv\"\n",
    "        metadata_file = profiler.export_folder / \"test_dataset_metadata_info.csv\"\n",
    "\n",
    "        assert column_file.exists(), \"Column info CSV was not created.\"\n",
    "        assert metadata_file.exists(), \"Metadata CSV was not created.\"\n",
    "\n",
    "        assert \"columns_info\" in results[\"test_dataset\"]\n",
    "        assert \"metadata_info\" in results[\"test_dataset\"]\n",
    "\n",
    "        # Teste export com sufixo\n",
    "        profiler.export_all_infos_with_suffix(\"v2\")\n",
    "\n",
    "        column_file_suffix = profiler.export_folder / \"test_dataset_columns_info_v2.csv\"\n",
    "        metadata_file_suffix = profiler.export_folder / \"test_dataset_metadata_info_v2.csv\"\n",
    "\n",
    "        assert column_file_suffix.exists(), \"Column info CSV with suffix not created.\"\n",
    "        assert metadata_file_suffix.exists(), \"Metadata CSV with suffix not created.\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e736a-2e9c-4942-9fd5-69c6d23130be",
   "metadata": {},
   "source": [
    "# Test Execution - Dataset Info Profiler Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e58f1681-5be9-4db7-b6f6-d55b3122ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:02:13,682 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:02:13,685 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:02:13,686 | INFO | Info export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\n",
      "2026-02-25 15:02:13,707 | INFO | Column info exported: C:\\Users\\User\\AppData\\Local\\Temp\\tmp9qqkh3ny\\data\\files\\info\\test_dataset_columns_info.csv\n",
      "2026-02-25 15:02:13,709 | INFO | Metadata info exported: C:\\Users\\User\\AppData\\Local\\Temp\\tmp9qqkh3ny\\data\\files\\info\\test_dataset_metadata_info.csv\n",
      "2026-02-25 15:02:13,709 | INFO | Dataset info reports exported successfully.\n",
      "2026-02-25 15:02:13,720 | INFO | Column info exported: C:\\Users\\User\\AppData\\Local\\Temp\\tmp9qqkh3ny\\data\\files\\info\\test_dataset_columns_info_v2.csv\n",
      "2026-02-25 15:02:13,722 | INFO | Metadata info exported: C:\\Users\\User\\AppData\\Local\\Temp\\tmp9qqkh3ny\\data\\files\\info\\test_dataset_metadata_info_v2.csv\n",
      "2026-02-25 15:02:13,723 | INFO | Dataset info reports exported successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_dataset_info_profiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4988dd8-1696-4ba8-baff-edbdc813476c",
   "metadata": {},
   "source": [
    "# Dataset Info Profiler Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea8b0811-81a4-4750-908a-1173eb44d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:03:16,168 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:03:16,170 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:03:16,171 | INFO | Info export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\n",
      "2026-02-25 15:03:16,185 | INFO | Column info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\sample_dataset_columns_info.csv\n",
      "2026-02-25 15:03:16,186 | INFO | Metadata info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\sample_dataset_metadata_info.csv\n",
      "2026-02-25 15:03:16,200 | INFO | Column info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\sales_dataset_columns_info.csv\n",
      "2026-02-25 15:03:16,202 | INFO | Metadata info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\sales_dataset_metadata_info.csv\n",
      "2026-02-25 15:03:16,212 | INFO | Column info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\extra_variable_dataset_columns_info.csv\n",
      "2026-02-25 15:03:16,214 | INFO | Metadata info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\extra_variable_dataset_metadata_info.csv\n",
      "2026-02-25 15:03:16,229 | INFO | Column info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\extra_data_dataset_columns_info.csv\n",
      "2026-02-25 15:03:16,230 | INFO | Metadata info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\extra_data_dataset_metadata_info.csv\n",
      "2026-02-25 15:03:16,352 | INFO | Column info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\fact_sales_dataset_columns_info.csv\n",
      "2026-02-25 15:03:16,354 | INFO | Metadata info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\fact_sales_dataset_metadata_info.csv\n",
      "2026-02-25 15:03:16,427 | INFO | Column info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\dim_customers_dataset_columns_info.csv\n",
      "2026-02-25 15:03:16,428 | INFO | Metadata info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\dim_customers_dataset_metadata_info.csv\n",
      "2026-02-25 15:03:16,443 | INFO | Column info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\dim_products_dataset_columns_info.csv\n",
      "2026-02-25 15:03:16,444 | INFO | Metadata info exported: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\info\\dim_products_dataset_metadata_info.csv\n",
      "2026-02-25 15:03:16,445 | INFO | Dataset info reports exported successfully.\n"
     ]
    }
   ],
   "source": [
    "info_profiler = DatasetInfoProfiler(loader_dfs)\n",
    "info_results = info_profiler.export_all_infos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777189bf-637d-4b7c-ba75-24663cb9ca5b",
   "metadata": {},
   "source": [
    "# DataFrame Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d3571-6d4c-4ce2-8954-38f5298c6b3b",
   "metadata": {},
   "source": [
    "# Mixed Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a95698-1e1f-42cd-819a-4434a51accac",
   "metadata": {},
   "source": [
    "# Date Column Profiling Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40324035-b254-4284-b763-601ae2e0d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateColumnProfiler:\n",
    "    \"\"\"\n",
    "    DateColumnProfiler\n",
    "\n",
    "    Responsible for:\n",
    "    - Identifying columns containing 'date'\n",
    "    - Extracting dataset_name, column_name and dtype\n",
    "    - Exporting reports (before / after transformation)\n",
    "    - Preventing overwrite\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict):\n",
    "        self.logger = self._configure_logger()\n",
    "        self.datasets = datasets\n",
    "        self.base_dir = self._find_project_root()\n",
    "\n",
    "        self.export_folder = self.base_dir / \"data\" / \"files\" / \"date_columns\"\n",
    "        self.export_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self._validate_input()\n",
    "\n",
    "        self.logger.info(f\"Date columns export folder set to: {self.export_folder}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self):\n",
    "        logger = logging.getLogger(\"DateColumnProfiler\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _find_project_root(self):\n",
    "        current_path = Path().resolve()\n",
    "\n",
    "        while not (current_path / \"data\").exists() and current_path != current_path.parent:\n",
    "            current_path = current_path.parent\n",
    "\n",
    "        self.logger.info(f\"Project root located at: {current_path}\")\n",
    "        return current_path\n",
    "\n",
    "    def _validate_input(self):\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Datasets validated successfully.\")\n",
    "\n",
    "    def _extract_date_columns(self) -> pd.DataFrame:\n",
    "        results = []\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                self.logger.warning(f\"{dataset_name} is not a DataFrame. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            date_columns = [\n",
    "                col for col in df.columns\n",
    "                if \"date\" in col.lower()\n",
    "            ]\n",
    "\n",
    "            for col in date_columns:\n",
    "                results.append({\n",
    "                    \"dataset_name\": dataset_name,\n",
    "                    \"column_name\": col,\n",
    "                    \"dtype\": str(df[col].dtype)\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def _export(self, suffix: str):\n",
    "        \"\"\"\n",
    "        Internal export method.\n",
    "        Prevents overwriting existing files.\n",
    "        \"\"\"\n",
    "\n",
    "        filename = f\"date_columns_report_{suffix}.csv\"\n",
    "        output_path = self.export_folder / filename\n",
    "\n",
    "        # 🔒 Prevent overwrite\n",
    "        if output_path.exists():\n",
    "            self.logger.warning(\n",
    "                f\"File already exists and will NOT be overwritten: {output_path}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        result_df = self._extract_date_columns()\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"Date columns report exported to: {output_path}\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def export_before_transformation(self):\n",
    "        \"\"\"\n",
    "        Export report before transformation.\n",
    "        \"\"\"\n",
    "        return self._export(\"before_transformation\")\n",
    "\n",
    "    def export_after_transformation(self):\n",
    "        \"\"\"\n",
    "        Export report after transformation.\n",
    "        \"\"\"\n",
    "        return self._export(\"after_transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bbc40e-069f-4ede-9b0e-af6deeacd910",
   "metadata": {},
   "source": [
    "# Date Column Profiling Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ab92b94-acad-4460-b228-c0f32af55b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def test_date_column_profiler():\n",
    "    print(\"🔎 Iniciando teste do DateColumnProfiler...\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 1️⃣ Criar dataset de teste\n",
    "    # ==========================================================\n",
    "    df_test = pd.DataFrame({\n",
    "        \"id\": [1, 2, 3],\n",
    "        \"created_date\": pd.to_datetime([\"2024-01-01\", \"2024-01-02\", \"2024-01-03\"]),\n",
    "        \"value\": [10, 20, 30]\n",
    "    })\n",
    "\n",
    "    datasets = {\"test_dataset\": df_test}\n",
    "    profiler = DateColumnProfiler(datasets)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 2️⃣ Redirecionar export para modo TEST\n",
    "    # ==========================================================\n",
    "    test_export_folder = profiler.export_folder\n",
    "    profiler.export_folder = test_export_folder\n",
    "\n",
    "    before_path = profiler.export_folder / \"test_date_columns_report_before_transformation.csv\"\n",
    "    after_path = profiler.export_folder / \"test_date_columns_report_after_transformation.csv\"\n",
    "\n",
    "    # Limpar arquivos antigos\n",
    "    if before_path.exists():\n",
    "        before_path.unlink()\n",
    "\n",
    "    if after_path.exists():\n",
    "        after_path.unlink()\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3️⃣ Monkey patch do método _export para usar prefixo test\n",
    "    # ==========================================================\n",
    "    def _export_with_test_prefix(self, suffix: str):\n",
    "        filename = f\"test_date_columns_report_{suffix}.csv\"\n",
    "        output_path = self.export_folder / filename\n",
    "\n",
    "        if output_path.exists():\n",
    "            self.logger.warning(\n",
    "                f\"File already exists and will NOT be overwritten: {output_path}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        result_df = self._extract_date_columns()\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        return result_df\n",
    "\n",
    "    # Substitui temporariamente método\n",
    "    profiler._export = _export_with_test_prefix.__get__(profiler)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 4️⃣ Testar BEFORE\n",
    "    # ==========================================================\n",
    "    result_before = profiler.export_before_transformation()\n",
    "\n",
    "    assert before_path.exists(), \"❌ Arquivo before_transformation não foi criado.\"\n",
    "    assert isinstance(result_before, pd.DataFrame), \"❌ Retorno inválido.\"\n",
    "    assert \"created_date\" in result_before[\"column_name\"].values, \\\n",
    "        \"❌ Coluna date não detectada.\"\n",
    "\n",
    "    print(\"✅ Export before_transformation funcionando.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 5️⃣ Testar bloqueio de sobrescrita\n",
    "    # ==========================================================\n",
    "    result_before_again = profiler.export_before_transformation()\n",
    "\n",
    "    assert result_before_again is None, \\\n",
    "        \"❌ Arquivo foi sobrescrito quando não deveria.\"\n",
    "\n",
    "    print(\"✅ Bloqueio de sobrescrita funcionando.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 6️⃣ Testar AFTER\n",
    "    # ==========================================================\n",
    "    result_after = profiler.export_after_transformation()\n",
    "\n",
    "    assert after_path.exists(), \"❌ Arquivo after_transformation não foi criado.\"\n",
    "    assert isinstance(result_after, pd.DataFrame), \"❌ Retorno inválido.\"\n",
    "\n",
    "    print(\"✅ Export after_transformation funcionando.\")\n",
    "\n",
    "    print(\"\\n🎯 TODOS OS TESTES PASSARAM COM SUCESSO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c682f-7ae7-4fb8-b17b-6228771c7596",
   "metadata": {},
   "source": [
    "# Test Execution - Date Column Profiling Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3eaa78f0-40db-4f74-a0fd-70387467911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:03:41,425 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:03:41,427 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:03:41,428 | INFO | Date columns export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\date_columns\n",
      "2026-02-25 15:03:41,435 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\date_columns\\test_date_columns_report_before_transformation.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Iniciando teste do DateColumnProfiler...\n",
      "✅ Export before_transformation funcionando.\n",
      "✅ Bloqueio de sobrescrita funcionando.\n",
      "✅ Export after_transformation funcionando.\n",
      "\n",
      "🎯 TODOS OS TESTES PASSARAM COM SUCESSO!\n"
     ]
    }
   ],
   "source": [
    "test_date_column_profiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a77ef6e-2560-4eda-a79c-2fcc252b8933",
   "metadata": {},
   "source": [
    "# Date Column Profiling Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa1ef2dc-6561-46e8-abae-6907ae50cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:03:47,111 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:03:47,114 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:03:47,115 | INFO | Date columns export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\date_columns\n",
      "2026-02-25 15:03:47,117 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\date_columns\\date_columns_report_before_transformation.csv\n"
     ]
    }
   ],
   "source": [
    "profiler = DateColumnProfiler(loader_dfs)\n",
    "# Antes da transformação\n",
    "profiler.export_before_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5150666-af42-4bf8-81c9-b5435f2dac1a",
   "metadata": {},
   "source": [
    "# Date Type Converter Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b6d4af8-77d3-48c9-9d68-6abe15e81b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTypeConverter:\n",
    "    \"\"\"\n",
    "    DateTypeConverter\n",
    "\n",
    "    Responsible for:\n",
    "    - Identifying object columns containing 'date'\n",
    "    - Converting them to datetime64[ns]\n",
    "    - Returning a new transformed dictionary (immutable input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loader_dfs: dict):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        loader_dfs : dict\n",
    "            Dictionary containing dataset_name -> DataFrame\n",
    "        \"\"\"\n",
    "        self.logger = self._configure_logger()\n",
    "        self.loader_dfs = loader_dfs\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self):\n",
    "        logger = logging.getLogger(\"DateTypeConverter\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _validate_input(self):\n",
    "        if not isinstance(self.loader_dfs, dict):\n",
    "            raise TypeError(\"Input must be a dictionary of DataFrames.\")\n",
    "\n",
    "        if len(self.loader_dfs) == 0:\n",
    "            raise ValueError(\"Input dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Input datasets validated successfully.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def transform(self) -> dict:\n",
    "        \"\"\"\n",
    "        Convert object columns containing 'date'\n",
    "        to datetime64[ns] in a NEW dictionary.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            New transformed dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # Deep copy to avoid mutating original\n",
    "        transformed_dfs = {\n",
    "            name: df.copy(deep=True)\n",
    "            for name, df in self.loader_dfs.items()\n",
    "        }\n",
    "\n",
    "        for dataset_name, df in transformed_dfs.items():\n",
    "\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                self.logger.warning(f\"{dataset_name} is not a DataFrame. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            for col in df.columns:\n",
    "\n",
    "                if (\n",
    "                    \"date\" in col.lower()\n",
    "                    and df[col].dtype == \"object\"\n",
    "                ):\n",
    "                    self.logger.info(\n",
    "                        f\"Converting '{col}' in '{dataset_name}' to datetime64[ns]\"\n",
    "                    )\n",
    "\n",
    "                    df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "        self.logger.info(\"Date conversion completed successfully.\")\n",
    "\n",
    "        return transformed_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43796352-dcd1-4ee0-b264-34882aca8274",
   "metadata": {},
   "source": [
    "# Date Type Converter Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52013236-0669-4d00-b496-7cc4261de4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_date_type_converter():\n",
    "    \"\"\"\n",
    "    Unit test for DateTypeConverter.\n",
    "\n",
    "    Validates:\n",
    "    - Output is a dictionary\n",
    "    - Original dictionary is not mutated\n",
    "    - Only object columns containing 'date' are converted\n",
    "    - Converted dtype is datetime64[ns]\n",
    "    - Invalid dates are coerced to NaT\n",
    "    \"\"\"\n",
    "\n",
    "    # Arrange – mock datasets\n",
    "    original_dfs = {\n",
    "        \"dataset_a\": pd.DataFrame({\n",
    "            \"order_date\": [\"2024-01-01\", \"invalid_date\"],  # should convert\n",
    "            \"shipDate\": [\"2024-02-01\", \"2024-02-02\"],      # should convert\n",
    "            \"created_date\": pd.to_datetime([\"2023-01-01\", \"2023-01-02\"]),  # already datetime\n",
    "            \"price\": [\"100\", \"200\"]  # should NOT convert\n",
    "        })\n",
    "    }\n",
    "\n",
    "    converter = DateTypeConverter(original_dfs)\n",
    "\n",
    "    # Act\n",
    "    transformed_dfs = converter.transform()\n",
    "\n",
    "    # Assert – tipo de retorno\n",
    "    assert isinstance(transformed_dfs, dict), \"Output is not a dictionary\"\n",
    "\n",
    "    # Assert – original não foi alterado\n",
    "    assert original_dfs[\"dataset_a\"][\"order_date\"].dtype == object, \\\n",
    "        \"Original dictionary was mutated\"\n",
    "\n",
    "    # Assert – conversão correta\n",
    "    transformed_df = transformed_dfs[\"dataset_a\"]\n",
    "\n",
    "    assert pd.api.types.is_datetime64_any_dtype(transformed_df[\"order_date\"]), \\\n",
    "        \"order_date was not converted to datetime\"\n",
    "\n",
    "    assert pd.api.types.is_datetime64_any_dtype(transformed_df[\"shipDate\"]), \\\n",
    "        \"shipDate was not converted to datetime\"\n",
    "\n",
    "    # Assert – invalid date virou NaT\n",
    "    assert pd.isna(transformed_df[\"order_date\"].iloc[1]), \\\n",
    "        \"Invalid date was not coerced to NaT\"\n",
    "\n",
    "    # Assert – coluna já datetime permanece datetime\n",
    "    assert pd.api.types.is_datetime64_any_dtype(transformed_df[\"created_date\"]), \\\n",
    "        \"created_date should remain datetime\"\n",
    "\n",
    "    # Assert – coluna sem 'date' não foi alterada\n",
    "    assert transformed_df[\"price\"].dtype == object, \\\n",
    "        \"Non-date column was incorrectly converted\"\n",
    "\n",
    "    print(\"DateTypeConverter test passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9762d-9b86-428f-ab18-2de63f6d9892",
   "metadata": {},
   "source": [
    "# Test Execution - Date Type Converter Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aacc2737-9b73-40c7-94d8-7a7b6617aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:03:53,003 | INFO | Input datasets validated successfully.\n",
      "2026-02-25 15:03:53,005 | INFO | Converting 'order_date' in 'dataset_a' to datetime64[ns]\n",
      "2026-02-25 15:03:53,011 | INFO | Converting 'shipDate' in 'dataset_a' to datetime64[ns]\n",
      "2026-02-25 15:03:53,015 | INFO | Date conversion completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTypeConverter test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_date_type_converter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b413e8-084a-4aa2-85eb-0dcb0754073c",
   "metadata": {},
   "source": [
    "# Date Type Converter Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba1841ac-3d67-4071-ad2b-43bad260baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:03:55,128 | INFO | Input datasets validated successfully.\n",
      "2026-02-25 15:03:55,137 | INFO | Converting 'date' in 'sales_dataset' to datetime64[ns]\n",
      "2026-02-25 15:03:55,143 | INFO | Converting 'date' in 'extra_data_dataset' to datetime64[ns]\n",
      "2026-02-25 15:03:55,148 | INFO | Converting 'order_date' in 'fact_sales_dataset' to datetime64[ns]\n",
      "2026-02-25 15:03:55,166 | INFO | Converting 'shipping_date' in 'fact_sales_dataset' to datetime64[ns]\n",
      "2026-02-25 15:03:55,183 | INFO | Converting 'due_date' in 'fact_sales_dataset' to datetime64[ns]\n",
      "2026-02-25 15:03:55,197 | INFO | Converting 'birthdate' in 'dim_customers_dataset' to datetime64[ns]\n",
      "2026-02-25 15:03:55,220 | INFO | Converting 'create_date' in 'dim_customers_dataset' to datetime64[ns]\n",
      "2026-02-25 15:03:55,229 | INFO | Converting 'start_date' in 'dim_products_dataset' to datetime64[ns]\n",
      "2026-02-25 15:03:55,233 | INFO | Date conversion completed successfully.\n"
     ]
    }
   ],
   "source": [
    "converter = DateTypeConverter(loader_dfs)\n",
    "transformed_dfs = converter.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab6bef-5f55-44e6-a228-233e648cf6f5",
   "metadata": {},
   "source": [
    "# Date Column Profiling Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f41e12f2-86d1-40ef-8ab7-fdb5b79d8772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:05:45,415 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:05:45,418 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:05:45,419 | INFO | Date columns export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\date_columns\n",
      "2026-02-25 15:05:45,426 | INFO | Date columns report exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\date_columns\\date_columns_report_after_transformation.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sales_dataset</td>\n",
       "      <td>date</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>extra_data_dataset</td>\n",
       "      <td>date</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fact_sales_dataset</td>\n",
       "      <td>order_date</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fact_sales_dataset</td>\n",
       "      <td>shipping_date</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fact_sales_dataset</td>\n",
       "      <td>due_date</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dim_customers_dataset</td>\n",
       "      <td>birthdate</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dim_customers_dataset</td>\n",
       "      <td>create_date</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dim_products_dataset</td>\n",
       "      <td>start_date</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dataset_name    column_name           dtype\n",
       "0          sales_dataset           date  datetime64[ns]\n",
       "1     extra_data_dataset           date  datetime64[ns]\n",
       "2     fact_sales_dataset     order_date  datetime64[ns]\n",
       "3     fact_sales_dataset  shipping_date  datetime64[ns]\n",
       "4     fact_sales_dataset       due_date  datetime64[ns]\n",
       "5  dim_customers_dataset      birthdate  datetime64[ns]\n",
       "6  dim_customers_dataset    create_date  datetime64[ns]\n",
       "7   dim_products_dataset     start_date  datetime64[ns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler = DateColumnProfiler(transformed_dfs)\n",
    "# Antes da transformação\n",
    "profiler.export_after_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3134c-88e5-4f69-8dec-e59a33e455af",
   "metadata": {},
   "source": [
    "# Cost Column Profiling Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9552016a-c2c4-4679-9d9a-bd467ed163a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class CostColumnProfiler:\n",
    "    \"\"\"\n",
    "    CostColumnProfiler\n",
    "\n",
    "    Responsible for:\n",
    "    - Identifying columns containing 'cost'\n",
    "    - Extracting dataset_name, column_name and dtype\n",
    "    - Exporting reports (before / after transformation)\n",
    "    - Preventing overwrite\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict):\n",
    "        self.logger = self._configure_logger()\n",
    "        self.datasets = datasets\n",
    "        self.base_dir = self._find_project_root()\n",
    "\n",
    "        self.export_folder = self.base_dir / \"data\" / \"files\" / \"cost_columns\"\n",
    "        self.export_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self._validate_input()\n",
    "\n",
    "        self.logger.info(f\"Cost columns export folder set to: {self.export_folder}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self):\n",
    "        logger = logging.getLogger(\"CostColumnProfiler\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _find_project_root(self):\n",
    "        current_path = Path().resolve()\n",
    "\n",
    "        while not (current_path / \"data\").exists() and current_path != current_path.parent:\n",
    "            current_path = current_path.parent\n",
    "\n",
    "        self.logger.info(f\"Project root located at: {current_path}\")\n",
    "        return current_path\n",
    "\n",
    "    def _validate_input(self):\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Datasets validated successfully.\")\n",
    "\n",
    "    def _extract_cost_columns(self) -> pd.DataFrame:\n",
    "        results = []\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                self.logger.warning(f\"{dataset_name} is not a DataFrame. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            cost_columns = [\n",
    "                col for col in df.columns\n",
    "                if \"cost\" in col.lower()\n",
    "            ]\n",
    "\n",
    "            for col in cost_columns:\n",
    "                results.append({\n",
    "                    \"dataset_name\": dataset_name,\n",
    "                    \"column_name\": col,\n",
    "                    \"dtype\": str(df[col].dtype)\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def _export(self, suffix: str):\n",
    "        \"\"\"\n",
    "        Internal export method.\n",
    "        Prevents overwriting existing files.\n",
    "        \"\"\"\n",
    "\n",
    "        filename = f\"cost_columns_report_{suffix}.csv\"\n",
    "        output_path = self.export_folder / filename\n",
    "\n",
    "        # 🔒 Prevent overwrite\n",
    "        if output_path.exists():\n",
    "            self.logger.warning(\n",
    "                f\"File already exists and will NOT be overwritten: {output_path}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        result_df = self._extract_cost_columns()\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"Cost columns report exported to: {output_path}\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def export_before_transformation(self):\n",
    "        \"\"\"\n",
    "        Export cost columns report before transformation.\n",
    "        \"\"\"\n",
    "        return self._export(\"before_transformation\")\n",
    "\n",
    "    def export_after_transformation(self):\n",
    "        \"\"\"\n",
    "        Export cost columns report after transformation.\n",
    "        \"\"\"\n",
    "        return self._export(\"after_transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed0365-83c4-4776-87dd-48a8a3da448f",
   "metadata": {},
   "source": [
    "# Cost Column Profiling Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51e82a13-e90c-45ed-96d6-6b94e4699794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def test_cost_column_profiler():\n",
    "    print(\"🔎 Iniciando teste do CostColumnProfiler...\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 1️⃣ Criar dataset de teste\n",
    "    # ==========================================================\n",
    "    df_test = pd.DataFrame({\n",
    "        \"id\": [1, 2, 3],\n",
    "        \"total_cost\": [100.0, 200.0, 300.0],\n",
    "        \"description\": [\"A\", \"B\", \"C\"]\n",
    "    })\n",
    "\n",
    "    datasets = {\"test_dataset\": df_test}\n",
    "    profiler = CostColumnProfiler(datasets)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 2️⃣ Preparar caminhos com prefixo test_\n",
    "    # ==========================================================\n",
    "    before_path = profiler.export_folder / \"test_cost_columns_report_before_transformation.csv\"\n",
    "    after_path = profiler.export_folder / \"test_cost_columns_report_after_transformation.csv\"\n",
    "\n",
    "    # Limpar arquivos antigos\n",
    "    if before_path.exists():\n",
    "        before_path.unlink()\n",
    "\n",
    "    if after_path.exists():\n",
    "        after_path.unlink()\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3️⃣ Monkey patch para adicionar prefixo test_\n",
    "    # ==========================================================\n",
    "    def _export_with_test_prefix(self, suffix: str):\n",
    "        filename = f\"test_cost_columns_report_{suffix}.csv\"\n",
    "        output_path = self.export_folder / filename\n",
    "\n",
    "        if output_path.exists():\n",
    "            self.logger.warning(\n",
    "                f\"File already exists and will NOT be overwritten: {output_path}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        result_df = self._extract_cost_columns()\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        return result_df\n",
    "\n",
    "    profiler._export = _export_with_test_prefix.__get__(profiler)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 4️⃣ Testar BEFORE\n",
    "    # ==========================================================\n",
    "    result_before = profiler.export_before_transformation()\n",
    "\n",
    "    assert before_path.exists(), \"❌ Arquivo before_transformation não foi criado.\"\n",
    "    assert isinstance(result_before, pd.DataFrame), \"❌ Retorno inválido.\"\n",
    "    assert \"total_cost\" in result_before[\"column_name\"].values, \\\n",
    "        \"❌ Coluna cost não detectada.\"\n",
    "\n",
    "    print(\"✅ Export before_transformation funcionando.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 5️⃣ Testar bloqueio de sobrescrita\n",
    "    # ==========================================================\n",
    "    result_before_again = profiler.export_before_transformation()\n",
    "\n",
    "    assert result_before_again is None, \\\n",
    "        \"❌ Arquivo foi sobrescrito quando não deveria.\"\n",
    "\n",
    "    print(\"✅ Bloqueio de sobrescrita funcionando.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # 6️⃣ Testar AFTER\n",
    "    # ==========================================================\n",
    "    result_after = profiler.export_after_transformation()\n",
    "\n",
    "    assert after_path.exists(), \"❌ Arquivo after_transformation não foi criado.\"\n",
    "    assert isinstance(result_after, pd.DataFrame), \"❌ Retorno inválido.\"\n",
    "\n",
    "    print(\"✅ Export after_transformation funcionando.\")\n",
    "\n",
    "    print(\"\\n🎯 TODOS OS TESTES PASSARAM COM SUCESSO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12cf5a-9129-4466-9c91-f8a41f545637",
   "metadata": {},
   "source": [
    "# Test Execution - Cost Column Profiling Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "313c8820-b2c3-405e-a950-6956ebd3ed6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:07:36,003 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:07:36,006 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:07:36,007 | INFO | Cost columns export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\cost_columns\n",
      "2026-02-25 15:07:36,013 | WARNING | File already exists and will NOT be overwritten: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\cost_columns\\test_cost_columns_report_before_transformation.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Iniciando teste do CostColumnProfiler...\n",
      "✅ Export before_transformation funcionando.\n",
      "✅ Bloqueio de sobrescrita funcionando.\n",
      "✅ Export after_transformation funcionando.\n",
      "\n",
      "🎯 TODOS OS TESTES PASSARAM COM SUCESSO!\n"
     ]
    }
   ],
   "source": [
    "test_cost_column_profiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00159876-0b4f-4870-b60c-f1512054331c",
   "metadata": {},
   "source": [
    "# Cost Column Profiling Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7d3b458-195d-40ba-9f30-81d44599d72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:06:23,316 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:06:23,318 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:06:23,319 | INFO | Cost columns export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\cost_columns\n",
      "2026-02-25 15:06:23,326 | INFO | Cost columns report exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\cost_columns\\cost_columns_report_before_transformation.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sales_dataset</td>\n",
       "      <td>cost</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>extra_data_dataset</td>\n",
       "      <td>cost</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dim_products_dataset</td>\n",
       "      <td>cost</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset_name column_name    dtype\n",
       "0         sales_dataset        cost   object\n",
       "1    extra_data_dataset        cost  float64\n",
       "2  dim_products_dataset        cost    int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler = CostColumnProfiler(transformed_dfs)\n",
    "\n",
    "# Antes da transformação\n",
    "profiler.export_before_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a5924-062c-4081-a0e1-bfbfcfe0c915",
   "metadata": {},
   "source": [
    "# Cost Type Converter Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81b008bb-87fd-4bc4-bba6-a8327bce17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostTypeConverter:\n",
    "    \"\"\"\n",
    "    CostTypeConverter\n",
    "\n",
    "    Responsible for:\n",
    "    - Identifying non-numeric columns containing 'cost'\n",
    "    - Converting them safely to float\n",
    "    - Returning a new transformed dictionary (no mutation)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transformed_dfs: dict):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        transformed_dfs : dict\n",
    "            Dictionary containing dataset_name -> DataFrame\n",
    "        \"\"\"\n",
    "        self.logger = self._configure_logger()\n",
    "        self.transformed_dfs = transformed_dfs\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self):\n",
    "        logger = logging.getLogger(\"CostTypeConverter\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _validate_input(self):\n",
    "        if not isinstance(self.transformed_dfs, dict):\n",
    "            raise TypeError(\"Input must be a dictionary of DataFrames.\")\n",
    "\n",
    "        if len(self.transformed_dfs) == 0:\n",
    "            raise ValueError(\"Input dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Input datasets validated successfully.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def transform(self) -> dict:\n",
    "        \"\"\"\n",
    "        Convert non-numeric 'cost' columns to float.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            New dictionary with transformed DataFrames.\n",
    "        \"\"\"\n",
    "\n",
    "        # Deep copy to avoid mutating original\n",
    "        new_dfs = {\n",
    "            name: df.copy(deep=True)\n",
    "            for name, df in self.transformed_dfs.items()\n",
    "        }\n",
    "\n",
    "        for dataset_name, df in new_dfs.items():\n",
    "\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                self.logger.warning(f\"{dataset_name} is not a DataFrame. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            for col in df.columns:\n",
    "\n",
    "                if \"cost\" in col.lower():\n",
    "\n",
    "                    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "\n",
    "                        self.logger.info(\n",
    "                            f\"Converting '{col}' in '{dataset_name}' to float\"\n",
    "                        )\n",
    "\n",
    "                        df[col] = pd.to_numeric(\n",
    "                            df[col],\n",
    "                            errors=\"coerce\"\n",
    "                        ).astype(float)\n",
    "\n",
    "        self.logger.info(\"Cost type conversion completed successfully.\")\n",
    "\n",
    "        return new_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd537e0-ea6a-4404-b6b6-f1bf9cbcc521",
   "metadata": {},
   "source": [
    "# Cost Type Converter Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53beedc8-d1fb-48f1-a32e-9c0ce9177704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cost_type_converter():\n",
    "    \"\"\"\n",
    "    Unit test for CostTypeConverter.\n",
    "\n",
    "    Validates:\n",
    "    - Output is a dictionary\n",
    "    - Original dictionary is not mutated\n",
    "    - Only non-numeric 'cost' columns are converted\n",
    "    - Converted dtype is float\n",
    "    \"\"\"\n",
    "\n",
    "    # Arrange – mock dataset\n",
    "    original_dfs = {\n",
    "        \"dataset_a\": pd.DataFrame({\n",
    "            \"product_cost\": [\"100\", \"200\"],      # should convert\n",
    "            \"shippingCost\": [\"5.5\", \"7.5\"],      # should convert\n",
    "            \"already_cost\": [10.0, 20.0],        # already numeric\n",
    "            \"price\": [\"300\", \"400\"]              # should NOT convert\n",
    "        })\n",
    "    }\n",
    "\n",
    "    converter = CostTypeConverter(original_dfs)\n",
    "\n",
    "    # Act\n",
    "    converted_dfs = converter.transform()\n",
    "\n",
    "    # Assert – tipo de retorno\n",
    "    assert isinstance(converted_dfs, dict), \"Output is not a dictionary\"\n",
    "\n",
    "    # Assert – original não foi alterado\n",
    "    assert original_dfs[\"dataset_a\"][\"product_cost\"].dtype == object, \\\n",
    "        \"Original dictionary was mutated\"\n",
    "\n",
    "    # Assert – colunas convertidas\n",
    "    converted_df = converted_dfs[\"dataset_a\"]\n",
    "\n",
    "    assert pd.api.types.is_float_dtype(converted_df[\"product_cost\"]), \\\n",
    "        \"product_cost was not converted to float\"\n",
    "\n",
    "    assert pd.api.types.is_float_dtype(converted_df[\"shippingCost\"]), \\\n",
    "        \"shippingCost was not converted to float\"\n",
    "\n",
    "    # Assert – coluna já numérica permanece float\n",
    "    assert pd.api.types.is_float_dtype(converted_df[\"already_cost\"]), \\\n",
    "        \"already_cost should remain float\"\n",
    "\n",
    "    # Assert – coluna sem 'cost' não foi alterada\n",
    "    assert converted_df[\"price\"].dtype == object, \\\n",
    "        \"Non-cost column was incorrectly converted\"\n",
    "\n",
    "    print(\"CostTypeConverter test passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ab826-cd4d-4ce8-ad8d-169afaf5ec7e",
   "metadata": {},
   "source": [
    "# Test Execution - Cost Type Converter Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75a4fe8d-12ee-43a1-ba58-75d5dbb582c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:07:53,239 | INFO | Input datasets validated successfully.\n",
      "2026-02-25 15:07:53,241 | INFO | Converting 'product_cost' in 'dataset_a' to float\n",
      "2026-02-25 15:07:53,243 | INFO | Converting 'shippingCost' in 'dataset_a' to float\n",
      "2026-02-25 15:07:53,247 | INFO | Cost type conversion completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CostTypeConverter test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_cost_type_converter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0955d-4c37-4a92-93a4-a530910e8207",
   "metadata": {},
   "source": [
    "# Cost Type Converter Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61679043-e6e4-42d3-8b5e-a80226d25a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:07:57,127 | INFO | Input datasets validated successfully.\n",
      "2026-02-25 15:07:57,139 | INFO | Converting 'cost' in 'sales_dataset' to float\n",
      "2026-02-25 15:07:57,142 | INFO | Cost type conversion completed successfully.\n"
     ]
    }
   ],
   "source": [
    "converter = CostTypeConverter(transformed_dfs)\n",
    "converted_dfs = converter.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa103f5-3367-4796-86f6-b91ce421c850",
   "metadata": {},
   "source": [
    "# Cost Column Profiling Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fecb30fd-95a9-44b9-8af2-a467959a73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 15:08:38,218 | INFO | Project root located at: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\n",
      "2026-02-25 15:08:38,220 | INFO | Datasets validated successfully.\n",
      "2026-02-25 15:08:38,221 | INFO | Cost columns export folder set to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\cost_columns\n",
      "2026-02-25 15:08:38,227 | INFO | Cost columns report exported to: C:\\Users\\User\\Desktop\\project-exploratory-data-analysis-framework\\data\\files\\cost_columns\\cost_columns_report_after_transformation.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sales_dataset</td>\n",
       "      <td>cost</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>extra_data_dataset</td>\n",
       "      <td>cost</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dim_products_dataset</td>\n",
       "      <td>cost</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset_name column_name    dtype\n",
       "0         sales_dataset        cost  float64\n",
       "1    extra_data_dataset        cost  float64\n",
       "2  dim_products_dataset        cost    int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler = CostColumnProfiler(converted_dfs)\n",
    "\n",
    "# Antes da transformação\n",
    "profiler.export_after_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac735af-8d58-4251-b69c-8d41e5f2c76b",
   "metadata": {},
   "source": [
    "# Duplicated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07637533-d4ac-4c01-9a20-b9580c59bbb8",
   "metadata": {},
   "source": [
    "# Duplicate Removal Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a872ba-07e4-41c2-9e79-2ea23df100e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateRemover:\n",
    "    \"\"\"\n",
    "    DuplicateRemover\n",
    "\n",
    "    Responsible for:\n",
    "    - Removing duplicated rows from each dataset\n",
    "    - Preserving original datasets (no mutation)\n",
    "    - Logging duplicate removal operations\n",
    "    - Returning a new cleaned dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        datasets : dict\n",
    "            Dictionary containing dataset_name -> DataFrame\n",
    "        \"\"\"\n",
    "        self.logger = self._configure_logger()\n",
    "        self.datasets = datasets\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        \"\"\"\n",
    "        Configure structured logger.\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(\"DuplicateRemover\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _validate_input(self):\n",
    "        \"\"\"\n",
    "        Validate input datasets.\n",
    "        \"\"\"\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Input must be a dictionary of DataFrames.\")\n",
    "\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Input dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Datasets validated successfully.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def transform(self) -> dict:\n",
    "        \"\"\"\n",
    "        Remove duplicated rows from all datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            New dictionary with duplicates removed.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create new dictionary to avoid mutating original\n",
    "        cleaned_datasets = {}\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                self.logger.warning(\n",
    "                    f\"{dataset_name} is not a DataFrame. Skipping.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Store original row count\n",
    "            original_rows = df.shape[0]\n",
    "\n",
    "            # Remove duplicates\n",
    "            df_no_duplicates = df.drop_duplicates().copy()\n",
    "\n",
    "            # Calculate removed rows\n",
    "            new_rows = df_no_duplicates.shape[0]\n",
    "            removed_rows = original_rows - new_rows\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"{dataset_name} | \"\n",
    "                f\"Original Rows: {original_rows} | \"\n",
    "                f\"Removed: {removed_rows} | \"\n",
    "                f\"Final Rows: {new_rows}\"\n",
    "            )\n",
    "\n",
    "            cleaned_datasets[dataset_name] = df_no_duplicates\n",
    "\n",
    "        self.logger.info(\"Duplicate removal process completed successfully.\")\n",
    "\n",
    "        return cleaned_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebd90e-88a4-4f07-a7b8-6343bb2b3bcb",
   "metadata": {},
   "source": [
    "# Duplicate Removal Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b22f3-3360-43aa-904c-2453309b58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_duplicate_remover():\n",
    "    \"\"\"\n",
    "    Unit test for DuplicateRemover.\n",
    "\n",
    "    Validates:\n",
    "    - Output is a dictionary\n",
    "    - Original dictionary is not mutated\n",
    "    - Duplicates are removed correctly\n",
    "    - Non-DataFrame entries are skipped safely\n",
    "    \"\"\"\n",
    "\n",
    "    # Arrange – mock datasets\n",
    "    original_datasets = {\n",
    "        \"dataset_a\": pd.DataFrame({\n",
    "            \"id\": [1, 1, 2, 3],\n",
    "            \"value\": [\"A\", \"A\", \"B\", \"C\"]\n",
    "        }),\n",
    "        \"dataset_b\": pd.DataFrame({\n",
    "            \"id\": [10, 20, 20],\n",
    "            \"amount\": [100, 200, 200]\n",
    "        }),\n",
    "        \"invalid_entry\": \"not_a_dataframe\"\n",
    "    }\n",
    "\n",
    "    remover = DuplicateRemover(original_datasets)\n",
    "\n",
    "    # Act\n",
    "    cleaned_datasets = remover.transform()\n",
    "\n",
    "    # Assert – tipo de retorno\n",
    "    assert isinstance(cleaned_datasets, dict), \"Output is not a dictionary\"\n",
    "\n",
    "    # Assert – original não foi alterado\n",
    "    assert original_datasets[\"dataset_a\"].shape[0] == 4, \\\n",
    "        \"Original dataset was mutated\"\n",
    "\n",
    "    # Assert – duplicatas removidas corretamente\n",
    "    assert cleaned_datasets[\"dataset_a\"].shape[0] == 3, \\\n",
    "        \"Duplicates were not removed correctly in dataset_a\"\n",
    "\n",
    "    assert cleaned_datasets[\"dataset_b\"].shape[0] == 2, \\\n",
    "        \"Duplicates were not removed correctly in dataset_b\"\n",
    "\n",
    "    # 🔹 Assert – invalid entry não foi incluído\n",
    "    assert \"invalid_entry\" not in cleaned_datasets, \\\n",
    "        \"Non-DataFrame entry should not be included\"\n",
    "\n",
    "    print(\"DuplicateRemover test passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff920d1-c989-4ff9-823b-0a40fc0cd7b6",
   "metadata": {},
   "source": [
    "# Test Execution - Duplicate Removal Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfca1d-b093-43c3-abed-01bd652e2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_duplicate_remover()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808996c1-49a6-4ebc-b546-afcd09aecedb",
   "metadata": {},
   "source": [
    "# Duplicate Removal Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c25fa-2f51-4459-9240-052d8a842b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = DuplicateRemover(converted_dfs)\n",
    "clean_dfs = remover.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed47790-c419-4f0e-acbd-b43b0407d101",
   "metadata": {},
   "source": [
    "# MISSING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ba014-e7d5-4773-97d1-9fc93671680d",
   "metadata": {},
   "source": [
    "# Missing Values Analyzer Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c91945-6183-4c88-9cfd-9d9789a0bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "missing_analyzer.py\n",
    "\n",
    "Module responsible for:\n",
    "- Calculating missing value ratio per column\n",
    "- Creating output directory structure if necessary\n",
    "- Exporting missing value reports to CSV\n",
    "- Supporting optional filename suffix\n",
    "- Logging dataset analysis operations\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class MissingValueAnalyzer:\n",
    "    \"\"\"\n",
    "    MissingValueAnalyzer\n",
    "\n",
    "    Responsible for:\n",
    "    - Calculating missing value ratios for multiple datasets\n",
    "    - Creating directory structure if it does not exist\n",
    "    - Exporting results to individual CSV files\n",
    "    - Supporting export with optional suffix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict[str, pd.DataFrame]):\n",
    "\n",
    "        self.datasets = datasets\n",
    "        self.project_root = Path().resolve().parent\n",
    "        self.output_path = self.project_root / \"data\" / \"files\" / \"missing_data\"\n",
    "\n",
    "        self._configure_logging()\n",
    "        self._create_output_directory()\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logging(self) -> None:\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "        )\n",
    "\n",
    "    def _create_output_directory(self) -> None:\n",
    "        if not self.output_path.exists():\n",
    "            self.output_path.mkdir(parents=True, exist_ok=True)\n",
    "            logging.info(f\"Directory created: {self.output_path}\")\n",
    "        else:\n",
    "            logging.info(f\"Directory already exists: {self.output_path}\")\n",
    "\n",
    "    def _validate_input(self) -> None:\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "        logging.info(\"Datasets validated successfully.\")\n",
    "\n",
    "    def _export_with_suffix(self, suffix: str) -> None:\n",
    "        \"\"\"\n",
    "        Internal reusable export logic.\n",
    "        \"\"\"\n",
    "\n",
    "        for name, df in self.datasets.items():\n",
    "\n",
    "            logging.info(f\"Analyzing dataset: {name}\")\n",
    "\n",
    "            report_df = self.calculate_missing_ratio(df)\n",
    "\n",
    "            filename = f\"missing_report_{name}{suffix}.csv\"\n",
    "            filepath = self.output_path / filename\n",
    "\n",
    "            report_df.to_csv(filepath, index=False)\n",
    "\n",
    "            logging.info(f\"Report exported successfully: {filepath}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def calculate_missing_ratio(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate missing value ratio per column.\n",
    "        \"\"\"\n",
    "\n",
    "        if df.empty:\n",
    "            logging.warning(\"Received empty DataFrame.\")\n",
    "            return pd.DataFrame(columns=[\"column\", \"missing_ratio\"])\n",
    "\n",
    "        missing_ratio = (df.isna().sum() / df.shape[0]).sort_values()\n",
    "\n",
    "        result_df = missing_ratio.reset_index()\n",
    "        result_df.columns = [\"column\", \"missing_ratio\"]\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def analyze_and_export(self) -> None:\n",
    "        \"\"\"\n",
    "        Export default CSV files:\n",
    "        - missing_report_<dataset>.csv\n",
    "        \"\"\"\n",
    "        self._export_with_suffix(suffix=\"\")\n",
    "\n",
    "    def analyze_and_export_with_suffix(self, suffix: str) -> None:\n",
    "        \"\"\"\n",
    "        Export CSV files adding a suffix at the end of filename.\n",
    "\n",
    "        Example:\n",
    "        missing_report_sales_v2.csv\n",
    "        \"\"\"\n",
    "        if not suffix:\n",
    "            raise ValueError(\"Suffix cannot be empty.\")\n",
    "\n",
    "        self._export_with_suffix(suffix=f\"_{suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec3e42-e465-4164-bcd6-cbc42c8b0e1c",
   "metadata": {},
   "source": [
    "# Missing Values Analyzer Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ed7b7-e4ef-48b2-9a20-0f8dcd9f9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_missing_value_analyzer():\n",
    "    \"\"\"\n",
    "    Simple unit test for MissingValueAnalyzer.\n",
    "\n",
    "    Validates:\n",
    "    - Directory creation\n",
    "    - Default CSV export\n",
    "    - Export with suffix\n",
    "    - Structure of exported CSV\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Criando DataFrame de teste\n",
    "    df = pd.DataFrame({\n",
    "        \"A\": [1, None, 3],\n",
    "        \"B\": [None, None, 3]\n",
    "    })\n",
    "\n",
    "    datasets = {\"test_dataset\": df}\n",
    "\n",
    "    # Ambiente isolado\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "\n",
    "        temp_path = Path(temp_dir)\n",
    "\n",
    "        # Criando estrutura mínima\n",
    "        (temp_path / \"data\").mkdir()\n",
    "\n",
    "        analyzer = MissingValueAnalyzer(datasets)\n",
    "\n",
    "        # Sobrescrevendo caminhos para não afetar projeto real\n",
    "        analyzer.project_root = temp_path\n",
    "        analyzer.output_path = temp_path / \"data\" / \"files\" / \"missing_data\"\n",
    "        analyzer.output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Teste export padrão\n",
    "        analyzer.analyze_and_export()\n",
    "\n",
    "        default_file = analyzer.output_path / \"missing_report_test_dataset.csv\"\n",
    "        assert default_file.exists(), \"Default CSV was not created.\"\n",
    "\n",
    "        exported_df = pd.read_csv(default_file)\n",
    "        assert \"column\" in exported_df.columns\n",
    "        assert \"missing_ratio\" in exported_df.columns\n",
    "\n",
    "        # Teste export com sufixo\n",
    "        analyzer.analyze_and_export_with_suffix(\"v2\")\n",
    "\n",
    "        suffix_file = analyzer.output_path / \"missing_report_test_dataset_v2.csv\"\n",
    "        assert suffix_file.exists(), \"Suffix CSV was not created.\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efada9b-00f0-47f5-b4b3-a04aed894b2b",
   "metadata": {},
   "source": [
    "# Test Execution - Missing Values Analyzer Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250d733-1ce6-4692-866c-79c44dfe6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_missing_value_analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b96db-252f-4e52-ac7d-ef041059ce13",
   "metadata": {},
   "source": [
    "# Missing Values Analyzer Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32cb2f1-87ed-4df6-9904-ef6e176e8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = MissingValueAnalyzer(clean_dfs)\n",
    "analyzer.analyze_and_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33feab0-d5d4-4538-bf22-f8a54bc68fc3",
   "metadata": {},
   "source": [
    "# Missing Data Plotter Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ab6bf-1f19-4621-8ce6-e1cc2c82b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "missing_plotter.py\n",
    "\n",
    "Module responsible for:\n",
    "- Reading missing value report CSV files\n",
    "- Generating bar plots for each dataset\n",
    "- Logging operations\n",
    "\"\"\"\n",
    "\n",
    "class MissingDataPlotter:\n",
    "    \"\"\"\n",
    "    MissingDataPlotter\n",
    "\n",
    "    Responsible for:\n",
    "    - Loading missing value CSV reports\n",
    "    - Generating plots for each report\n",
    "    - Handling directory validation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize plotter and configure paths.\n",
    "        \"\"\"\n",
    "        self.project_root = Path().resolve().parent\n",
    "        self.data_path = self.project_root / \"data\" / \"files\" / \"missing_data\"\n",
    "\n",
    "        self._configure_logging()\n",
    "        self._validate_directory()\n",
    "\n",
    "    def _configure_logging(self) -> None:\n",
    "        \"\"\"\n",
    "        Configure logging settings.\n",
    "        \"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "        )\n",
    "\n",
    "    def _validate_directory(self) -> None:\n",
    "        \"\"\"\n",
    "        Validate if directory exists.\n",
    "        \"\"\"\n",
    "        if not self.data_path.exists():\n",
    "            logging.error(f\"Directory not found: {self.data_path}\")\n",
    "            raise FileNotFoundError(\n",
    "                f\"Directory not found: {self.data_path}\"\n",
    "            )\n",
    "\n",
    "    def _load_csv_files(self) -> list[Path]:\n",
    "        \"\"\"\n",
    "        Load all CSV files from directory.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[Path]\n",
    "            List of CSV file paths.\n",
    "        \"\"\"\n",
    "        csv_files = list(self.data_path.glob(\"*.csv\"))\n",
    "\n",
    "        if not csv_files:\n",
    "            logging.warning(\"No CSV files found in directory.\")\n",
    "\n",
    "        return csv_files\n",
    "\n",
    "    def plot_all(self) -> None:\n",
    "        \"\"\"\n",
    "        Generate a bar plot for each CSV file found.\n",
    "        \"\"\"\n",
    "        csv_files = self._load_csv_files()\n",
    "\n",
    "        for file_path in csv_files:\n",
    "            logging.info(f\"Processing file: {file_path.name}\")\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if df.empty:\n",
    "                logging.warning(f\"File is empty: {file_path.name}\")\n",
    "                continue\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            plt.bar(df[\"column\"], df[\"missing_ratio\"])\n",
    "\n",
    "            plt.title(f\"Missing Values Ratio - {file_path.stem}\")\n",
    "            plt.ylabel(\"Proportion of Missing Values\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            logging.info(f\"Plot generated for: {file_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffbd33-770f-4af4-b03d-c6124f2a0fbf",
   "metadata": {},
   "source": [
    "# Missing Data Plotter Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931668d7-fd02-4e7b-87a8-6923bebe4d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_missing_data_plotter():\n",
    "    \"\"\"\n",
    "    Simple unit test for MissingDataPlotter.\n",
    "\n",
    "    Validates:\n",
    "    - Directory detection\n",
    "    - CSV loading\n",
    "    - Plot generation without errors\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    matplotlib.use(\"Agg\")  # evita abrir janela gráfica no teste\n",
    "\n",
    "    # Criando diretório temporário\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "\n",
    "        temp_path = Path(temp_dir)\n",
    "        data_path = temp_path / \"data\" / \"files\" / \"missing_data\"\n",
    "        data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Criando CSV fake\n",
    "        test_csv = data_path / \"missing_report_test.csv\"\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"column\": [\"A\", \"B\", \"C\"],\n",
    "            \"missing_ratio\": [0.1, 0.3, 0.0]\n",
    "        })\n",
    "\n",
    "        df.to_csv(test_csv, index=False)\n",
    "\n",
    "        # Instanciando plotter\n",
    "        plotter = MissingDataPlotter()\n",
    "\n",
    "        # Sobrescrevendo caminhos para não afetar projeto real\n",
    "        plotter.project_root = temp_path\n",
    "        plotter.data_path = data_path\n",
    "\n",
    "        # Verifica se arquivo foi detectado\n",
    "        csv_files = plotter._load_csv_files()\n",
    "        assert len(csv_files) == 1, \"CSV file was not detected.\"\n",
    "\n",
    "        # Executa plot (não deve gerar erro)\n",
    "        plotter.plot_all()\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc789edd-e325-4fe9-9324-5d02bd457c63",
   "metadata": {},
   "source": [
    "# Test Execution - Missing Data Plotter Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496cedc-e07d-4c67-afb4-ebdd4914f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_missing_data_plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e673052-cb65-4ed1-9300-1f514679e988",
   "metadata": {},
   "source": [
    "# Missing Data Plotter Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc836e-e665-4722-92bb-5c5605659712",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = MissingDataPlotter()\n",
    "plotter.plot_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e279f-1312-4fa2-82c8-b0907010b4ee",
   "metadata": {},
   "source": [
    "# Dataset Simple Inputer Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d43276-be22-4046-a8ea-5f4787189fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "class DatasetImputer:\n",
    "\n",
    "    def __init__(self, datasets: dict[str, pd.DataFrame]):\n",
    "        self.datasets = datasets\n",
    "        self.logger = self._configure_logger()\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # LOGGER\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetImputer\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _validate_input(self):\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Datasets validated successfully.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # IMPUTATION LOGIC\n",
    "    # ==========================================================\n",
    "\n",
    "    def impute_all(self) -> dict[str, pd.DataFrame]:\n",
    "\n",
    "        imputed_datasets = {}\n",
    "\n",
    "        for name, df in self.datasets.items():\n",
    "\n",
    "            self.logger.info(f\"Processing dataset: {name}\")\n",
    "\n",
    "            df_imputed = df.copy()\n",
    "\n",
    "            # -------------------------\n",
    "            # NUMERIC (all numeric types)\n",
    "            # -------------------------\n",
    "            numeric_cols = df_imputed.select_dtypes(\n",
    "                include=[\"number\"]\n",
    "            ).columns\n",
    "\n",
    "            for col in numeric_cols:\n",
    "\n",
    "                if df_imputed[col].isna().all():\n",
    "                    df_imputed[col] = 0\n",
    "                    continue\n",
    "\n",
    "                imputer = SimpleImputer(strategy=\"median\")\n",
    "                df_imputed[[col]] = imputer.fit_transform(df_imputed[[col]])\n",
    "\n",
    "                self.logger.info(f\"Numeric column imputed: {col}\")\n",
    "\n",
    "            # -------------------------\n",
    "            # CATEGORICAL\n",
    "            # -------------------------\n",
    "            categorical_cols = df_imputed.select_dtypes(\n",
    "                include=[\"object\", \"category\", \"string\"]\n",
    "            ).columns\n",
    "\n",
    "            for col in categorical_cols:\n",
    "\n",
    "                if df_imputed[col].isna().all():\n",
    "                    df_imputed[col] = \"Unknown\"\n",
    "                    continue\n",
    "\n",
    "                imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "                df_imputed[[col]] = imputer.fit_transform(df_imputed[[col]])\n",
    "\n",
    "                self.logger.info(f\"Categorical column imputed: {col}\")\n",
    "\n",
    "            # -------------------------\n",
    "            # DATETIME (any datetime type)\n",
    "            # -------------------------\n",
    "            datetime_cols = df_imputed.select_dtypes(\n",
    "                include=[\"datetime\"]\n",
    "            ).columns\n",
    "\n",
    "            for col in datetime_cols:\n",
    "\n",
    "                if df_imputed[col].isna().all():\n",
    "                    df_imputed[col] = pd.Timestamp(\"1970-01-01\")\n",
    "                    continue\n",
    "\n",
    "                numeric_dates = df_imputed[col].map(\n",
    "                    lambda x: x.timestamp() if pd.notnull(x) else np.nan\n",
    "                )\n",
    "\n",
    "                imputer = SimpleImputer(strategy=\"median\")\n",
    "                imputed_values = imputer.fit_transform(\n",
    "                    numeric_dates.values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "\n",
    "                df_imputed[col] = pd.to_datetime(imputed_values, unit=\"s\")\n",
    "\n",
    "                self.logger.info(f\"Datetime column imputed: {col}\")\n",
    "\n",
    "            # -------------------------\n",
    "            # FINAL SAFETY CHECK\n",
    "            # -------------------------\n",
    "            remaining_missing = df_imputed.isna().sum().sum()\n",
    "\n",
    "            if remaining_missing > 0:\n",
    "                self.logger.warning(\n",
    "                    f\"{name} still had {remaining_missing} missing values. Applying fallback.\"\n",
    "                )\n",
    "\n",
    "                df_imputed = df_imputed.fillna(0)\n",
    "\n",
    "            else:\n",
    "                self.logger.info(f\"{name} fully imputed successfully.\")\n",
    "\n",
    "            imputed_datasets[name] = df_imputed\n",
    "\n",
    "        self.logger.info(\"All datasets processed successfully.\")\n",
    "        return imputed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291502d-0d9f-4d09-8b0e-bc088f1bbec2",
   "metadata": {},
   "source": [
    "# Dataset Simple Inputer Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dccf581-2ebc-48dd-8956-3de636c9a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_imputer():\n",
    "    \"\"\"\n",
    "    Simple unit test for DatasetImputer.\n",
    "\n",
    "    Validates:\n",
    "    - Numeric imputation\n",
    "    - Categorical imputation\n",
    "    - Datetime imputation\n",
    "    - No remaining missing values\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Criando DataFrame de teste\n",
    "    df = pd.DataFrame({\n",
    "        \"numeric_col\": [1, 2, np.nan, 4],\n",
    "        \"categorical_col\": [\"A\", None, \"A\", \"B\"],\n",
    "        \"order_date\": pd.to_datetime(\n",
    "            [\"2023-01-01\", None, \"2023-01-03\", None]\n",
    "        ),\n",
    "        \"birthdate\": pd.to_datetime(\n",
    "            [None, \"1990-05-01\", None, \"1985-08-10\"]\n",
    "        )\n",
    "    })\n",
    "\n",
    "    datasets = {\"test_dataset\": df}\n",
    "\n",
    "    # Executando imputação\n",
    "    imputer = DatasetImputer(datasets)\n",
    "    imputed = imputer.impute_all()\n",
    "\n",
    "    df_imputed = imputed[\"test_dataset\"]\n",
    "\n",
    "    # Garantir que não há valores faltantes\n",
    "    total_missing = df_imputed.isna().sum().sum()\n",
    "    assert total_missing == 0, \"There are still missing values after imputation.\"\n",
    "\n",
    "    # Garantir que tipos foram preservados\n",
    "    assert pd.api.types.is_datetime64_any_dtype(df_imputed[\"order_date\"]), \\\n",
    "        \"order_date is no longer datetime.\"\n",
    "\n",
    "    assert pd.api.types.is_datetime64_any_dtype(df_imputed[\"birthdate\"]), \\\n",
    "        \"birthdate is no longer datetime.\"\n",
    "\n",
    "    # Garantir que numérica foi imputada com mediana\n",
    "    expected_median = 2.0  # mediana de [1,2,4]\n",
    "    assert expected_median in df_imputed[\"numeric_col\"].values, \\\n",
    "        \"Numeric median imputation failed.\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122cc93-1f6a-4a8f-becf-8baee9c21678",
   "metadata": {},
   "source": [
    "# Test Execution - Dataset Simple Inputer Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc43dc-a16e-4865-8a7c-7e60ccd7a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_imputer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e01bfe-a034-4381-a9ba-c2c492aa8bf8",
   "metadata": {},
   "source": [
    "# Dataset Simple Inputer Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111cca3-9b3f-4d7d-91e3-d8836c6d8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = DatasetImputer(clean_dfs)\n",
    "imputed_dfs = imputer.impute_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f357f9-162d-4508-ad33-7eeedab8fc75",
   "metadata": {},
   "source": [
    "# Dataset Info Profiler Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a4dc8-4439-41d7-897b-70682e49f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_profiler = DatasetInfoProfiler(imputed_dfs)\n",
    "info_results = info_profiler.export_all_infos_with_suffix(\"imputed_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063df08-daf8-41bc-8a57-cb56f42800fd",
   "metadata": {},
   "source": [
    "# Missing Value Analyzer Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c89d3-4cb6-458e-af32-54db19ff02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = MissingValueAnalyzer(imputed_dfs)\n",
    "analyzer.analyze_and_export_with_suffix(\"imputed_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cc644-e6b7-40f5-b68f-95ccdc07410e",
   "metadata": {},
   "source": [
    "# DIMENSION x MEASURE\n",
    "- Dimesion is not a numeric value and If is, it is can not be aggregated\n",
    "- Examples: category, product, birthdate, id\n",
    "- Measure is numeric value and can be aggregated\n",
    "- Examples: sales, quantity, age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a9ff1-a62a-4996-8e8d-b1b049861879",
   "metadata": {},
   "source": [
    "# Dimension Column Profiler Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df6a397-3399-41ca-b9e6-3a80f1d4952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dimension_column_profiler.py\n",
    "\n",
    "Extended functionality:\n",
    "- Identify dimension-like columns\n",
    "- Export dimension structure\n",
    "- Export object unique values\n",
    "- Export object unique value counts\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class DimensionColumnProfiler:\n",
    "\n",
    "    def __init__(self, datasets: dict[str, pd.DataFrame]):\n",
    "\n",
    "        self.datasets = datasets\n",
    "\n",
    "        self.ignored_columns = [\n",
    "            \"cost\",\n",
    "            \"quantity\",\n",
    "            \"order_value_eur\",\n",
    "            \"refund\"\n",
    "        ]\n",
    "\n",
    "        self.project_root = Path().resolve().parent\n",
    "        self.output_path = (\n",
    "            self.project_root /\n",
    "            \"data\" / \"files\" / \"dimensions_columns\"\n",
    "        )\n",
    "\n",
    "        self._configure_logging()\n",
    "        self._create_output_directory()\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logging(self) -> None:\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "        )\n",
    "\n",
    "    def _create_output_directory(self) -> None:\n",
    "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _validate_input(self) -> None:\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # ORIGINAL LOGIC (INTACT)\n",
    "    # ==========================================================\n",
    "\n",
    "    def _build_dimension_rows(self, dataset_name: str, df: pd.DataFrame) -> list[dict]:\n",
    "\n",
    "        result_rows = []\n",
    "\n",
    "        for col in df.columns:\n",
    "\n",
    "            if col.lower() in self.ignored_columns:\n",
    "                continue\n",
    "\n",
    "            dtype = df[col].dtype\n",
    "\n",
    "            if not pd.api.types.is_numeric_dtype(dtype):\n",
    "\n",
    "                result_rows.append({\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"column\": col,\n",
    "                    \"dtype\": str(dtype)\n",
    "                })\n",
    "\n",
    "            else:\n",
    "                unique_count = df[col].nunique()\n",
    "\n",
    "                if unique_count <= 20:\n",
    "                    result_rows.append({\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"column\": col,\n",
    "                        \"dtype\": str(dtype)\n",
    "                    })\n",
    "\n",
    "        return result_rows\n",
    "\n",
    "    def export_all(self) -> dict[str, pd.DataFrame]:\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "\n",
    "            rows = self._build_dimension_rows(dataset_name, df)\n",
    "            result_df = pd.DataFrame(rows)\n",
    "\n",
    "            filename = f\"{dataset_name}_dimension_columns.csv\"\n",
    "            filepath = self.output_path / filename\n",
    "\n",
    "            result_df.to_csv(filepath, index=False)\n",
    "\n",
    "            results[dataset_name] = result_df\n",
    "\n",
    "        return results\n",
    "\n",
    "    # ==========================================================\n",
    "    # NEW FUNCTIONALITY\n",
    "    # ==========================================================\n",
    "\n",
    "    def export_object_unique_values(self) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Export unique values for object columns (excluding ignored).\n",
    "        \"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "\n",
    "            unique_rows = []\n",
    "\n",
    "            for col in df.columns:\n",
    "\n",
    "                if col.lower() in self.ignored_columns:\n",
    "                    continue\n",
    "\n",
    "                if df[col].dtype == \"object\":\n",
    "\n",
    "                    unique_values = df[col].dropna().unique()\n",
    "\n",
    "                    for val in unique_values:\n",
    "                        unique_rows.append({\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"column\": col,\n",
    "                            \"value\": val\n",
    "                        })\n",
    "\n",
    "            result_df = pd.DataFrame(unique_rows)\n",
    "\n",
    "            filename = f\"{dataset_name}_object_unique_values.csv\"\n",
    "            filepath = self.output_path / filename\n",
    "\n",
    "            result_df.to_csv(filepath, index=False)\n",
    "\n",
    "            results[dataset_name] = result_df\n",
    "\n",
    "        return results\n",
    "\n",
    "    def export_object_unique_counts(self) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Count number of unique values per object column\n",
    "        and export one CSV per dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "\n",
    "            count_rows = []\n",
    "\n",
    "            for col in df.columns:\n",
    "\n",
    "                if col.lower() in self.ignored_columns:\n",
    "                    continue\n",
    "\n",
    "                if df[col].dtype == \"object\":\n",
    "\n",
    "                    unique_count = df[col].dropna().nunique()\n",
    "\n",
    "                    count_rows.append({\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"column\": col,\n",
    "                        \"unique_value_count\": unique_count\n",
    "                    })\n",
    "\n",
    "            result_df = pd.DataFrame(count_rows)\n",
    "\n",
    "            filename = f\"{dataset_name}_object_unique_count.csv\"\n",
    "            filepath = self.output_path / filename\n",
    "\n",
    "            result_df.to_csv(filepath, index=False)\n",
    "\n",
    "            results[dataset_name] = result_df\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816bdbba-193e-47fa-a2de-43f4783d6cf1",
   "metadata": {},
   "source": [
    "# Dimension Column Profiler Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6158eff-9f98-460c-a066-b7b9c3bbc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dimension_column_profiler():\n",
    "    \"\"\"\n",
    "    Unit test for DimensionColumnProfiler.\n",
    "\n",
    "    Validates:\n",
    "    - Dimension export\n",
    "    - Object unique values export\n",
    "    - Object unique counts export\n",
    "    - Ignored columns logic\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Criando DataFrame de teste\n",
    "    df = pd.DataFrame({\n",
    "        \"category_col\": [\"A\", \"B\", \"A\", None],\n",
    "        \"numeric_low_card\": [1, 2, 1, 2],   # <=20 únicos\n",
    "        \"numeric_high_card\": [1, 2, 3, 4],  # também <=20 (4 únicos)\n",
    "        \"cost\": [10, 20, 30, 40]            # ignorada\n",
    "    })\n",
    "\n",
    "    datasets = {\"test_dataset\": df}\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "\n",
    "        temp_path = Path(temp_dir)\n",
    "\n",
    "        # Criar estrutura mínima para não quebrar a classe\n",
    "        (temp_path / \"data\").mkdir()\n",
    "\n",
    "        profiler = DimensionColumnProfiler(datasets)\n",
    "\n",
    "        # Sobrescrever caminhos para ambiente isolado\n",
    "        profiler.project_root = temp_path\n",
    "        profiler.output_path = temp_path / \"data\" / \"files\" / \"dimensions_columns\"\n",
    "        profiler.output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Executar as 3 funcionalidades\n",
    "        dim_results = profiler.export_all()\n",
    "        unique_results = profiler.export_object_unique_values()\n",
    "        count_results = profiler.export_object_unique_counts()\n",
    "\n",
    "        # Verificar arquivos criados\n",
    "        assert (profiler.output_path / \"test_dataset_dimension_columns.csv\").exists()\n",
    "        assert (profiler.output_path / \"test_dataset_object_unique_values.csv\").exists()\n",
    "        assert (profiler.output_path / \"test_dataset_object_unique_count.csv\").exists()\n",
    "\n",
    "        # Verificar se coluna ignorada não aparece\n",
    "        dim_df = dim_results[\"test_dataset\"]\n",
    "        assert \"cost\" not in dim_df[\"column\"].values\n",
    "\n",
    "        # Verificar se categoria aparece nos uniques\n",
    "        unique_df = unique_results[\"test_dataset\"]\n",
    "        assert \"category_col\" in unique_df[\"column\"].values\n",
    "\n",
    "        # Verificar contagem correta\n",
    "        count_df = count_results[\"test_dataset\"]\n",
    "        count_val = count_df.loc[\n",
    "            count_df[\"column\"] == \"category_col\",\n",
    "            \"unique_value_count\"\n",
    "        ].values[0]\n",
    "\n",
    "        # Valores únicos em category_col → A, B → 2\n",
    "        assert count_val == 2\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5788cd-043e-4ad7-bc1e-117b2594c262",
   "metadata": {},
   "source": [
    "# Test Execution - Dimension Column Profiler Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214213f4-c0db-451c-83ad-9193a0014526",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dimension_column_profiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f884c-67a5-4597-b342-841844ea470b",
   "metadata": {},
   "source": [
    "# Dimension Column Profiler Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bebbb4-856f-44e6-99d7-a40b9210410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = DimensionColumnProfiler(imputed_dfs)\n",
    "dimension_results = profiler.export_all()\n",
    "unique_values_results = profiler.export_object_unique_values()\n",
    "unique_count_results = profiler.export_object_unique_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423167f3-c0ac-41f6-a604-19f27777531d",
   "metadata": {},
   "source": [
    "# Unique Value Extractor Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada30dc0-1bad-40a2-83ec-bd1f04a508f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "unique_value_extractor.py\n",
    "\n",
    "Module responsible for:\n",
    "- Extracting unique non-null values per column\n",
    "- Exporting one CSV per dataset\n",
    "- Reading exported files and counting unique values per column\n",
    "- Exporting count reports\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class UniqueValueExtractor:\n",
    "\n",
    "    def __init__(self, datasets: dict[str, pd.DataFrame]):\n",
    "\n",
    "        self.datasets = datasets\n",
    "\n",
    "        self.project_root = Path().resolve().parent\n",
    "        self.output_path = (\n",
    "            self.project_root /\n",
    "            \"data\" / \"files\" / \"unique_values\"\n",
    "        )\n",
    "\n",
    "        self.logger = self._configure_logger()\n",
    "\n",
    "        self._create_output_directory()\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"UniqueValueExtractor\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _create_output_directory(self) -> None:\n",
    "        if not self.output_path.exists():\n",
    "            self.output_path.mkdir(parents=True, exist_ok=True)\n",
    "            self.logger.info(f\"Directory created: {self.output_path}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Directory already exists: {self.output_path}\")\n",
    "\n",
    "    def _validate_input(self) -> None:\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "        self.logger.info(\"Datasets validated successfully.\")\n",
    "\n",
    "    def _extract_unique_from_dataset(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        df: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        col_dfs = []\n",
    "\n",
    "        for col in df.columns:\n",
    "\n",
    "            temp_df = pd.DataFrame({\n",
    "                \"column\": col,\n",
    "                \"value\": df[col].dropna().unique()\n",
    "            })\n",
    "\n",
    "            col_dfs.append(temp_df)\n",
    "\n",
    "        if col_dfs:\n",
    "            result_df = pd.concat(col_dfs, ignore_index=True)\n",
    "        else:\n",
    "            result_df = pd.DataFrame(columns=[\"column\", \"value\"])\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    # ==========================================================\n",
    "    # EXPORT ORIGINAL\n",
    "    # ==========================================================\n",
    "\n",
    "    def export_all(self) -> dict[str, pd.DataFrame]:\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for dataset_name, df in self.datasets.items():\n",
    "\n",
    "            self.logger.info(f\"Processing dataset: {dataset_name}\")\n",
    "\n",
    "            result_df = self._extract_unique_from_dataset(\n",
    "                dataset_name,\n",
    "                df\n",
    "            )\n",
    "\n",
    "            filename = f\"{dataset_name}_unique_values.csv\"\n",
    "            filepath = self.output_path / filename\n",
    "\n",
    "            result_df.to_csv(filepath, index=False)\n",
    "\n",
    "            self.logger.info(f\"Exported: {filepath}\")\n",
    "\n",
    "            results[dataset_name] = result_df\n",
    "\n",
    "        self.logger.info(\"All unique value files exported successfully.\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    # ==========================================================\n",
    "    # NOVA FUNCIONALIDADE\n",
    "    # ==========================================================\n",
    "\n",
    "    def export_unique_value_counts(self) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Read each *_unique_values.csv file,\n",
    "        count number of values per column,\n",
    "        and export a new CSV:\n",
    "            <dataset>_unique_value_count.csv\n",
    "        \"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        csv_files = list(self.output_path.glob(\"*_unique_values.csv\"))\n",
    "\n",
    "        for file_path in csv_files:\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if df.empty:\n",
    "                self.logger.warning(f\"{file_path.name} is empty.\")\n",
    "                continue\n",
    "\n",
    "            # Contagem correta por coluna\n",
    "            count_df = (\n",
    "                df.groupby(\"column\")[\"value\"]\n",
    "                .count()\n",
    "                .reset_index()\n",
    "                .rename(columns={\"value\": \"unique_value_count\"})\n",
    "            )\n",
    "\n",
    "            dataset_name = file_path.stem.replace(\n",
    "                \"_unique_values\", \"\"\n",
    "            )\n",
    "\n",
    "            new_filename = f\"{dataset_name}_unique_value_count.csv\"\n",
    "            new_filepath = self.output_path / new_filename\n",
    "\n",
    "            count_df.to_csv(new_filepath, index=False)\n",
    "\n",
    "            self.logger.info(f\"Exported count file: {new_filepath}\")\n",
    "\n",
    "            results[dataset_name] = count_df\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a6b4d-971e-46ae-ba84-dc9f3aebfd4c",
   "metadata": {},
   "source": [
    "# Unique Value Extractor Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdf151-d968-448b-8186-ddf9b0b62ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unique_value_extractor():\n",
    "    \"\"\"\n",
    "    Simple unit test for UniqueValueExtractor.\n",
    "\n",
    "    Validates:\n",
    "    - Unique value export\n",
    "    - Unique value count export\n",
    "    - Correct counting logic per column\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Criando DataFrame de teste\n",
    "    df = pd.DataFrame({\n",
    "        \"A\": [\"x\", \"y\", \"x\", None],\n",
    "        \"B\": [1, 2, 2, 3]\n",
    "    })\n",
    "\n",
    "    datasets = {\"test_dataset\": df}\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "\n",
    "        temp_path = Path(temp_dir)\n",
    "\n",
    "        # Criando estrutura mínima\n",
    "        (temp_path / \"data\").mkdir()\n",
    "\n",
    "        extractor = UniqueValueExtractor(datasets)\n",
    "\n",
    "        # Sobrescrevendo caminho para ambiente isolado\n",
    "        extractor.project_root = temp_path\n",
    "        extractor.output_path = temp_path / \"data\" / \"files\" / \"unique_values\"\n",
    "        extractor.output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Exportar valores únicos\n",
    "        extractor.export_all()\n",
    "\n",
    "        unique_file = extractor.output_path / \"test_dataset_unique_values.csv\"\n",
    "        assert unique_file.exists(), \"Unique values CSV not created.\"\n",
    "\n",
    "        # Exportar contagem\n",
    "        counts = extractor.export_unique_value_counts()\n",
    "\n",
    "        count_file = extractor.output_path / \"test_dataset_unique_value_count.csv\"\n",
    "        assert count_file.exists(), \"Unique value count CSV not created.\"\n",
    "\n",
    "        count_df = counts[\"test_dataset\"]\n",
    "\n",
    "        # Verificar contagem correta\n",
    "        # Coluna A tem valores únicos: x, y → 2\n",
    "        # Coluna B tem valores únicos: 1,2,3 → 3\n",
    "\n",
    "        count_A = count_df.loc[count_df[\"column\"] == \"A\", \"unique_value_count\"].values[0]\n",
    "        count_B = count_df.loc[count_df[\"column\"] == \"B\", \"unique_value_count\"].values[0]\n",
    "\n",
    "        assert count_A == 2, \"Incorrect unique count for column A.\"\n",
    "        assert count_B == 3, \"Incorrect unique count for column B.\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888be98f-11ea-4777-bec0-f5e368170cac",
   "metadata": {},
   "source": [
    "# Test Execution - Unique Value Extractor Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e9513-6caa-4fda-91a5-7d50b2221eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique_value_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f80c3a-e266-4973-b5dc-307556f5ec3d",
   "metadata": {},
   "source": [
    "# Unique Value Extractor Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c93a09-c0bc-48f0-b1ab-d953177f572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = UniqueValueExtractor(imputed_dfs)\n",
    "extractor.export_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e01dee4-091d-49bf-9b1e-5ccd4cab8479",
   "metadata": {},
   "source": [
    "# Unique Value Count Extractor Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6609cee-8fc8-48f1-a5e0-4e05fe1d3581",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_results = extractor.export_unique_value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f574a5b-babc-4311-8831-9f6626e7149a",
   "metadata": {},
   "source": [
    "# Count the Dataset in the Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f46a7-3761-4f24-868d-82b477c8b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imputed_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2141b7bd-ed17-4418-8fb3-e08aca2d92d7",
   "metadata": {},
   "source": [
    "# Dataset Merger Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5681a9-194b-4650-9429-4ac9901d0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMerger:\n",
    "    \"\"\"\n",
    "    DatasetMerger\n",
    "\n",
    "    Responsible for:\n",
    "    - Performing safe merges between datasets\n",
    "    - Validating dataset names and join keys\n",
    "    - Logging operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict[str, pd.DataFrame]):\n",
    "        self.datasets = datasets\n",
    "        self.logger = self._configure_logger()\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetMerger\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _validate_input(self) -> None:\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC\n",
    "    # ==========================================================\n",
    "\n",
    "    def merge_datasets(\n",
    "        self,\n",
    "        left_name: str,\n",
    "        right_name: str,\n",
    "        on: str,\n",
    "        how: str = \"left\",\n",
    "        new_dataset_name: str | None = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Merge two datasets and store result.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        left_name : str\n",
    "        right_name : str\n",
    "        on : str\n",
    "            Column name to merge on\n",
    "        how : str\n",
    "            Type of merge (default: left)\n",
    "        new_dataset_name : str | None\n",
    "            Name to store merged dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Merged DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        if left_name not in self.datasets:\n",
    "            raise KeyError(f\"{left_name} not found.\")\n",
    "\n",
    "        if right_name not in self.datasets:\n",
    "            raise KeyError(f\"{right_name} not found.\")\n",
    "\n",
    "        left_df = self.datasets[left_name]\n",
    "        right_df = self.datasets[right_name]\n",
    "\n",
    "        if on not in left_df.columns:\n",
    "            raise KeyError(f\"{on} not found in {left_name}\")\n",
    "\n",
    "        if on not in right_df.columns:\n",
    "            raise KeyError(f\"{on} not found in {right_name}\")\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Merging {left_name} with {right_name} on '{on}' ({how})\"\n",
    "        )\n",
    "\n",
    "        merged_df = left_df.merge(\n",
    "            right_df,\n",
    "            on=on,\n",
    "            how=how\n",
    "        )\n",
    "\n",
    "        if new_dataset_name:\n",
    "            self.datasets[new_dataset_name] = merged_df\n",
    "            self.logger.info(f\"Stored as: {new_dataset_name}\")\n",
    "\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a189e-036a-44f3-bb19-61ef92e03a6b",
   "metadata": {},
   "source": [
    "# Dataset Merger Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99136c4-cf6c-4b64-a97c-8f0e736517bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_merger():\n",
    "    \"\"\"\n",
    "    Unit test for DatasetMerger.\n",
    "\n",
    "    Validates:\n",
    "    - Correct left merge\n",
    "    - Storage of merged dataset\n",
    "    - Validation errors\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Criando DataFrames de teste\n",
    "    sales_df = pd.DataFrame({\n",
    "        \"order_id\": [1, 2, 3],\n",
    "        \"amount\": [100, 200, 300]\n",
    "    })\n",
    "\n",
    "    extra_df = pd.DataFrame({\n",
    "        \"order_id\": [1, 2],\n",
    "        \"extra_info\": [\"A\", \"B\"]\n",
    "    })\n",
    "\n",
    "    datasets = {\n",
    "        \"sales_dataset\": sales_df,\n",
    "        \"extra_variable_dataset\": extra_df\n",
    "    }\n",
    "\n",
    "    merger = DatasetMerger(datasets)\n",
    "\n",
    "    # Testar merge correto\n",
    "    merged_df = merger.merge_datasets(\n",
    "        left_name=\"sales_dataset\",\n",
    "        right_name=\"extra_variable_dataset\",\n",
    "        on=\"order_id\",\n",
    "        how=\"left\",\n",
    "        new_dataset_name=\"sales_extra_variables_dataset\"\n",
    "    )\n",
    "\n",
    "    # Verificar número de linhas preservado (left join)\n",
    "    assert len(merged_df) == 3, \"Left merge did not preserve row count.\"\n",
    "\n",
    "    # Verificar nova coluna adicionada\n",
    "    assert \"extra_info\" in merged_df.columns, \"Merge column missing.\"\n",
    "\n",
    "    #  Verificar se dataset foi armazenado\n",
    "    assert \"sales_extra_variables_dataset\" in merger.datasets\n",
    "\n",
    "    # Testar erro de dataset inexistente\n",
    "    try:\n",
    "        merger.merge_datasets(\n",
    "            \"invalid_dataset\",\n",
    "            \"extra_variable_dataset\",\n",
    "            on=\"order_id\"\n",
    "        )\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        raise AssertionError(\"Did not raise KeyError for invalid dataset.\")\n",
    "\n",
    "    # Testar erro de coluna inexistente\n",
    "    try:\n",
    "        merger.merge_datasets(\n",
    "            \"sales_dataset\",\n",
    "            \"extra_variable_dataset\",\n",
    "            on=\"invalid_column\"\n",
    "        )\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        raise AssertionError(\"Did not raise KeyError for invalid column.\")\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41d564-1e23-40e1-adfe-1a358a06528e",
   "metadata": {},
   "source": [
    "# Test Execution - Dataset Merger Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d562c0b-7af3-4415-96bc-3d41e78e4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_merger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edc302-b54b-404d-ae5f-f6b1c6d0ac25",
   "metadata": {},
   "source": [
    "# Dataset Merger Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078c8eb-b1b3-4e5c-9325-a11fa99ccf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger = DatasetMerger(imputed_dfs)\n",
    "\n",
    "merged_df = merger.merge_datasets(\n",
    "    left_name=\"sales_dataset\",\n",
    "    right_name=\"extra_variable_dataset\",\n",
    "    on=\"order_id\",\n",
    "    how=\"left\",\n",
    "    new_dataset_name=\"sales_extra_variables_dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8568025d-fa40-494c-9903-be61f030ce52",
   "metadata": {},
   "source": [
    "# Dataset Concatenator Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28759053-1c88-40d4-86c7-9ccd7a804b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset_concatenator.py\n",
    "\n",
    "Module responsible for:\n",
    "- Safely concatenating datasets\n",
    "- Validating dataset names\n",
    "- Logging operations\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DatasetConcatenator:\n",
    "    \"\"\"\n",
    "    DatasetConcatenator\n",
    "\n",
    "    Responsible for:\n",
    "    - Performing safe concatenation of datasets\n",
    "    - Validating dataset existence\n",
    "    - Logging operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: dict[str, pd.DataFrame]):\n",
    "        self.datasets = datasets\n",
    "        self.logger = self._configure_logger()\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetConcatenator\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _validate_input(self) -> None:\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC\n",
    "    # ==========================================================\n",
    "\n",
    "    def concat_datasets(\n",
    "        self,\n",
    "        dataset_names: list[str],\n",
    "        new_dataset_name: str | None = None,\n",
    "        ignore_index: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Concatenate multiple datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset_names : list[str]\n",
    "            List of dataset names to concatenate.\n",
    "        new_dataset_name : str | None\n",
    "            Optional name to store result in dictionary.\n",
    "        ignore_index : bool\n",
    "            Whether to reset index (default True).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Concatenated DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        dfs_to_concat = []\n",
    "\n",
    "        for name in dataset_names:\n",
    "            if name not in self.datasets:\n",
    "                raise KeyError(f\"{name} not found in datasets.\")\n",
    "\n",
    "            dfs_to_concat.append(self.datasets[name])\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Concatenating datasets: {dataset_names}\"\n",
    "        )\n",
    "\n",
    "        concatenated_df = pd.concat(\n",
    "            dfs_to_concat,\n",
    "            ignore_index=ignore_index\n",
    "        )\n",
    "\n",
    "        if new_dataset_name:\n",
    "            self.datasets[new_dataset_name] = concatenated_df\n",
    "            self.logger.info(f\"Stored as: {new_dataset_name}\")\n",
    "\n",
    "        return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f30602-336f-4a61-9419-cada4671ab39",
   "metadata": {},
   "source": [
    "# Dataset Concatenator Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09df3c-caa1-4380-97ac-097402d7d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_concatenator():\n",
    "    \"\"\"\n",
    "    Unit test for DatasetConcatenator.\n",
    "\n",
    "    Validates:\n",
    "    - Correct concatenation\n",
    "    - ignore_index behavior\n",
    "    - Storage of concatenated dataset\n",
    "    - Validation errors\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Criando DataFrames de teste\n",
    "    df1 = pd.DataFrame({\n",
    "        \"id\": [1, 2],\n",
    "        \"value\": [\"A\", \"B\"]\n",
    "    })\n",
    "\n",
    "    df2 = pd.DataFrame({\n",
    "        \"id\": [3, 4],\n",
    "        \"value\": [\"C\", \"D\"]\n",
    "    })\n",
    "\n",
    "    datasets = {\n",
    "        \"sales_extra_variables\": df1,\n",
    "        \"extra_data\": df2\n",
    "    }\n",
    "\n",
    "    concatenator = DatasetConcatenator(datasets)\n",
    "\n",
    "    # Testar concatenação correta\n",
    "    result_df = concatenator.concat_datasets(\n",
    "        dataset_names=[\n",
    "            \"sales_extra_variables\",\n",
    "            \"extra_data\"\n",
    "        ],\n",
    "        new_dataset_name=\"sales_extra_variables_extra_data\",\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Verificar número total de linhas\n",
    "    assert len(result_df) == 4, \"Incorrect number of rows after concat.\"\n",
    "\n",
    "    # Verificar se índice foi resetado\n",
    "    assert list(result_df.index) == [0, 1, 2, 3], \"Index not reset properly.\"\n",
    "\n",
    "    # Verificar se dataset foi armazenado\n",
    "    assert \"sales_extra_variables_extra_data\" in concatenator.datasets\n",
    "\n",
    "    # Testar erro de dataset inexistente\n",
    "    try:\n",
    "        concatenator.concat_datasets(\n",
    "            dataset_names=[\"invalid_dataset\"]\n",
    "        )\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        raise AssertionError(\"Did not raise KeyError for invalid dataset.\")\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6440eca-03aa-4378-afd2-a160783c1716",
   "metadata": {},
   "source": [
    "# Test Execution - Dataset Concatenator Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d559e89-1497-43ff-be7a-56751ee9fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_concatenator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672a6ad-3fd6-4f1c-8198-a5c486f73acf",
   "metadata": {},
   "source": [
    "# Dataset Concatenator Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecaabf7-c296-4e66-a823-01dd980ffeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenator = DatasetConcatenator(imputed_dfs)\n",
    "\n",
    "merged_df = concatenator.concat_datasets(\n",
    "    dataset_names=[\n",
    "        \"sales_extra_variables_dataset\",\n",
    "        \"extra_data_dataset\"\n",
    "    ],\n",
    "    new_dataset_name=\"sales_extra_variables_extra_data_dataset\",\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f37335-1444-4b4e-ad3d-0fb7f4a184a5",
   "metadata": {},
   "source": [
    "# Insightful Data Filtering By Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e577ed-9f88-4c65-a988-1601783eead8",
   "metadata": {},
   "source": [
    "# Dataset Sorter Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ffbce5-1929-4182-9d8b-d524ad8d74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset_sorter.py\n",
    "\n",
    "Module responsible for:\n",
    "- Sorting datasets by a given column\n",
    "- Exporting sorted datasets to CSV\n",
    "- Overwriting existing files\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class DatasetSorter:\n",
    "\n",
    "    def __init__(self, datasets: dict[str, pd.DataFrame]):\n",
    "\n",
    "        self.datasets = datasets\n",
    "        self.logger = self._configure_logger()\n",
    "\n",
    "        self.project_root = Path().resolve().parent\n",
    "        self.output_path = (\n",
    "            self.project_root /\n",
    "            \"data\" / \"files\" / \"sorted_dataframe\"\n",
    "        )\n",
    "\n",
    "        self._create_output_directory()\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetSorter\")\n",
    "\n",
    "        if not logger.handlers:\n",
    "            logger.setLevel(logging.INFO)\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _create_output_directory(self) -> None:\n",
    "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _validate_input(self) -> None:\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PUBLIC\n",
    "    # ==========================================================\n",
    "\n",
    "    def sort_and_export(\n",
    "        self,\n",
    "        column_name: str,\n",
    "        ascending: bool = True\n",
    "    ) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Sort datasets by column and export each as CSV.\n",
    "        Existing files will be overwritten.\n",
    "        \"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for name, df in self.datasets.items():\n",
    "\n",
    "            if column_name in df.columns:\n",
    "\n",
    "                self.logger.info(\n",
    "                    f\"Sorting dataset '{name}' by '{column_name}'\"\n",
    "                )\n",
    "\n",
    "                sorted_df = df.sort_values(\n",
    "                    column_name,\n",
    "                    ascending=ascending\n",
    "                )\n",
    "\n",
    "                filename = f\"{name}_sorted_by_{column_name}.csv\"\n",
    "                filepath = self.output_path / filename\n",
    "\n",
    "                # Sobrescreve automaticamente\n",
    "                sorted_df.to_csv(filepath, index=False)\n",
    "\n",
    "                self.logger.info(f\"Exported (overwritten if existed): {filepath}\")\n",
    "\n",
    "                results[name] = sorted_df\n",
    "\n",
    "            else:\n",
    "                self.logger.info(\n",
    "                    f\"Column '{column_name}' not found in '{name}'. Skipping.\"\n",
    "                )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5893a6-26b0-4489-a8e0-0c87c9282fb8",
   "metadata": {},
   "source": [
    "# Dataset Sorter Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e938c-ab3e-46ac-bf9b-f500b0af735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_sorter():\n",
    "    \"\"\"\n",
    "    Unit test for DatasetSorter.\n",
    "\n",
    "    Validates:\n",
    "    - Correct sorting\n",
    "    - CSV export\n",
    "    - Overwrite behavior\n",
    "    - Skipping datasets without target column\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Criar DataFrames de teste\n",
    "    df1 = pd.DataFrame({\n",
    "        \"cost\": [30, 10, 20],\n",
    "        \"value\": [\"A\", \"B\", \"C\"]\n",
    "    })\n",
    "\n",
    "    df2 = pd.DataFrame({\n",
    "        \"value\": [\"X\", \"Y\", \"Z\"]  # não possui coluna 'cost'\n",
    "    })\n",
    "\n",
    "    datasets = {\n",
    "        \"dataset_with_cost\": df1,\n",
    "        \"dataset_without_cost\": df2\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "\n",
    "        temp_path = Path(temp_dir)\n",
    "\n",
    "        # Criar estrutura mínima\n",
    "        (temp_path / \"data\").mkdir()\n",
    "\n",
    "        sorter = DatasetSorter(datasets)\n",
    "\n",
    "        # Sobrescrever caminho para ambiente isolado\n",
    "        sorter.project_root = temp_path\n",
    "        sorter.output_path = temp_path / \"data\" / \"files\" / \"sorted_dataframe\"\n",
    "        sorter.output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Executar ordenação\n",
    "        results = sorter.sort_and_export(\"cost\", ascending=True)\n",
    "\n",
    "        # Verificar se arquivo foi criado\n",
    "        expected_file = sorter.output_path / \"dataset_with_cost_sorted_by_cost.csv\"\n",
    "        assert expected_file.exists(), \"CSV file was not created.\"\n",
    "\n",
    "        # Verificar ordenação correta\n",
    "        sorted_df = results[\"dataset_with_cost\"]\n",
    "        assert list(sorted_df[\"cost\"]) == [10, 20, 30], \\\n",
    "            \"Sorting did not work correctly.\"\n",
    "\n",
    "        # Verificar que dataset sem coluna não foi exportado\n",
    "        skipped_file = sorter.output_path / \"dataset_without_cost_sorted_by_cost.csv\"\n",
    "        assert not skipped_file.exists(), \\\n",
    "            \"Dataset without column should not be exported.\"\n",
    "\n",
    "        # Testar sobrescrita\n",
    "        sorter.sort_and_export(\"cost\", ascending=False)\n",
    "\n",
    "        # Após sobrescrever, verificar nova ordem\n",
    "        overwritten_df = pd.read_csv(expected_file)\n",
    "        assert list(overwritten_df[\"cost\"]) == [30, 20, 10], \\\n",
    "            \"File was not overwritten correctly.\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e65b17-78e8-4be6-a3e1-cbf774de0775",
   "metadata": {},
   "source": [
    "# Test Execution - Dataset Sorter Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbd47b-8e1b-4ebb-aebd-c5b93ee6e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_sorter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d873e-c845-4423-82ca-73d90c5f2f67",
   "metadata": {},
   "source": [
    "# Dataset Sorter Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b372f1-cf0b-48af-a329-9c3b1e4599d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorter = DatasetSorter(imputed_dfs)\n",
    "\n",
    "sorted_results = sorter.sort_and_export(\n",
    "    column_name=\"cost\",\n",
    "    ascending=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424929bd-0140-4cb5-a249-a34dc055888d",
   "metadata": {},
   "source": [
    "# Filtering Dataset By Column "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915a43c-2d23-4831-9d2a-a4f25a3f9d25",
   "metadata": {},
   "source": [
    "#  Dataset Filter Manager Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6e5a0-a47d-4e5d-9f10-67b06e892f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset_filter_manager.py\n",
    "\n",
    "Module responsible for:\n",
    "- Filtering datasets dynamically by column and value\n",
    "- Removing specific values from a column\n",
    "- Exporting one CSV per dataset (versioned)\n",
    "- Preserving original input datasets (immutability)\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class DatasetFilterManager:\n",
    "\n",
    "    def __init__(self, datasets: dict[str, pd.DataFrame]):\n",
    "\n",
    "        # 🔒 Deep copy to avoid mutating original dictionary\n",
    "        self.datasets = {\n",
    "            name: df.copy(deep=True)\n",
    "            for name, df in datasets.items()\n",
    "        }\n",
    "\n",
    "        self.logger = self._configure_logger()\n",
    "\n",
    "        self.project_root = Path().resolve().parent\n",
    "        self.output_path = (\n",
    "            self.project_root /\n",
    "            \"data\" / \"files\" / \"filtered_datasets\"\n",
    "        )\n",
    "\n",
    "        self._create_output_directory()\n",
    "        self._validate_input()\n",
    "\n",
    "    # ==========================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================\n",
    "\n",
    "    def _configure_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(\"DatasetFilterManager\")\n",
    "\n",
    "        logger.handlers.clear()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        logger.propagate = False\n",
    "\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "\n",
    "        logger.addHandler(handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _create_output_directory(self) -> None:\n",
    "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _validate_input(self) -> None:\n",
    "        if not isinstance(self.datasets, dict):\n",
    "            raise TypeError(\"Datasets must be a dictionary.\")\n",
    "        if len(self.datasets) == 0:\n",
    "            raise ValueError(\"Datasets dictionary is empty.\")\n",
    "\n",
    "    def _generate_version_suffix(self) -> str:\n",
    "        return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # FUNCTIONALITY 1 — FILTER KEEP VALUE\n",
    "    # ==========================================================\n",
    "\n",
    "    def filter_by_column_value(\n",
    "        self,\n",
    "        column_name: str,\n",
    "        value,\n",
    "        dataset_names: list[str]\n",
    "    ) -> dict[str, pd.DataFrame]:\n",
    "\n",
    "        results = {}\n",
    "        suffix = self._generate_version_suffix()\n",
    "\n",
    "        for name in dataset_names:\n",
    "\n",
    "            if name not in self.datasets:\n",
    "                self.logger.warning(f\"{name} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = self.datasets[name]\n",
    "\n",
    "            if column_name not in df.columns:\n",
    "                self.logger.info(\n",
    "                    f\"Column '{column_name}' not found in '{name}'. Skipping.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            filtered_df = df[df[column_name] == value].copy()\n",
    "\n",
    "            if filtered_df.empty:\n",
    "                self.logger.info(\n",
    "                    f\"No matching rows found in '{name}'.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            filename = (\n",
    "                f\"{name}_keep_{column_name}_{value}_{suffix}.csv\"\n",
    "            )\n",
    "\n",
    "            filepath = self.output_path / filename\n",
    "            filtered_df.to_csv(filepath, index=False)\n",
    "\n",
    "            self.logger.info(f\"Exported: {filepath}\")\n",
    "\n",
    "            results[name] = filtered_df\n",
    "\n",
    "        return results\n",
    "\n",
    "    # ==========================================================\n",
    "    # FUNCTIONALITY 2 — REMOVE VALUE\n",
    "    # ==========================================================\n",
    "\n",
    "    def remove_value_from_column(\n",
    "        self,\n",
    "        column_name: str,\n",
    "        value,\n",
    "        dataset_names: list[str]\n",
    "    ) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Remove rows where column == value.\n",
    "        Export one CSV per dataset (versioned).\n",
    "        \"\"\"\n",
    "\n",
    "        results = {}\n",
    "        suffix = self._generate_version_suffix()\n",
    "\n",
    "        for name in dataset_names:\n",
    "\n",
    "            if name not in self.datasets:\n",
    "                self.logger.warning(f\"{name} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = self.datasets[name]\n",
    "\n",
    "            if column_name not in df.columns:\n",
    "                self.logger.info(\n",
    "                    f\"Column '{column_name}' not found in '{name}'. Skipping.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Removing rows from '{name}' \"\n",
    "                f\"where {column_name} == {value}\"\n",
    "            )\n",
    "\n",
    "            modified_df = df[df[column_name] != value].copy()\n",
    "\n",
    "            filename = (\n",
    "                f\"{name}_remove_{column_name}_{value}_{suffix}.csv\"\n",
    "            )\n",
    "\n",
    "            filepath = self.output_path / filename\n",
    "            modified_df.to_csv(filepath, index=False)\n",
    "\n",
    "            self.logger.info(f\"Exported: {filepath}\")\n",
    "\n",
    "            results[name] = modified_df\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e244cc8-bc33-4905-a01e-438ab7dfcc08",
   "metadata": {},
   "source": [
    "# Dataset Filter Manager Engine Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8d5b8-9845-4c21-a3aa-a0f4f28b6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_filter_manager():\n",
    "    \"\"\"\n",
    "    Unit test for DatasetFilterManager.\n",
    "\n",
    "    Validates:\n",
    "    - Deep copy (immutability)\n",
    "    - Keep filter\n",
    "    - Remove filter\n",
    "    - CSV export per dataset\n",
    "    - Versioned file creation\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Criar DataFrame de teste\n",
    "    df = pd.DataFrame({\n",
    "        \"country\": [\"France\", \"Germany\", \"France\", \"Spain\"],\n",
    "        \"value\": [10, 20, 30, 40]\n",
    "    })\n",
    "\n",
    "    datasets = {\"sales_dataset\": df}\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "\n",
    "        temp_path = Path(temp_dir)\n",
    "        (temp_path / \"data\").mkdir()\n",
    "\n",
    "        manager = DatasetFilterManager(datasets)\n",
    "\n",
    "        # Redirecionar pasta de saída para ambiente isolado\n",
    "        manager.project_root = temp_path\n",
    "        manager.output_path = temp_path / \"data\" / \"files\" / \"filtered_datasets\"\n",
    "        manager.output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Testar filtro KEEP\n",
    "        keep_results = manager.filter_by_column_value(\n",
    "            column_name=\"country\",\n",
    "            value=\"France\",\n",
    "            dataset_names=[\"sales_dataset\"]\n",
    "        )\n",
    "\n",
    "        keep_df = keep_results[\"sales_dataset\"]\n",
    "\n",
    "        assert len(keep_df) == 2, \"Keep filter failed.\"\n",
    "        assert all(keep_df[\"country\"] == \"France\"), \\\n",
    "            \"Keep filter returned wrong values.\"\n",
    "\n",
    "        # Testar filtro REMOVE\n",
    "        remove_results = manager.remove_value_from_column(\n",
    "            column_name=\"country\",\n",
    "            value=\"France\",\n",
    "            dataset_names=[\"sales_dataset\"]\n",
    "        )\n",
    "\n",
    "        remove_df = remove_results[\"sales_dataset\"]\n",
    "\n",
    "        assert \"France\" not in remove_df[\"country\"].values, \\\n",
    "            \"Remove filter failed.\"\n",
    "        assert len(remove_df) == 2, \\\n",
    "            \"Remove filter returned wrong row count.\"\n",
    "\n",
    "        # Verificar exportação de CSV\n",
    "        exported_files = list(manager.output_path.glob(\"*.csv\"))\n",
    "        assert len(exported_files) >= 2, \\\n",
    "            \"CSV files were not created.\"\n",
    "\n",
    "        # Garantir que original não foi alterado\n",
    "        assert len(datasets[\"sales_dataset\"]) == 4, \\\n",
    "            \"Original dataset was mutated.\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e64bc-c1db-43d2-9d95-8777fff96d85",
   "metadata": {},
   "source": [
    "# Test Execution - Dataset Filter Manager Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507d1c5-982f-470c-9951-adb41b318582",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_filter_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6873e68-ca2e-45ec-8d34-915ff808e85f",
   "metadata": {},
   "source": [
    "# Dataset Filter Manager Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055779b-e69b-4c6c-a0f4-b07ee9bcb273",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_manager = DatasetFilterManager(imputed_dfs)\n",
    "results = filter_manager.filter_by_column_value(\n",
    "    column_name=\"country\",\n",
    "    value=\"France\",\n",
    "    dataset_names=[\n",
    "        \"sample_dataset\",\n",
    "        \"sales_dataset\",\n",
    "        \"extra_variable_dataset\",\n",
    "        \"extra_data_dataset\",\n",
    "        \"fact_sales_dataset\",\n",
    "        \"dim_customers_dataset\",\n",
    "        \"dim_products_dataset\",\n",
    "        \"sales_extra_variables_dataset\",\n",
    "        \"sales_extra_variables_extra_data_dataset\"\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e815f8c3-2d0c-47f8-8c1d-301263509eea",
   "metadata": {},
   "source": [
    "# Dataset Filter Manager Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92abd60-042f-40ea-9362-3d944dec0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_results = filter_manager.remove_value_from_column(\n",
    "    column_name=\"country\",\n",
    "    value=\"France\",\n",
    "    dataset_names=[\n",
    "        \"sample_dataset\",\n",
    "        \"sales_dataset\",\n",
    "        \"extra_variable_dataset\",\n",
    "        \"extra_data_dataset\",\n",
    "        \"fact_sales_dataset\",\n",
    "        \"dim_customers_dataset\",\n",
    "        \"dim_products_dataset\",\n",
    "        \"sales_extra_variables_dataset\",\n",
    "        \"sales_extra_variables_extra_data_dataset\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d95dc-50c7-4720-90d2-2e812c1b30e0",
   "metadata": {},
   "source": [
    "# Versioned Github Dataset Exporter Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b0042-4f75-4c76-8e07-e931b657e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class VersionedGitHubDatasetExporter:\n",
    "    \"\"\"\n",
    "    VersionedGitHubDatasetExporter\n",
    "\n",
    "    Responsável por:\n",
    "    - Receber um dicionário de DataFrames\n",
    "    - Selecionar o DataFrame por nome\n",
    "    - Identificar o maior prefixo numérico existente na pasta\n",
    "    - Gerar o próximo número sequencial\n",
    "    - Exportar no padrão: 09.nome_dataframe.csv\n",
    "    - Garantir logging limpo\n",
    "    \"\"\"\n",
    "\n",
    "    OUTPUT_DIR = Path(\"../data/github_dataset\")\n",
    "\n",
    "    def __init__(self, datasets: Dict[str, pd.DataFrame], dataframe_name: str):\n",
    "        self.datasets = datasets\n",
    "        self.dataframe_name = dataframe_name\n",
    "        self.logger = self._setup_logger()\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Versioned exporter initialized for: {self.dataframe_name}\"\n",
    "        )\n",
    "\n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(self.__class__.__name__)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        if not logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "        logger.propagate = False\n",
    "        return logger\n",
    "\n",
    "    def _ensure_directory(self):\n",
    "        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        self.logger.info(f\"Directory ensured at: {self.OUTPUT_DIR}\")\n",
    "\n",
    "    def _get_dataframe(self) -> pd.DataFrame:\n",
    "        if self.dataframe_name not in self.datasets:\n",
    "            raise KeyError(\n",
    "                f\"DataFrame '{self.dataframe_name}' not found in dictionary.\"\n",
    "            )\n",
    "\n",
    "        df = self.datasets[self.dataframe_name]\n",
    "\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(\n",
    "                f\"Object '{self.dataframe_name}' is not a pandas DataFrame.\"\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _get_next_version_number(self) -> str:\n",
    "        \"\"\"\n",
    "        Lista os arquivos existentes e identifica\n",
    "        o próximo número sequencial (2 dígitos).\n",
    "        \"\"\"\n",
    "        existing_files = list(self.OUTPUT_DIR.glob(\"*.csv\"))\n",
    "\n",
    "        numbers = []\n",
    "\n",
    "        for file in existing_files:\n",
    "            match = re.match(r\"^(\\d{2})\\.\", file.name)\n",
    "            if match:\n",
    "                numbers.append(int(match.group(1)))\n",
    "\n",
    "        if not numbers:\n",
    "            next_number = 1\n",
    "        else:\n",
    "            next_number = max(numbers) + 1\n",
    "\n",
    "        return f\"{next_number:02d}\"\n",
    "\n",
    "    def export(self) -> Path:\n",
    "        \"\"\"\n",
    "        Exporta o DataFrame com versionamento automático.\n",
    "        \"\"\"\n",
    "        self._ensure_directory()\n",
    "\n",
    "        df = self._get_dataframe()\n",
    "\n",
    "        version_number = self._get_next_version_number()\n",
    "\n",
    "        filename = f\"{version_number}.{self.dataframe_name}.csv\"\n",
    "        output_path = self.OUTPUT_DIR / filename\n",
    "\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"Exported versioned file: {output_path}\")\n",
    "\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d293f-5f66-484a-bfbe-fff165a0349d",
   "metadata": {},
   "source": [
    "# Versioned Github Dataset Exporter Engine Exeuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02ec49-0c48-44f2-a057-c58ee5091db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = VersionedGitHubDatasetExporter(\n",
    "    datasets=imputed_dfs,\n",
    "    dataframe_name=\"sales_extra_variables_extra_data_dataset\"\n",
    ")\n",
    "\n",
    "path = exporter.export()\n",
    "\n",
    "print(f\"Arquivo salvo em: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276c2ed-4fd1-443a-aa0d-08b7ee00a325",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf453b-cda4-463d-a3f9-8b2f9704abce",
   "metadata": {},
   "source": [
    "# Univariate analysis\n",
    "* Univariate Analysis (Análise Univariada) é a análise estatística de uma única variável por vez, com o objetivo de entender seu comportamento, distribuição, tendência central e dispersão\n",
    "* Objetivo Principal\n",
    "    * Entender o formato da distribuição\n",
    "    * Identificar outliers\n",
    "    * Medir tendência central\n",
    "    * Medir dispersão\n",
    "    * Detectar assimetria\n",
    "    * Avaliar qualidade dos dados\n",
    "* Variáveis Numéricas\n",
    "    * Medidas principais:\n",
    "        * Média\n",
    "            * Arithmetic mean\n",
    "            * geometric mean\n",
    "            * harmonic mean\n",
    "        * Mediana\n",
    "        * Moda\n",
    "        * Variância\n",
    "        * Desvio padrão\n",
    "        * Mínimo / Máximo\n",
    "        * Quartis\n",
    "        * Skewness (assimetria)\n",
    "        * Kurtosis (curtose)\n",
    "* Visualizações comuns\n",
    "    * Histograma\n",
    "    * Boxplot\n",
    "    * Curva de densidade\n",
    "    * Distribuição acumulada\n",
    "* Variáveis Categóricas\n",
    "    * Métricas principais:\n",
    "    * Frequência absoluta\n",
    "    * Frequência relativa (%)\n",
    "    * Moda\n",
    "    * Cardinalidade\n",
    "* Visualizações comuns\n",
    "    * Gráfico de barras\n",
    "    * Tabela de frequência\n",
    "    * Countplot\n",
    "    * Pizza (menos recomendado em DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f072d0d-8578-49a8-86e1-8a5116143183",
   "metadata": {},
   "source": [
    "# Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1d701-56a9-436d-9811-f0020acd5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666a732-b046-4661-ae60-1f887875224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed['sample'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8391aee-5bc8-4d64-8196-f4c7d04f7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed['sample'].iloc[:,0:4].hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60351535-8add-4973-88fa-bad3afe9a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed['sample']['area error'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b462ee-7b7d-4c8e-9316-e259058fc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed['sales_extra_variables_extra_data'].hist(bins='rice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b875844-4eee-4731-ad38-76bcad157301",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed['fact_sales'].hist(bins='rice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f5b25-56a0-406c-9e59-029ca59e6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed['dim_customers'].hist(bins='rice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23ae1f-59d2-49e8-b096-5737730a3445",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dfs_imputed['dim_products'].hist(bins='rice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ced18-5392-4a20-b713-7efde6281b28",
   "metadata": {},
   "source": [
    "# Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e838291-765b-4cd5-b36c-efa4aefdf0c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dfs_imputed['sample'][['mean radius']].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498de91b-4bbc-43ba-b85e-2b6ff50721d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dfs_imputed['sample'][['mean radius']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df145457-b1fb-40f1-9cea-001eaa65772d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dfs_imputed['sample'][['mean radius', 'mean texture']].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8e399-bac9-41f2-acd2-0a50b6ebaa3f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dfs_imputed['sample'].iloc[:, 0:5].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd09a7b-e804-4923-b377-d6fb0549bb74",
   "metadata": {},
   "source": [
    "# Shape of distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1593db6-2eb9-4d78-8840-5c97608a1de2",
   "metadata": {},
   "source": [
    "# Multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dcd67e-d713-4740-93fb-075e10a1a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfs_imputed['sample'].iloc[:,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f10869-d801-413a-93f1-bcb4e48a672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfs_imputed['sample'].iloc[0:100, 0:5], kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f67544-bc5e-4fd1-bc5f-f1b7cb943380",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfs_imputed['sample'].iloc[:, 0:5], kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3e58a-b74f-4ce5-b5fa-6bf8d212773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfs_imputed['sample'].iloc[:, 0:5], corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a35a7-25e5-434a-b988-64967ac8cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    dfs_imputed['sample'],\n",
    "    x_vars=['mean radius','mean texture'],\n",
    "    y_vars=['mean radius','mean texture', 'mean area']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be98846-fb39-47db-9493-ce3ef23ce6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    dfs_imputed['sample'],\n",
    "    x_vars=['mean radius','mean texture'],\n",
    "    y_vars=['mean radius','mean texture', 'mean area']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089088e1-629d-4a8b-8fee-b43d1d2b8d7a",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "* refers o a statiscal relatioship between two variables where changes in one variable are associated with changes in another variable.\n",
    "* Types\n",
    "    * Kenda's Tau\n",
    "        * Ordinal or ranked\n",
    "        * non-linear\n",
    "        * less sensitive\n",
    "        * ca handle ties\n",
    "        * strength and direction of aggreement in rankings\n",
    "        *  concordant and discordant pairs\n",
    "        *  probability of concordnce - probability of discordance\n",
    "        *  mre robust to outliers and ties\n",
    "        *  sall tomoderate\n",
    "        *  more accurate for small samples\n",
    "        *  handles ties well\n",
    "        *  ore accurate for strong correlatios\n",
    "        *  slightly more efficient than spearman for large samples\n",
    "        *  less widey used\n",
    "    * Pearson Correlation\n",
    "        * interval or ratio\n",
    "        * linear\n",
    "        * more sensitive\n",
    "        * cannot handle tiesstrength and direction of linear relatioship\n",
    "    * Spearman\n",
    "        * works with ranks rather than raw data values\n",
    "        * suitable for ordinal data\n",
    "        * non-liner relationship\n",
    "        * non-parametric test\n",
    "        * less robust to outliers and ties\n",
    "        * moderate to large\n",
    "        * ess accurate for sma samples\n",
    "        * lesss robust to ties\n",
    "        * more accurate o weak correlations\n",
    "        * slightly less efficient than kendall for large samples\n",
    "        * more widely used\n",
    "# Causation\n",
    "* implies a direct causa and aeffect relatioshi between two variables where changes in one variable directly result in changes in other in another variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f99e7-2141-47b4-a2c7-be20dba9ea9e",
   "metadata": {},
   "source": [
    "# Correlation matrix and histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab24a9-60d4-4230-b427-ab2bf17faba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed['sample'].corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a77ad-468a-477c-8b36-428722775a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed['sample'].corr(method=\"spearman\", numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2afad-8c31-46c6-9ef4-285366748774",
   "metadata": {},
   "source": [
    "# Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378b037-353d-457e-bc13-539fe0ae0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(dfs_imputed['sample'].corr(numeric_only=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06903f3b-d3fd-427e-a15d-d0d3892b270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(dfs_imputed['sample'].corr(numeric_only=True).abs())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f47e7-5ff6-47b2-95d5-52169482d217",
   "metadata": {},
   "source": [
    "# Stacked histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c348f-ae23-4f5c-bb8b-d07af9b2994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    dfs_imputed['sample'],\n",
    "    x=\"mean radius\",\n",
    "    hue=\"target\",\n",
    "    multiple=\"stack\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215cd2d-a8c3-4bec-b03f-ccd696ad62fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    dfs_imputed['sample'],\n",
    "    x=\"worst concavity\",\n",
    "    hue=\"target\",\n",
    "    multiple=\"stack\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62011f7c-6ce8-47e4-97f7-f3227362c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  = dfs_imputed['sample'].select_dtypes(exclude='object').drop('target', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a15bb-0e09-41a6-9991-8014d163c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(columns), 1, figsize=(5, 5*len(columns)))\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    column_name = columns[i]\n",
    "    sns.histplot(\n",
    "        dfs_imputed['sample'],\n",
    "        x=column_name,\n",
    "        hue='target',\n",
    "        multiple='stack',\n",
    "        ax= axs[i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae532f86-064d-4399-9d17-6d0ddb46adb4",
   "metadata": {},
   "source": [
    "# Conditional Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70559aeb-5f6f-4803-82bf-ceb62ce210a2",
   "metadata": {},
   "source": [
    "# Pair Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78659512-cbd1-4b5b-aa9c-64a3184a0ed8",
   "metadata": {},
   "source": [
    "# DATE EXPLORATION\n",
    "- Identify he earliest and atest dates (boundaries).\n",
    "- Unerstand the scope of data and the timespan.\n",
    "- Example:\n",
    "- MIN/MAX(Date Dimesion)\n",
    "- Min Order-date\n",
    "- Max Create_date\n",
    "- Min Birthdate\n",
    "- DIFFDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20fa2c-9dda-48bb-b856-971e321c7883",
   "metadata": {},
   "source": [
    "# Date Column Profiler Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e43c9-cdd0-4e21-8d52-008a179258fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_profiler = DateColumnProfiler(imputed_dfs)\n",
    "date_columns_df = date_profiler.export_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682f22b-7c8a-4230-b866-090c09ac7baf",
   "metadata": {},
   "source": [
    "📂 1. ARQUIVO: 05.fact_sales_dataset.csv\n",
    "\n",
    "🗂 DataFrame: FACT_SALES\n",
    "\n",
    "🕒 Coluna datetime: order_date\n",
    "\n",
    "❓ Questionário\n",
    "\n",
    "Como o volume de pedidos evoluiu?\n",
    "\n",
    "Existe crescimento YoY?\n",
    "\n",
    "Existe sazonalidade mensal?\n",
    "\n",
    "Existe padrão semanal?\n",
    "\n",
    "Existem picos concentrados?\n",
    "\n",
    "O volume está acelerando?\n",
    "\n",
    "Existe desaceleração recente?\n",
    "\n",
    "Existem períodos críticos?\n",
    "\n",
    "Há estabilidade operacional?\n",
    "\n",
    "Existe tendência de longo prazo?\n",
    "\n",
    "📊 Métricas\n",
    "\n",
    "Pedidos diários\n",
    "\n",
    "Pedidos mensais\n",
    "\n",
    "Pedidos anuais\n",
    "\n",
    "Crescimento YoY\n",
    "\n",
    "Crescimento MoM\n",
    "\n",
    "Rolling 12 meses\n",
    "\n",
    "Índice de sazonalidade\n",
    "\n",
    "Desvio padrão mensal\n",
    "\n",
    "Tendência temporal\n",
    "\n",
    "Frequência média\n",
    "\n",
    "📈 KPIs\n",
    "\n",
    "Pedidos Médios/Mês\n",
    "\n",
    "Crescimento YoY\n",
    "\n",
    "Pico Histórico\n",
    "\n",
    "Índice de Sazonalidade\n",
    "\n",
    "Volatilidade\n",
    "\n",
    "Rolling 12M\n",
    "\n",
    "Aceleração Recente\n",
    "\n",
    "Tendência Estrutural\n",
    "\n",
    "Concentração Temporal\n",
    "\n",
    "Estabilidade Operacional\n",
    "\n",
    "💡 Insights\n",
    "\n",
    "Crescimento consistente sustenta receita\n",
    "\n",
    "Alta volatilidade indica risco operacional\n",
    "\n",
    "Picos indicam dependência de campanhas\n",
    "\n",
    "Volume estável indica maturidade\n",
    "\n",
    "Desaceleração pode indicar saturação\n",
    "\n",
    "Forte sazonalidade exige planejamento\n",
    "\n",
    "Tendência positiva sustenta expansão\n",
    "\n",
    "Concentração indica risco estrutural\n",
    "\n",
    "Aceleração recente pode indicar nova fase\n",
    "\n",
    "Padrão semanal revela comportamento do consumidor\n",
    "\n",
    "# Melhorias\n",
    "\n",
    "Adicionar ADF Test (estacionariedade)\n",
    "\n",
    "criar análise de autoccorrelação\n",
    "\n",
    "Adicionar testes estatísticos avançados\n",
    "\n",
    "Adicionar testes estatísticos avançados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17d3b9-4ffe-4b34-a4a8-46f4922bf5c4",
   "metadata": {},
   "source": [
    "# Setup Inicial- Dataset Fact Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d637d21-b025-4ed9-9264-b661f16e83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "df = pd.read_csv(\"../data/github_dataset/05.fact_sales_dataset.csv\")\n",
    "\n",
    "# Converter para datetime\n",
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"])\n",
    "\n",
    "# Ordenar\n",
    "df = df.sort_values(\"order_date\")\n",
    "\n",
    "# Definir índice temporal\n",
    "df.set_index(\"order_date\", inplace=True)\n",
    "\n",
    "# Criar série de volume diário\n",
    "daily_orders = df.resample(\"D\").size()\n",
    "\n",
    "daily_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a870e-7ded-4bd9-9bb5-8997840c44c9",
   "metadata": {},
   "source": [
    "# Questionário: Como o volume de pedidos evoluiu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba12387-ee53-4664-ae25-61830a77fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Garantir que é datetime\n",
    "daily_orders.index = pd.to_datetime(daily_orders.index)\n",
    "\n",
    "# Limpar qualquer figura anterior\n",
    "plt.close(\"all\")\n",
    "\n",
    "# Criar figura nova explicitamente\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(daily_orders.index, daily_orders.values)\n",
    "\n",
    "ax.set_title(\"Evolução do Volume de Pedidos (Diário)\")\n",
    "ax.set_xlabel(\"Data\")\n",
    "ax.set_ylabel(\"Volume de Pedidos\")\n",
    "\n",
    "# Formatador de datas\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41328533-4fd8-4b18-a635-2d3b688b0d18",
   "metadata": {},
   "source": [
    "# Questionário: Existe crescimento YoY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ccacf9-e67c-4bc8-b932-b92cbc4e3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_orders = daily_orders.resample(\"YE\").sum()\n",
    "yoy_growth = annual_orders.pct_change() * 100\n",
    "\n",
    "print(\"Crescimento YoY (%):\")\n",
    "print(yoy_growth)\n",
    "\n",
    "annual_orders.plot(title=\"Pedidos Anuais\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1114d569-ed81-43dd-b091-0f6f53102093",
   "metadata": {},
   "source": [
    "# Questionário: Existe sazonalidade mensal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4704e-feea-44d8-9260-6bd87eccc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_orders = daily_orders.resample(\"ME\").sum()\n",
    "\n",
    "monthly_orders.plot(title=\"Sazonalidade Mensal\")\n",
    "plt.show()\n",
    "\n",
    "print(monthly_orders.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1737f9-5b4b-4b79-893b-692c5e10a1f9",
   "metadata": {},
   "source": [
    "# Questionário: Existe padrão semanal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61056a94-c36c-4767-b36d-2dd9cd6c804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_pattern = daily_orders.groupby(daily_orders.index.dayofweek).mean()\n",
    "\n",
    "weekly_pattern.index = [\"Seg\", \"Ter\", \"Qua\", \"Qui\", \"Sex\", \"Sab\", \"Dom\"]\n",
    "\n",
    "weekly_pattern.plot(kind=\"bar\", title=\"Padrão Semanal Médio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d544467f-570a-40df-b823-6bcd036ac477",
   "metadata": {},
   "source": [
    "# Questionário: Existem picos concentrados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5806c1-0781-47ff-9bb2-5853daa7f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = daily_orders.mean() + 2 * daily_orders.std()\n",
    "peaks = daily_orders[daily_orders > threshold]\n",
    "\n",
    "print(\"Dias com picos concentrados:\")\n",
    "print(peaks)\n",
    "\n",
    "daily_orders.plot()\n",
    "plt.scatter(peaks.index, peaks.values)\n",
    "plt.title(\"Picos de Pedidos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b7259-1d3b-4a02-8918-d3c479dfadb1",
   "metadata": {},
   "source": [
    "# Questionário: O volume está acelerando?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57378406-2e77-4081-8ad3-b6ea3d4cf737",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_orders_diff = daily_orders.diff()\n",
    "acceleration = daily_orders_diff.diff()\n",
    "\n",
    "print(\"Aceleração média recente:\", acceleration.last(\"90D\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06b755-f2f6-4ff5-8da2-1594da5cd971",
   "metadata": {},
   "source": [
    "# Questionário: Existe desaceleração recente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f72647-eb33-42cb-aa1d-dd308c478133",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent = daily_orders.last(\"90D\").mean()\n",
    "historical = daily_orders.iloc[:-90].mean()\n",
    "\n",
    "print(\"Média recente:\", recent)\n",
    "print(\"Média histórica:\", historical)\n",
    "\n",
    "print(\"Variação percentual:\",\n",
    "      ((recent / historical) - 1) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801dbf0-8849-4b04-9b36-8db97c633e53",
   "metadata": {},
   "source": [
    "# Questionário: Existem períodos críticos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e06d9c-a466-4917-bc4a-148c30420a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_std = daily_orders.rolling(30).std()\n",
    "\n",
    "rolling_std.plot(title=\"Volatilidade Rolling 30 dias\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Períodos de alta instabilidade:\")\n",
    "print(rolling_std.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6ba69-8044-4448-92c8-cd5749528a2f",
   "metadata": {},
   "source": [
    "# Questionário: Há estabilidade operacional?\n",
    "\n",
    "CV baixo → operação estável\n",
    "\n",
    "CV alto → instabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5d306-4c68-43bf-a2d5-26dd1d55cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_variation = daily_orders.std() / daily_orders.mean()\n",
    "print(\"Coeficiente de Variação:\", coef_variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efc650-5a09-43c5-ab53-ea163f76934e",
   "metadata": {},
   "source": [
    "# Questionário: Existe tendência de longo prazo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aede1d4-0982-48b4-b638-eb1d4e083093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Garantir tipo numérico\n",
    "daily_orders = pd.to_numeric(daily_orders, errors=\"coerce\")\n",
    "\n",
    "# Remover NaNs\n",
    "daily_orders = daily_orders.dropna()\n",
    "\n",
    "# Criar arrays\n",
    "y = daily_orders.values.astype(float)\n",
    "x = np.arange(len(y)).astype(float)\n",
    "\n",
    "# Regressão linear\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "print(\"Slope:\", slope)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"R²:\", r_value**2)\n",
    "\n",
    "# Plot profissional\n",
    "plt.close(\"all\")\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax.plot(daily_orders.index, y, label=\"Pedidos\")\n",
    "ax.plot(\n",
    "    daily_orders.index,\n",
    "    intercept + slope * x,\n",
    "    color=\"red\",\n",
    "    label=\"Tendência Linear\"\n",
    ")\n",
    "\n",
    "ax.set_title(\"Tendência de Longo Prazo\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fbd67-5bb1-49f3-b012-e87c72323897",
   "metadata": {},
   "source": [
    "# Quesionário: Quando foram o primeiro e último pedido? Quanto tempo entre um e outro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483ec98-5663-4f46-ba0a-a818fb4adb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando o dataframe dentro do dicionário\n",
    "df_fact_sales = imputed_dfs[\"fact_sales_dataset\"]\n",
    "\n",
    "# Garantir formato datetime\n",
    "df_fact_sales[\"order_date\"] = pd.to_datetime(df_fact_sales[\"order_date\"])\n",
    "\n",
    "# Calcular métricas\n",
    "result_df_date_years = pd.DataFrame({\n",
    "    \"first_order_date\": [df_fact_sales[\"order_date\"].min()],\n",
    "    \"last_order_date\": [df_fact_sales[\"order_date\"].max()],\n",
    "    \"order_range_years\": [\n",
    "        df_fact_sales[\"order_date\"].max().year - df_fact_sales[\"order_date\"].min().year\n",
    "    ]\n",
    "})\n",
    "\n",
    "result_df_date_years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f4569-26e4-4701-858b-12d039769577",
   "metadata": {},
   "source": [
    "# Quesionário: Quando foram o primeiro e último pedido? Quanto tempo entre um e outro em meses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0b4a9-18cd-41b3-9691-22d83bb80667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_sales = imputed_dfs[\"fact_sales_dataset\"]\n",
    "\n",
    "# Garantir datetime\n",
    "df_fact_sales[\"order_date\"] = pd.to_datetime(df_fact_sales[\"order_date\"])\n",
    "\n",
    "first_date = df_fact_sales[\"order_date\"].min()\n",
    "last_date = df_fact_sales[\"order_date\"].max()\n",
    "\n",
    "# Diferença em meses\n",
    "order_range_months = (\n",
    "    (last_date.year - first_date.year) * 12\n",
    "    + (last_date.month - first_date.month)\n",
    ")\n",
    "\n",
    "result_df_date_months = pd.DataFrame({\n",
    "    \"first_order_date\": [first_date],\n",
    "    \"last_order_date\": [last_date],\n",
    "    \"order_range_months\": [order_range_months]\n",
    "})\n",
    "\n",
    "result_df_date_months\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225c857-c21e-4bd3-ade4-f6f4b0337885",
   "metadata": {},
   "source": [
    "# Quesionário: Qual a data de nascimento mais anatiga e mais nova? Qual a idade mais antiga e mais nova?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70559cd-9f7d-4f89-b671-768f0e432f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando o dataframe no dicionário\n",
    "df_customers = imputed_dfs[\"dim_customers_dataset\"]\n",
    "\n",
    "# Garantir formato datetime\n",
    "df_customers[\"birthdate\"] = pd.to_datetime(df_customers[\"birthdate\"])\n",
    "\n",
    "# Datas extrema\n",
    "oldest_birthdate = df_customers[\"birthdate\"].min()\n",
    "youngest_birthdate = df_customers[\"birthdate\"].max()\n",
    "\n",
    "# Data atual\n",
    "today = pd.Timestamp.today()\n",
    "\n",
    "# Cálculo idade (equivalente ao DATEDIFF(year,...))\n",
    "oldest_age = today.year - oldest_birthdate.year\n",
    "youngest_age = today.year - youngest_birthdate.year\n",
    "\n",
    "# Resultado final\n",
    "result_df_age_oldest_youngest = pd.DataFrame({\n",
    "    \"oldest_birthdate\": [oldest_birthdate],\n",
    "    \"oldest_age\": [oldest_age],\n",
    "    \"youngest_birthdate\": [youngest_birthdate],\n",
    "    \"youngest_age\": [youngest_age]\n",
    "})\n",
    "\n",
    "result_df_age_oldest_youngest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236461b7-6936-4e4a-b4f5-08d6e8d7c047",
   "metadata": {},
   "source": [
    "# Setup Inicial - Dataset Fact Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd3bb2-c8f3-40df-9d6b-ab5884f6610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "df = pd.read_csv(\"../data/github_dataset/05.fact_sales_dataset.csv\")\n",
    "\n",
    "# Converter para datetime\n",
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"])\n",
    "\n",
    "# Ordenar\n",
    "df = df.sort_values(\"order_date\")\n",
    "\n",
    "# Definir índice\n",
    "df.set_index(\"order_date\", inplace=True)\n",
    "\n",
    "# Criar série base (volume diário)\n",
    "daily_orders = df.resample(\"D\").size()\n",
    "\n",
    "daily_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ee24d-75f8-437b-9d56-ed00a85de0eb",
   "metadata": {},
   "source": [
    "# Métricas - Pedidos Diários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07228bc7-08d7-4147-9b21-aed8cee386c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_orders.plot(title=\"Pedidos Diários\", figsize=(12,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da052159-4fc8-4404-a842-44f7d5ebf4f4",
   "metadata": {},
   "source": [
    "# Métricas - Pedidos Mensais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74cd1c-761b-4e6d-8a8a-a872a8706105",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_orders = daily_orders.resample(\"ME\").sum()\n",
    "\n",
    "monthly_orders.plot(title=\"Pedidos Mensais\", figsize=(12,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b5b72-8522-4b70-b64c-674dc45202b4",
   "metadata": {},
   "source": [
    "# Métricas - Pedidos Anuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300af29-7566-4250-bc99-df53b234cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_orders = daily_orders.resample(\"Y\").sum()\n",
    "\n",
    "annual_orders.plot(title=\"Pedidos Anuais\", figsize=(12,6))\n",
    "plt.show()\n",
    "\n",
    "annual_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd3a7b-4835-4939-aaa3-6a2fd7233f59",
   "metadata": {},
   "source": [
    "# Métricas- Crescimento YoY (Year over Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81ad3e-56b9-4aa5-a208-fe2aabddcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoy_growth = annual_orders.pct_change() * 100\n",
    "\n",
    "print(\"Crescimento YoY (%):\")\n",
    "print(yoy_growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a062d-c2f6-4f20-acd0-61952fc8bc77",
   "metadata": {},
   "source": [
    "# Métricas - Crescimento MoM (Month over Month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab133e-f6b6-450d-a003-1d0eed8124a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_growth = monthly_orders.pct_change() * 100\n",
    "\n",
    "print(\"Crescimento MoM (%):\")\n",
    "print(mom_growth.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa2f17-53f6-4da0-be3f-10086489b233",
   "metadata": {},
   "source": [
    "# Métricas - Rolling 12 Meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc17e4e-4545-4456-9bb1-98b3bfaa2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_12m = monthly_orders.rolling(12).sum()\n",
    "\n",
    "rolling_12m.plot(title=\"Rolling 12 Meses\", figsize=(12,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b48c1-3240-4706-8322-7a07bbd75291",
   "metadata": {},
   "source": [
    "# Métricas - Índice de Sazonalidade\n",
    "\n",
    "Se > 1 → mês acima da média\n",
    "\n",
    "Se < 1 → mês abaixo da média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba78334-82ae-41d0-802c-2aa1f610ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_avg = monthly_orders.groupby(monthly_orders.index.month).mean()\n",
    "overall_avg = monthly_orders.mean()\n",
    "\n",
    "seasonality_index = monthly_avg / overall_avg\n",
    "\n",
    "seasonality_index.index = [\n",
    "    \"Jan\",\"Fev\",\"Mar\",\"Abr\",\"Mai\",\"Jun\",\n",
    "    \"Jul\",\"Ago\",\"Set\",\"Out\",\"Nov\",\"Dez\"\n",
    "]\n",
    "\n",
    "print(\"Índice de Sazonalidade:\")\n",
    "print(seasonality_index)\n",
    "\n",
    "seasonality_index.plot(kind=\"bar\", title=\"Índice de Sazonalidade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab896025-a27d-43ae-99fc-0d645eab0703",
   "metadata": {},
   "source": [
    "# Métricas - Desvio Padrão Mensal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c77728-4656-4b1e-b1e6-c54b4af12bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_std = monthly_orders.std()\n",
    "\n",
    "print(\"Desvio Padrão Mensal:\", monthly_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7382a-01af-4c16-8a27-b89d399b0464",
   "metadata": {},
   "source": [
    "# Métricas - Tendência Temporal (Regressão Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eebd72b-e2dd-4164-aa8b-be94dd17a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantir tipo numérico\n",
    "monthly_orders = monthly_orders.dropna()\n",
    "\n",
    "y = monthly_orders.values.astype(float)\n",
    "x = np.arange(len(y)).astype(float)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "print(\"Slope:\", slope)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"R²:\", r_value**2)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(monthly_orders.index, y, label=\"Pedidos Mensais\")\n",
    "plt.plot(monthly_orders.index, intercept + slope * x,\n",
    "         color=\"red\", label=\"Tendência Linear\")\n",
    "plt.legend()\n",
    "plt.title(\"Tendência Temporal\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75bfc35-c11a-4c0e-8466-046aa8901aca",
   "metadata": {},
   "source": [
    "# Métricas - Frequência Média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce230e-f793-45cb-bd81-e96574c50216",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_daily = daily_orders.mean()\n",
    "average_monthly = monthly_orders.mean()\n",
    "average_annual = annual_orders.mean()\n",
    "\n",
    "print(\"Frequência Média Diária:\", average_daily)\n",
    "print(\"Frequência Média Mensal:\", average_monthly)\n",
    "print(\"Frequência Média Anual:\", average_annual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340467e-a171-4549-b06b-80c2be5ff428",
   "metadata": {},
   "source": [
    "# Setup Inicial - Dataset Fact Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88055dc3-0f25-4abf-aa99-cc3c6c3a49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "df = pd.read_csv(\"../data/github_dataset/05.fact_sales_dataset.csv\")\n",
    "\n",
    "# Converter para datetime\n",
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"])\n",
    "\n",
    "# Ordenar\n",
    "df = df.sort_values(\"order_date\")\n",
    "\n",
    "# Definir índice temporal\n",
    "df.set_index(\"order_date\", inplace=True)\n",
    "\n",
    "# Série base\n",
    "daily_orders = df.resample(\"D\").size()\n",
    "monthly_orders = daily_orders.resample(\"ME\").sum()\n",
    "annual_orders = daily_orders.resample(\"YE\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8524b6-b190-47a0-b85d-5f91366d42bf",
   "metadata": {},
   "source": [
    "# KPIs - Pedidos Médios / Mês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b000b5-8367-4a09-9bc9-09047cb7e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_orders_month = monthly_orders.mean()\n",
    "print(\"Pedidos Médios por Mês:\", round(avg_orders_month, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4ef05-3386-46db-9cde-4159fe65dce2",
   "metadata": {},
   "source": [
    "# KPIs - Crescimento YoY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c91951-fd8d-49f9-ac78-ea2d7d2d4a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoy_growth = annual_orders.pct_change() * 100\n",
    "print(\"Crescimento YoY (%):\")\n",
    "print(yoy_growth.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68b2ca-778c-4e3d-a455-48d6ace50794",
   "metadata": {},
   "source": [
    "# KPIs - Pico Histórico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47a259-3707-440a-8949-bf491cee7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_day = daily_orders.idxmax()\n",
    "peak_value = daily_orders.max()\n",
    "\n",
    "print(\"Pico Histórico:\")\n",
    "print(\"Data:\", peak_day)\n",
    "print(\"Volume:\", peak_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b771ec-2713-4f0e-b99b-6e450e837dd6",
   "metadata": {},
   "source": [
    "# KPIs - Índice de Sazonalidade\n",
    "\n",
    "1 = mês acima da média\n",
    "\n",
    "< 1 = mês abaixo da média\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3bd3b9-f073-46f0-9c77-8ed5136598df",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_avg_by_month = monthly_orders.groupby(monthly_orders.index.month).mean()\n",
    "overall_avg = monthly_orders.mean()\n",
    "\n",
    "seasonality_index = monthly_avg_by_month / overall_avg\n",
    "\n",
    "print(\"Índice de Sazonalidade:\")\n",
    "print(seasonality_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234255f-431e-4779-ac6d-b9d46ff52e58",
   "metadata": {},
   "source": [
    "# KPIs - Volatilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d559f2-4795-49e4-b9ca-b4a051dc0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility = monthly_orders.std() / monthly_orders.mean()\n",
    "print(\"Volatilidade (Coeficiente de Variação):\", round(volatility, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b5e17a-6f19-4c04-b268-2cb8ef87be37",
   "metadata": {},
   "source": [
    "# KPIs - Rolling 12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f61ee6-aaee-47de-b042-fab307bae429",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_12m = monthly_orders.rolling(12).sum()\n",
    "\n",
    "rolling_12m.plot(title=\"Rolling 12 Meses\", figsize=(12,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05061993-d091-4e0f-adcb-db7ac1361603",
   "metadata": {},
   "source": [
    "# KPIs - Aceleração Recente\n",
    "\n",
    "Comparação últimos 3 meses vs 3 meses anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ccbb6-b63b-4f4d-9485-98dc2d0a9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_3m = monthly_orders.tail(3).mean()\n",
    "previous_3m = monthly_orders.tail(6).head(3).mean()\n",
    "\n",
    "acceleration = ((recent_3m / previous_3m) - 1) * 100\n",
    "\n",
    "print(\"Aceleração Recente (%):\", round(acceleration, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c237a06-59be-4513-8b18-3c27842f4cab",
   "metadata": {},
   "source": [
    "# KPIs - Tendência Estrutural\n",
    "\n",
    "Regressão linear mensal\n",
    "\n",
    "Se p-value < 0.05 → tendência significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3923f-c588-4cbc-9419-351f35161d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = monthly_orders.values.astype(float)\n",
    "x = np.arange(len(y)).astype(float)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "print(\"Slope:\", slope)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"R²:\", r_value**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9d1f2-b93c-4a37-b027-be4ff7711e9f",
   "metadata": {},
   "source": [
    "# KPIs - Concentração Temporal\n",
    "\n",
    "Percentual dos 3 maiores meses sobre o total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79689ef0-6b00-4141-8d7d-979946658162",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_months = monthly_orders.sort_values(ascending=False).head(3).sum()\n",
    "total_orders = monthly_orders.sum()\n",
    "\n",
    "concentration_ratio = (top_3_months / total_orders) * 100\n",
    "\n",
    "print(\"Concentração Temporal (% top 3 meses):\",\n",
    "      round(concentration_ratio, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76433dd0-e2b7-48a7-bf37-031b3cd7f6f3",
   "metadata": {},
   "source": [
    "# KPIs - Estabilidade Operacional\n",
    "\n",
    "Coeficiente de variação diário:\n",
    "\n",
    "CV baixo → operação estável\n",
    "\n",
    "CV alto → instabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d273c30-9355-4b70-8c7b-64ba990c02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "operational_stability = daily_orders.std() / daily_orders.mean()\n",
    "\n",
    "print(\"Estabilidade Operacional (CV Diário):\",\n",
    "      round(operational_stability, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33862ae3-cac5-4cd0-b3c1-f1f1b91e3a93",
   "metadata": {},
   "source": [
    "# Setup inicial - Dataset Fact Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca439a-a904-4f40-8ad3-2c8b2cc295b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "df = pd.read_csv(\"../data/github_dataset/05.fact_sales_dataset.csv\")\n",
    "\n",
    "# Converter para datetime\n",
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"])\n",
    "\n",
    "# Ordenar\n",
    "df = df.sort_values(\"order_date\")\n",
    "\n",
    "# Definir índice\n",
    "df.set_index(\"order_date\", inplace=True)\n",
    "\n",
    "# Séries base\n",
    "daily_orders = df.resample(\"D\").size()\n",
    "monthly_orders = daily_orders.resample(\"ME\").sum()\n",
    "annual_orders = daily_orders.resample(\"YE\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113d1fe-3dbf-44f8-9afc-3d0cf7afe350",
   "metadata": {},
   "source": [
    "# Insights - Crescimento consistente sustenta receita\n",
    "\n",
    "Se crescimento positivo e consistente → evidência do insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7e6b5-dd19-462b-905c-ad402a1fcc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoy_growth = annual_orders.pct_change() * 100\n",
    "\n",
    "print(\"Crescimento YoY (%):\")\n",
    "print(yoy_growth)\n",
    "\n",
    "print(\"Média de Crescimento YoY:\", yoy_growth.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cc5e2-d4dc-438c-aa63-c3229c3d9e04",
   "metadata": {},
   "source": [
    "# Insights - Alta volatilidade indica risco operacional\n",
    "\n",
    "CV > 0.3 geralmente indica alta volatilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57821d6-a5a3-4b38-a819-fc0706b3931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility = daily_orders.std() / daily_orders.mean()\n",
    "\n",
    "print(\"Coeficiente de Variação Diário:\", volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372d091-2682-4bfa-a6e2-586323084b3a",
   "metadata": {},
   "source": [
    "# Insights - Picos indicam dependência de campanhas\n",
    "\n",
    "Alta concentração de picos pode indicar campanhas específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653cd875-d997-4457-ba17-58488f27f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = daily_orders.mean() + 2 * daily_orders.std()\n",
    "peaks = daily_orders[daily_orders > threshold]\n",
    "\n",
    "print(\"Dias com picos significativos:\")\n",
    "print(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42612a66-cdcc-403f-b78a-7c2894fad151",
   "metadata": {},
   "source": [
    "# Insights - Volume estável indica maturidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68442143-a275-4c08-9693-048cf0f2b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_std = daily_orders.rolling(30).std()\n",
    "\n",
    "rolling_std.plot(title=\"Volatilidade Rolling 30 dias\", figsize=(12,6))\n",
    "plt.show()\n",
    "\n",
    "print(\"Média da volatilidade:\", rolling_std.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58868e70-6eeb-464f-b904-bc4c8902efe9",
   "metadata": {},
   "source": [
    "# Insihts - Desaceleração pode indicar saturação\n",
    "\n",
    "Valor negativo → desaceleração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a505ba5-4a5b-4a5d-9564-4030cbbd7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_6m = monthly_orders.tail(6).mean()\n",
    "previous_6m = monthly_orders.tail(12).head(6).mean()\n",
    "\n",
    "deceleration = ((recent_6m / previous_6m) - 1) * 100\n",
    "\n",
    "print(\"Variação últimos 6 meses (%):\", deceleration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9573bd-aa78-49da-91ce-2357f8ee421e",
   "metadata": {},
   "source": [
    "# Insights - Forte sazonalidade exige planejamento\n",
    "\n",
    "Meses muito acima de 1 → sazonalidade forte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13df811-3baf-4854-9d1e-22f057c98ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality = monthly_orders.groupby(monthly_orders.index.month).mean()\n",
    "overall_mean = monthly_orders.mean()\n",
    "\n",
    "seasonality_index = seasonality / overall_mean\n",
    "\n",
    "print(\"Índice de Sazonalidade:\")\n",
    "print(seasonality_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f5cc3-8c4f-4bde-a0e0-a35cdea0b899",
   "metadata": {},
   "source": [
    "# Insights - Tendência positiva sustenta expansão\n",
    "\n",
    "Slope > 0 e p-value < 0.05 → tendência positiva significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725dd14-3c06-4ab0-b760-10c5950e1c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = monthly_orders.values.astype(float)\n",
    "x = np.arange(len(y)).astype(float)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "print(\"Slope:\", slope)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"R²:\", r_value**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453af8f-8937-4ed8-8e85-89e7ea1b4aa4",
   "metadata": {},
   "source": [
    "# Insights - Concentração indica risco estrutural\n",
    "\n",
    "Alta concentração → dependência estrutural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba0ff3-d68c-4d23-be2d-96f7937df82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_months = monthly_orders.sort_values(ascending=False).head(3).sum()\n",
    "total_orders = monthly_orders.sum()\n",
    "\n",
    "concentration_ratio = (top_3_months / total_orders) * 100\n",
    "\n",
    "print(\"Concentração nos 3 maiores meses (%):\",\n",
    "      concentration_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6260c-604a-48c9-bef8-a202353b2bf0",
   "metadata": {},
   "source": [
    "# Insights - Aceleração recente pode indicar nova fase\n",
    "Valor positivo relevante → possível nova fase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e54bf4-f7b3-42da-94db-28f95548cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_3m = monthly_orders.tail(3).mean()\n",
    "previous_3m = monthly_orders.tail(6).head(3).mean()\n",
    "\n",
    "acceleration = ((recent_3m / previous_3m) - 1) * 100\n",
    "\n",
    "print(\"Aceleração Recente (%):\", acceleration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5595cb3-4638-4f00-85ee-b993602498ed",
   "metadata": {},
   "source": [
    "# Insights - Padrão semanal revela comportamento do consumidor\n",
    "\n",
    "Diferenças claras → comportamento do consumidor por dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f627da6-8310-40fb-a04d-f595df632325",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_pattern = daily_orders.groupby(daily_orders.index.dayofweek).mean()\n",
    "\n",
    "weekly_pattern.index = [\"Seg\",\"Ter\",\"Qua\",\"Qui\",\"Sex\",\"Sab\",\"Dom\"]\n",
    "\n",
    "weekly_pattern.plot(kind=\"bar\", title=\"Padrão Semanal Médio\", figsize=(10,5))\n",
    "plt.show()\n",
    "\n",
    "print(\"Média por dia da semana:\")\n",
    "print(weekly_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368425c-4e95-4299-9e7f-c27a94c059f3",
   "metadata": {},
   "source": [
    "📂  ARQUIVO: 06.dim_customers_dataset.csv\n",
    "🗂 DataFrame: DIM_CUSTOMERS\n",
    "🕒 Colunas datetime: birthdate, create_date\n",
    "❓ Questionário\n",
    "\n",
    "Qual idade média dos clientes?\n",
    "\n",
    "A base está envelhecendo?\n",
    "\n",
    "Aquisição mensal está crescendo?\n",
    "\n",
    "Existe desaceleração?\n",
    "\n",
    "Tempo até primeira compra?\n",
    "\n",
    "Cohortes recentes são melhores?\n",
    "\n",
    "Existe retenção por período?\n",
    "\n",
    "Base está rejuvenescendo?\n",
    "\n",
    "Existe churn estrutural?\n",
    "\n",
    "Crescimento é sustentável?\n",
    "\n",
    "📊 Métricas\n",
    "\n",
    "Idade média\n",
    "\n",
    "Idade mediana\n",
    "\n",
    "Aquisição mensal\n",
    "\n",
    "Crescimento YoY base\n",
    "\n",
    "Tempo até primeira compra\n",
    "\n",
    "Retenção por coorte\n",
    "\n",
    "Lifetime médio\n",
    "\n",
    "Distribuição etária\n",
    "\n",
    "Taxa de aquisição\n",
    "\n",
    "Índice de envelhecimento\n",
    "\n",
    "📈 KPIs\n",
    "\n",
    "Idade Média\n",
    "\n",
    "Novos Clientes/Mês\n",
    "\n",
    "Crescimento Base\n",
    "\n",
    "Lifetime Médio\n",
    "\n",
    "Tempo até 1ª Compra\n",
    "\n",
    "Retenção por Coorte\n",
    "\n",
    "Churn Rate\n",
    "\n",
    "Taxa de Aquisição\n",
    "\n",
    "Índice de Envelhecimento\n",
    "\n",
    "ARPU por Coorte\n",
    "\n",
    "💡 Insights\n",
    "\n",
    "Base envelhecendo indica risco estrutural\n",
    "\n",
    "Aquisição desacelerando é alerta\n",
    "\n",
    "Lifetime crescente indica fidelização\n",
    "\n",
    "Cohortes recentes podem ser mais valiosas\n",
    "\n",
    "Churn alto ameaça sustentabilidade\n",
    "\n",
    "Base rejuvenescendo indica expansão\n",
    "\n",
    "Tempo alto até compra indica fricção\n",
    "\n",
    "Retenção forte sustenta crescimento\n",
    "\n",
    "Distribuição etária desequilibrada aumenta risco\n",
    "\n",
    "Crescimento saudável indica estabilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab359f-b25a-4372-b7b8-c7e6f27054e6",
   "metadata": {},
   "source": [
    "# Loading Dataset Dim Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f9d46-adec-40f1-9a7a-fc4e9c3b4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# LOAD DATA\n",
    "# ===============================\n",
    "df = imputed_dfs['dim_customers_dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96459829-036e-4a4d-a39b-60624e842e2b",
   "metadata": {},
   "source": [
    "# Quetionário - Qual idade média dos clientes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b39083-4de5-44e7-ae0f-1ae7c317e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1️⃣ IDADE MÉDIA DOS CLIENTES\n",
    "# ===============================\n",
    "today = pd.Timestamp.today()\n",
    "\n",
    "df[\"age\"] = (today - df[\"birthdate\"]).dt.days / 365.25\n",
    "idade_media = df[\"age\"].mean()\n",
    "\n",
    "print(f\"\\n📌 Idade média atual da base: {idade_media:.2f} anos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00a00f-8a43-4d56-952d-f812ce468d65",
   "metadata": {},
   "source": [
    "# Questionário - A base está envelhecendo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f948949-7c1b-4179-9b4a-652a6846c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 2️⃣ BASE ESTÁ ENVELHECENDO?\n",
    "# ===============================\n",
    "df[\"signup_year\"] = df[\"create_date\"].dt.year\n",
    "age_by_year = df.groupby(\"signup_year\")[\"age\"].mean()\n",
    "\n",
    "print(\"\\n📌 Idade média por ano de aquisição:\")\n",
    "print(age_by_year)\n",
    "\n",
    "plt.figure()\n",
    "age_by_year.plot()\n",
    "plt.title(\"Idade Média por Ano de Aquisição\")\n",
    "plt.xlabel(\"Ano\")\n",
    "plt.ylabel(\"Idade Média\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193590e7-0bdd-430a-b59f-b35cf1421426",
   "metadata": {},
   "source": [
    "# Questionário - Aquisição mensal está crescendo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bc10e-eba4-4968-9200-2c8dd772fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3️⃣ AQUISIÇÃO MENSAL\n",
    "# ===============================\n",
    "df[\"signup_month\"] = df[\"create_date\"].dt.to_period(\"M\")\n",
    "monthly_acquisition = df.groupby(\"signup_month\").size()\n",
    "\n",
    "print(\"\\n📌 Aquisição mensal:\")\n",
    "print(monthly_acquisition.tail())\n",
    "\n",
    "plt.figure()\n",
    "monthly_acquisition.plot()\n",
    "plt.title(\"Aquisição Mensal de Clientes\")\n",
    "plt.xlabel(\"Mês\")\n",
    "plt.ylabel(\"Novos Clientes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae9486-5380-44b5-b0a6-107986ab2519",
   "metadata": {},
   "source": [
    "# Questionário - Existe desaceleração?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac256e8-f333-4fbb-89ba-d5e55146ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 4️⃣ EXISTE DESACELERAÇÃO?\n",
    "# ===============================\n",
    "monthly_growth = monthly_acquisition.pct_change()\n",
    "\n",
    "print(\"\\n📌 Taxa de crescimento mensal (%):\")\n",
    "print(monthly_growth.tail())\n",
    "\n",
    "plt.figure()\n",
    "monthly_growth.plot()\n",
    "plt.title(\"Taxa de Crescimento da Aquisição\")\n",
    "plt.xlabel(\"Mês\")\n",
    "plt.ylabel(\"Growth Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649b915-d1fd-4d9c-a003-b37879f17f69",
   "metadata": {},
   "source": [
    "# Questionário - Base está rejuvenescendo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764e06f-6997-4330-9af5-85546638d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5️⃣ BASE ESTÁ REJUVENESCENDO?\n",
    "# ===============================\n",
    "# Idade no momento da aquisição\n",
    "df[\"age_at_signup\"] = (df[\"create_date\"] - df[\"birthdate\"]).dt.days / 365.25\n",
    "age_cohort = df.groupby(\"signup_year\")[\"age_at_signup\"].mean()\n",
    "\n",
    "print(\"\\n📌 Idade média no momento da aquisição por ano:\")\n",
    "print(age_cohort)\n",
    "\n",
    "plt.figure()\n",
    "age_cohort.plot()\n",
    "plt.title(\"Idade Média no Momento da Aquisição\")\n",
    "plt.xlabel(\"Ano\")\n",
    "plt.ylabel(\"Idade na Aquisição\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b46f5-2738-406a-b8f9-1c4a4a38b765",
   "metadata": {},
   "source": [
    "# Questionário - Cohortes recentes são melhores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36396f1-bb51-4a2e-a2fd-09813fdaf74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 6️⃣ COHORTES RECENTES SÃO MELHORES?\n",
    "# ===============================\n",
    "# Aqui só conseguimos avaliar perfil etário, não retenção.\n",
    "print(\"\\n⚠️ Avaliação de coortes limitada ao perfil demográfico (sem dados de compra).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bc8ca-7849-4ded-9255-4e7a41d0b87b",
   "metadata": {},
   "source": [
    "# Questionário - Existe churn estrutural?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a6fc7-6bc4-435b-857d-b02c4bff308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 7️⃣ RETENÇÃO / CHURN / TEMPO ATÉ PRIMEIRA COMPRA\n",
    "# ===============================\n",
    "print(\"\\n❗ Para calcular:\")\n",
    "print(\"- Tempo até primeira compra\")\n",
    "print(\"- Retenção por período\")\n",
    "print(\"- Churn estrutural\")\n",
    "print(\"- Sustentabilidade do crescimento\")\n",
    "print(\"É necessário dataset transacional (fato de vendas).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecea26-85b3-42dc-ac30-21181e476bc9",
   "metadata": {},
   "source": [
    "# Questionário - Crescimento é sustentável?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa31dfa6-8d48-40a7-baba-e8312fc2490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 8️⃣ CRESCIMENTO É SUSTENTÁVEL?\n",
    "# ===============================\n",
    "media_growth = monthly_growth.mean()\n",
    "\n",
    "print(f\"\\n📌 Crescimento médio mensal: {media_growth:.4f}\")\n",
    "\n",
    "if media_growth > 0:\n",
    "    print(\"📈 Tendência de crescimento positiva.\")\n",
    "else:\n",
    "    print(\"📉 Crescimento fraco ou negativo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe4bdd-2d3a-4a64-ac29-dfa562789189",
   "metadata": {},
   "source": [
    "# Loading the Dataset Dim Customers and Fact Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dfea60-efdb-4d73-a905-ffe2829f5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# LOAD DATA\n",
    "# ==========================================\n",
    "customers = imputed_dfs[\"dim_customers_dataset\"]\n",
    "fact = imputed_dfs[\"fact_sales_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e26b2c-cb03-4acc-b926-50f0c122d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# NORMALIZAÇÃO\n",
    "# ==========================================\n",
    "def normalize(df):\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    return df\n",
    "\n",
    "customers = normalize(customers)\n",
    "fact = normalize(fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fcf756-2b8e-45ee-8b0b-0f504384ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# DETECTAR COLUNAS PRINCIPAIS\n",
    "# ==========================================\n",
    "def find_col(df, keyword):\n",
    "    for col in df.columns:\n",
    "        if keyword in col:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "customer_key = find_col(customers, \"customer\")\n",
    "birth_col = find_col(customers, \"birth\")\n",
    "create_col = find_col(customers, \"create\")\n",
    "order_date_col = find_col(fact, \"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d626f-7ff8-4b84-a934-886e84a23502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# CONVERSÃO DE DATAS\n",
    "# ==========================================\n",
    "customers[birth_col] = pd.to_datetime(customers[birth_col], errors=\"coerce\")\n",
    "customers[create_col] = pd.to_datetime(customers[create_col], errors=\"coerce\")\n",
    "fact[order_date_col] = pd.to_datetime(fact[order_date_col], errors=\"coerce\")\n",
    "\n",
    "today = pd.Timestamp.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7fb47-52c7-413e-962c-32453b1b4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1️⃣ IDADE MÉDIA\n",
    "# ==========================================\n",
    "customers[\"age\"] = (today - customers[birth_col]).dt.days / 365.25\n",
    "print(\"\\n📌 Idade média:\", customers[\"age\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cccbf4-2d2c-42f5-8d29-781af29f3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2️⃣ IDADE MEDIANA\n",
    "# ==========================================\n",
    "print(\"📌 Idade mediana:\", customers[\"age\"].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aeb589-34cd-4544-96ed-c95858884739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3️⃣ AQUISIÇÃO MENSAL\n",
    "# ==========================================\n",
    "customers[\"signup_month\"] = customers[create_col].dt.to_period(\"M\")\n",
    "monthly_acq = customers.groupby(\"signup_month\")[customer_key].count()\n",
    "\n",
    "print(\"\\n📌 Aquisição mensal:\")\n",
    "print(monthly_acq.tail())\n",
    "\n",
    "plt.figure()\n",
    "monthly_acq.plot()\n",
    "plt.title(\"Aquisição Mensal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722350a-26fe-4ae1-8a8f-3140eee5b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4️⃣ CRESCIMENTO YoY DA BASE\n",
    "# ==========================================\n",
    "customers[\"signup_year\"] = customers[create_col].dt.year\n",
    "base_by_year = customers.groupby(\"signup_year\")[customer_key].count()\n",
    "\n",
    "yoy_growth = base_by_year.pct_change()\n",
    "\n",
    "print(\"\\n📌 Crescimento YoY da base:\")\n",
    "print(yoy_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8dc1b7-6d42-47a1-9162-4615411a57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5️⃣ TEMPO ATÉ PRIMEIRA COMPRA\n",
    "# ==========================================\n",
    "first_purchase = (\n",
    "    fact.groupby(customer_key)[order_date_col]\n",
    "    .min()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "first_purchase = first_purchase.merge(\n",
    "    customers[[customer_key, create_col]],\n",
    "    on=customer_key,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "first_purchase[\"days_to_first_purchase\"] = (\n",
    "    first_purchase[order_date_col] - first_purchase[create_col]\n",
    ").dt.days\n",
    "\n",
    "print(\"\\n📌 Tempo médio até primeira compra (dias):\",\n",
    "      first_purchase[\"days_to_first_purchase\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b9b4a-0c08-44ca-a862-e0f23fbe6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6️⃣ RETENÇÃO POR COORTE\n",
    "# ==========================================\n",
    "fact[\"order_month\"] = fact[order_date_col].dt.to_period(\"M\")\n",
    "\n",
    "cohort = fact.groupby(customer_key)[\"order_month\"].min().reset_index()\n",
    "cohort.columns = [customer_key, \"cohort_month\"]\n",
    "\n",
    "fact = fact.merge(cohort, on=customer_key)\n",
    "\n",
    "fact[\"period_number\"] = (\n",
    "    fact[\"order_month\"] - fact[\"cohort_month\"]\n",
    ").apply(lambda x: x.n)\n",
    "\n",
    "cohort_data = fact.groupby(\n",
    "    [\"cohort_month\", \"period_number\"]\n",
    ")[customer_key].nunique().reset_index()\n",
    "\n",
    "cohort_pivot = cohort_data.pivot_table(\n",
    "    index=\"cohort_month\",\n",
    "    columns=\"period_number\",\n",
    "    values=customer_key\n",
    ")\n",
    "\n",
    "cohort_size = cohort_pivot.iloc[:, 0]\n",
    "retention = cohort_pivot.divide(cohort_size, axis=0)\n",
    "\n",
    "print(\"\\n📌 Retenção por coorte:\")\n",
    "print(retention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedc18a-eb74-4eec-9e63-12d96ead6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7️⃣ LIFETIME MÉDIO (em meses)\n",
    "# ==========================================\n",
    "last_purchase = fact.groupby(customer_key)[order_date_col].max().reset_index()\n",
    "\n",
    "lifetime = first_purchase.merge(last_purchase, on=customer_key)\n",
    "lifetime[\"lifetime_months\"] = (\n",
    "    (lifetime[order_date_col + \"_y\"] - lifetime[order_date_col + \"_x\"])\n",
    "    .dt.days / 30\n",
    ")\n",
    "\n",
    "print(\"\\n📌 Lifetime médio (meses):\",\n",
    "      lifetime[\"lifetime_months\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a315441-9747-498f-8da8-dea81b049d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8️⃣ DISTRIBUIÇÃO ETÁRIA\n",
    "# ==========================================\n",
    "plt.figure()\n",
    "customers[\"age\"].hist(bins=10)\n",
    "plt.title(\"Distribuição Etária\")\n",
    "plt.xlabel(\"Idade\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67e876-7880-453c-8212-75284eae316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 9️⃣ TAXA DE AQUISIÇÃO\n",
    "# ==========================================\n",
    "total_customers = len(customers)\n",
    "period_months = customers[\"signup_month\"].nunique()\n",
    "\n",
    "acquisition_rate = total_customers / period_months\n",
    "\n",
    "print(\"\\n📌 Taxa média de aquisição mensal:\",\n",
    "      acquisition_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d118b93-6713-4c9b-8813-0e0f36e4207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 🔟 ÍNDICE DE ENVELHECIMENTO\n",
    "# ==========================================\n",
    "young = customers[customers[\"age\"] < 30].shape[0]\n",
    "old = customers[customers[\"age\"] > 60].shape[0]\n",
    "\n",
    "aging_index = (old / young) if young > 0 else np.nan\n",
    "\n",
    "print(\"\\n📌 Índice de envelhecimento (60+/ <30):\",\n",
    "      aging_index)\n",
    "\n",
    "print(\"\\n🚀 Todas as métricas calculadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da23a9-d25b-4109-851b-b417944accc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# LOAD DATA\n",
    "# ======================================================\n",
    "customers = imputed_dfs['dim_customers_dataset']\n",
    "fact = imputed_dfs['fact_sales_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66a372-92b9-4caa-9e0f-257234446e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# NORMALIZAÇÃO\n",
    "# ======================================================\n",
    "def normalize(df):\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    return df\n",
    "\n",
    "customers = normalize(customers)\n",
    "fact = normalize(fact)\n",
    "\n",
    "def find_col(df, keyword):\n",
    "    for col in df.columns:\n",
    "        if keyword in col:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "customer_key = find_col(customers, \"customer\")\n",
    "birth_col = find_col(customers, \"birth\")\n",
    "create_col = find_col(customers, \"create\")\n",
    "order_date_col = find_col(fact, \"date\")\n",
    "quantity_col = find_col(fact, \"quantity\")\n",
    "price_col = find_col(fact, \"price\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f6686-c2bf-4129-a85f-ef53839eb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CONVERSÃO DE DATAS\n",
    "# ======================================================\n",
    "customers[birth_col] = pd.to_datetime(customers[birth_col], errors=\"coerce\")\n",
    "customers[create_col] = pd.to_datetime(customers[create_col], errors=\"coerce\")\n",
    "fact[order_date_col] = pd.to_datetime(fact[order_date_col], errors=\"coerce\")\n",
    "\n",
    "today = pd.Timestamp.today()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c95e0-a7a8-4591-870b-034382fdd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1️⃣ IDADE MÉDIA\n",
    "# ======================================================\n",
    "customers[\"age\"] = (today - customers[birth_col]).dt.days / 365.25\n",
    "idade_media = customers[\"age\"].mean()\n",
    "print(\"\\n📌 Idade Média:\", idade_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86beabd5-da6e-4ba7-a4fc-0b0d058dcba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 2️⃣ NOVOS CLIENTES / MÊS\n",
    "# ======================================================\n",
    "customers[\"signup_month\"] = customers[create_col].dt.to_period(\"M\")\n",
    "novos_clientes_mes = customers.groupby(\"signup_month\")[customer_key].count()\n",
    "print(\"\\n📌 Novos Clientes / Mês:\")\n",
    "print(novos_clientes_mes.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e7857-1f6e-4a37-91d2-407dbcaf485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 3️⃣ CRESCIMENTO DA BASE\n",
    "# ======================================================\n",
    "base_acumulada = novos_clientes_mes.cumsum()\n",
    "crescimento_base = base_acumulada.pct_change()\n",
    "print(\"\\n📌 Crescimento da Base (%):\")\n",
    "print(crescimento_base.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b4c09-4806-4e13-afb9-781d62b761bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 4️⃣ TEMPO ATÉ 1ª COMPRA\n",
    "# ======================================================\n",
    "first_purchase = (\n",
    "    fact.groupby(customer_key)[order_date_col]\n",
    "    .min()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "first_purchase = first_purchase.merge(\n",
    "    customers[[customer_key, create_col]],\n",
    "    on=customer_key,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "first_purchase[\"days_to_first_purchase\"] = (\n",
    "    first_purchase[order_date_col] - first_purchase[create_col]\n",
    ").dt.days\n",
    "\n",
    "print(\"\\n📌 Tempo médio até 1ª compra (dias):\",\n",
    "      first_purchase[\"days_to_first_purchase\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced526f-aa41-4b0b-868f-7f09f93ff226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 5️⃣ LIFETIME MÉDIO\n",
    "# ======================================================\n",
    "last_purchase = (\n",
    "    fact.groupby(customer_key)[order_date_col]\n",
    "    .max()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "lifetime = first_purchase.merge(\n",
    "    last_purchase,\n",
    "    on=customer_key,\n",
    "    suffixes=(\"_first\", \"_last\")\n",
    ")\n",
    "\n",
    "lifetime[\"lifetime_months\"] = (\n",
    "    (lifetime[order_date_col + \"_last\"] -\n",
    "     lifetime[order_date_col + \"_first\"])\n",
    "    .dt.days / 30\n",
    ")\n",
    "\n",
    "print(\"\\n📌 Lifetime Médio (meses):\",\n",
    "      lifetime[\"lifetime_months\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d28cb-71bc-4aa9-b5d9-43d1d2c41df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 6️⃣ RETENÇÃO POR COORTE\n",
    "# ======================================================\n",
    "fact[\"order_month\"] = fact[order_date_col].dt.to_period(\"M\")\n",
    "\n",
    "cohort = fact.groupby(customer_key)[\"order_month\"].min().reset_index()\n",
    "cohort.columns = [customer_key, \"cohort_month\"]\n",
    "\n",
    "fact = fact.merge(cohort, on=customer_key)\n",
    "\n",
    "fact[\"period_number\"] = (\n",
    "    fact[\"order_month\"] - fact[\"cohort_month\"]\n",
    ").apply(lambda x: x.n)\n",
    "\n",
    "cohort_data = fact.groupby(\n",
    "    [\"cohort_month\", \"period_number\"]\n",
    ")[customer_key].nunique().reset_index()\n",
    "\n",
    "cohort_pivot = cohort_data.pivot_table(\n",
    "    index=\"cohort_month\",\n",
    "    columns=\"period_number\",\n",
    "    values=customer_key\n",
    ")\n",
    "\n",
    "cohort_size = cohort_pivot.iloc[:, 0]\n",
    "retention_matrix = cohort_pivot.divide(cohort_size, axis=0)\n",
    "\n",
    "print(\"\\n📌 Retenção por Coorte:\")\n",
    "print(retention_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff807e0-f030-457c-a2bb-a9e987c73b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 7️⃣ CHURN RATE\n",
    "# ======================================================\n",
    "latest_date = fact[order_date_col].max()\n",
    "\n",
    "last_purchase[\"months_since_last_purchase\"] = (\n",
    "    (latest_date - last_purchase[order_date_col]).dt.days / 30\n",
    ")\n",
    "\n",
    "churn_threshold = 6\n",
    "churn_rate = (\n",
    "    last_purchase[\"months_since_last_purchase\"] > churn_threshold\n",
    ").mean()\n",
    "\n",
    "print(f\"\\n📌 Churn Rate (> {churn_threshold} meses): {churn_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c2741-6ade-4111-8d85-ea11ca2f09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 8️⃣ TAXA DE AQUISIÇÃO\n",
    "# ======================================================\n",
    "total_clientes = len(customers)\n",
    "meses_ativos = customers[\"signup_month\"].nunique()\n",
    "\n",
    "taxa_aquisicao = total_clientes / meses_ativos\n",
    "\n",
    "print(\"\\n📌 Taxa Média de Aquisição:\", taxa_aquisicao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473f0bf-9ec7-4be6-adc4-41dd7a8a767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 9️⃣ ÍNDICE DE ENVELHECIMENTO\n",
    "# ======================================================\n",
    "young = customers[customers[\"age\"] < 30].shape[0]\n",
    "old = customers[customers[\"age\"] > 60].shape[0]\n",
    "\n",
    "indice_envelhecimento = (old / young) if young > 0 else np.nan\n",
    "\n",
    "print(\"\\n📌 Índice de Envelhecimento (60+ / <30):\",\n",
    "      indice_envelhecimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4aaa0-6956-4e2a-b3fc-2189e17cd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================================================\n",
    "# 🔟 ARPU POR COORTE\n",
    "# ======================================================\n",
    "if quantity_col and price_col:\n",
    "    fact[\"revenue\"] = fact[quantity_col] * fact[price_col]\n",
    "\n",
    "    revenue_cohort = fact.groupby(\"cohort_month\")[\"revenue\"].sum()\n",
    "    customers_cohort = cohort.groupby(\"cohort_month\")[customer_key].count()\n",
    "\n",
    "    arpu_cohort = revenue_cohort / customers_cohort\n",
    "\n",
    "    print(\"\\n📌 ARPU por Coorte:\")\n",
    "    print(arpu_cohort)\n",
    "\n",
    "print(\"\\n🚀 Todas as métricas calculadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bcba95-0752-4fa0-9fba-acec3fec64ea",
   "metadata": {},
   "source": [
    "# Insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e32b69-21a4-441e-baea-279651b0044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# LOAD\n",
    "# ======================================================\n",
    "customers = imputed_dfs['dim_customers_dataset']\n",
    "fact = imputed_dfs['fact_sales_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf17b5-3b28-411e-83cb-2d89b8577a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# NORMALIZAÇÃO\n",
    "# ======================================================\n",
    "def normalize(df):\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    return df\n",
    "\n",
    "customers = normalize(customers)\n",
    "fact = normalize(fact)\n",
    "\n",
    "def find_col(df, keyword):\n",
    "    for col in df.columns:\n",
    "        if keyword in col:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "customer_key = find_col(customers, \"customer\")\n",
    "birth_col = find_col(customers, \"birth\")\n",
    "create_col = find_col(customers, \"create\")\n",
    "order_date_col = find_col(fact, \"date\")\n",
    "quantity_col = find_col(fact, \"quantity\")\n",
    "price_col = find_col(fact, \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ff0ea-882e-49ed-bfe9-27d5e610e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# DATAS\n",
    "# ======================================================\n",
    "customers[birth_col] = pd.to_datetime(customers[birth_col], errors=\"coerce\")\n",
    "customers[create_col] = pd.to_datetime(customers[create_col], errors=\"coerce\")\n",
    "fact[order_date_col] = pd.to_datetime(fact[order_date_col], errors=\"coerce\")\n",
    "\n",
    "today = pd.Timestamp.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbee2d2-1303-46d8-bce1-2c0322f415cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1️⃣ BASE ENVELHECENDO?\n",
    "# ======================================================\n",
    "customers[\"age\"] = (today - customers[birth_col]).dt.days / 365.25\n",
    "customers[\"signup_year\"] = customers[create_col].dt.year\n",
    "\n",
    "age_by_year = customers.groupby(\"signup_year\")[\"age\"].mean()\n",
    "trend_age = age_by_year.pct_change().mean()\n",
    "\n",
    "if trend_age > 0:\n",
    "    print(\"⚠ Base envelhecendo → risco estrutural.\")\n",
    "else:\n",
    "    print(\"✅ Base rejuvenescendo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb815ec-5e64-426c-aa95-5a4987282e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 2️⃣ AQUISIÇÃO DESACELERANDO?\n",
    "# ======================================================\n",
    "customers[\"signup_month\"] = customers[create_col].dt.to_period(\"M\")\n",
    "monthly_acq = customers.groupby(\"signup_month\")[customer_key].count()\n",
    "\n",
    "acq_growth = monthly_acq.pct_change().mean()\n",
    "\n",
    "if acq_growth < 0:\n",
    "    print(\"⚠ Aquisição desacelerando.\")\n",
    "else:\n",
    "    print(\"✅ Aquisição saudável.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322aab7d-2b52-4848-8031-69be4b3430b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 3️⃣ LIFETIME CRESCENTE?\n",
    "# ======================================================\n",
    "first_purchase = fact.groupby(customer_key)[order_date_col].min()\n",
    "last_purchase = fact.groupby(customer_key)[order_date_col].max()\n",
    "\n",
    "lifetime = (last_purchase - first_purchase).dt.days / 30\n",
    "lifetime_mean = lifetime.mean()\n",
    "\n",
    "if lifetime_mean > 6:\n",
    "    print(\"✅ Lifetime crescente → fidelização.\")\n",
    "else:\n",
    "    print(\"⚠ Lifetime baixo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8111077-0022-40b3-8b24-d4b3209c1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 4️⃣ COHORTES RECENTES MAIS VALIOSAS?\n",
    "# ======================================================\n",
    "fact[\"order_month\"] = fact[order_date_col].dt.to_period(\"M\")\n",
    "cohort = fact.groupby(customer_key)[\"order_month\"].min().reset_index()\n",
    "cohort.columns = [customer_key, \"cohort_month\"]\n",
    "\n",
    "fact = fact.merge(cohort, on=customer_key)\n",
    "\n",
    "if quantity_col and price_col:\n",
    "    fact[\"revenue\"] = fact[quantity_col] * fact[price_col]\n",
    "\n",
    "cohort_revenue = fact.groupby(\"cohort_month\")[\"revenue\"].mean()\n",
    "\n",
    "if cohort_revenue.tail(3).mean() > cohort_revenue.head(3).mean():\n",
    "    print(\"✅ Cohortes recentes mais valiosas.\")\n",
    "else:\n",
    "    print(\"⚠ Cohortes recentes mais fracas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f021b4b-42a3-4a86-a612-32a180d4761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 5️⃣ CHURN ALTO?\n",
    "# ======================================================\n",
    "latest_date = fact[order_date_col].max()\n",
    "\n",
    "months_since_last = (\n",
    "    (latest_date - last_purchase).dt.days / 30\n",
    ")\n",
    "\n",
    "churn_rate = (months_since_last > 6).mean()\n",
    "\n",
    "if churn_rate > 0.3:\n",
    "    print(\"⚠ Churn alto ameaça sustentabilidade.\")\n",
    "else:\n",
    "    print(\"✅ Churn controlado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48809f8-f80a-4223-b521-bd7bcf92269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 6️⃣ TEMPO ALTO ATÉ COMPRA?\n",
    "# ======================================================\n",
    "first_purchase_df = first_purchase.reset_index()\n",
    "first_purchase_df = first_purchase_df.merge(\n",
    "    customers[[customer_key, create_col]],\n",
    "    on=customer_key,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "time_to_purchase = (\n",
    "    first_purchase_df[order_date_col] -\n",
    "    first_purchase_df[create_col]\n",
    ").dt.days.mean()\n",
    "\n",
    "if time_to_purchase > 30:\n",
    "    print(\"⚠ Tempo alto até compra → fricção no funil.\")\n",
    "else:\n",
    "    print(\"✅ Conversão rápida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a0eac-ac05-490b-be4c-213bdb6e0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 7️⃣ RETENÇÃO FORTE?\n",
    "# ======================================================\n",
    "cohort_size = fact.groupby(\"cohort_month\")[customer_key].nunique()\n",
    "repeat_buyers = fact.groupby(customer_key).size()\n",
    "\n",
    "retention_rate = (repeat_buyers > 1).mean()\n",
    "\n",
    "if retention_rate > 0.4:\n",
    "    print(\"✅ Retenção forte sustenta crescimento.\")\n",
    "else:\n",
    "    print(\"⚠ Retenção fraca.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c363e-4dbc-4146-94a8-35770faf09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 8️⃣ DISTRIBUIÇÃO ETÁRIA DESEQUILIBRADA?\n",
    "# ======================================================\n",
    "young = customers[customers[\"age\"] < 30].shape[0]\n",
    "old = customers[customers[\"age\"] > 60].shape[0]\n",
    "\n",
    "if old / max(young, 1) > 1:\n",
    "    print(\"⚠ Base envelhecida → risco demográfico.\")\n",
    "else:\n",
    "    print(\"✅ Distribuição equilibrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5648f7-27b0-447b-8074-086d62d789cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 9️⃣ CRESCIMENTO SAUDÁVEL?\n",
    "# ======================================================\n",
    "if acq_growth > 0 and churn_rate < 0.3:\n",
    "    print(\"✅ Crescimento saudável e sustentável.\")\n",
    "else:\n",
    "    print(\"⚠ Crescimento sob risco.\")\n",
    "\n",
    "print(\"\\n🚀 Insight Engine finalizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b3b8e-551e-48e8-9d3a-84953f2aa73d",
   "metadata": {},
   "source": [
    "📂 3. ARQUIVO: 07.dim_products_dataset.csv\n",
    "🗂 DataFrame: DIM_PRODUCTS\n",
    "🕒 Coluna datetime: start_date\n",
    "❓ Questionário\n",
    "\n",
    "Qual a idade média dos produtos?\n",
    "\n",
    "Produtos novos vendem mais?\n",
    "\n",
    "Existe dependência de produtos antigos?\n",
    "\n",
    "Quanto tempo até maturidade?\n",
    "\n",
    "Existe ciclo de vida claro?\n",
    "\n",
    "Portfólio está envelhecido?\n",
    "\n",
    "Lançamentos performam bem?\n",
    "\n",
    "Existe obsolescência rápida?\n",
    "\n",
    "Receita concentra-se em produtos antigos?\n",
    "\n",
    "Portfólio está renovando?\n",
    "\n",
    "📊 Métricas\n",
    "\n",
    "Idade do produto\n",
    "\n",
    "Tempo até pico\n",
    "\n",
    "Receita por idade\n",
    "\n",
    "Vida útil média\n",
    "\n",
    "Taxa de declínio\n",
    "\n",
    "% produtos novos\n",
    "\n",
    "Idade média portfólio\n",
    "\n",
    "Distribuição etária\n",
    "\n",
    "Taxa de substituição\n",
    "\n",
    "Receita rolling por geração\n",
    "\n",
    "📈 KPIs\n",
    "\n",
    "Idade Média Portfólio\n",
    "\n",
    "Tempo até Tração\n",
    "\n",
    "% Receita Produtos Novos\n",
    "\n",
    "Taxa de Declínio\n",
    "\n",
    "Vida Útil Média\n",
    "\n",
    "Renovação do Portfólio\n",
    "\n",
    "Índice de Inovação\n",
    "\n",
    "Dependência Produtos Antigos\n",
    "\n",
    "Obsolescência\n",
    "\n",
    "Receita por Geração\n",
    "\n",
    "💡 Insights\n",
    "\n",
    "Portfólio envelhecido reduz competitividade\n",
    "\n",
    "Dependência de antigos é risco\n",
    "\n",
    "Lançamentos fracos indicam problema estratégico\n",
    "\n",
    "Renovação sustenta crescimento\n",
    "\n",
    "Ciclo curto exige pipeline forte\n",
    "\n",
    "Declínio rápido indica produto sazonal\n",
    "\n",
    "Portfólio equilibrado reduz risco\n",
    "\n",
    "Receita concentrada aumenta vulnerabilidade\n",
    "\n",
    "Inovação contínua aumenta sustentabilidade\n",
    "\n",
    "Obsolescência alta exige P&D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef7bac-4ea1-4c13-983a-b8e6eb7e62c0",
   "metadata": {},
   "source": [
    "# Métrica - Idade do produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a1288-14e1-4aeb-bd3f-57b8ad05ab58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbe216ba-81f6-4fb6-992a-8ec0576517dc",
   "metadata": {},
   "source": [
    "# Métrica - Tempo até pico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d5258-b891-4e48-b482-55259d8c353a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "761ff478-5b34-4348-9e2d-c4862612e283",
   "metadata": {},
   "source": [
    "# Métrica - Receita por idade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd9252-938f-4893-9531-5075001228be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1076f9e-7a7d-401f-ba3d-94460cde4174",
   "metadata": {},
   "source": [
    "# Métrica - Vida útil média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5b5f6-7236-426d-bd72-e9d1b3086520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c04cf52-5e5a-4b52-ad6b-0b752cea4372",
   "metadata": {},
   "source": [
    "# Métrica - Taxa de declínio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f14be-a6ac-402d-8ed1-21ae90eb5962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49e6fef0-9728-439d-ad2d-e7f4020b76a8",
   "metadata": {},
   "source": [
    "# Métrica - % produtos novos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb41dd-f4ec-434f-9386-b4da9a2b2e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfd2c15c-2790-4707-a6c5-c6fbb573d542",
   "metadata": {},
   "source": [
    "# Métrica - Idade média portfólio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06ced7-f953-4907-8fed-82cb0d38b67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfa8a59b-d04a-4814-ba12-81b74b1c4160",
   "metadata": {},
   "source": [
    "# Métrica - Distribuição etária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259aed2-e665-4931-abd2-53b6a5d88a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6aecfa7-68a0-402d-9920-04cd75b9dc2e",
   "metadata": {},
   "source": [
    "# Métrica - Taxa de substituição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5dbca-6d72-409b-89fd-eda98bfa39ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b57a6290-413f-416a-94ea-dc89fef8b3f3",
   "metadata": {},
   "source": [
    "# Métrica - Receita rolling por geração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e362fac-dd1b-457c-9781-570844be2c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "721f25d9-22ca-4ef4-a94f-b6217c41ebe7",
   "metadata": {},
   "source": [
    "# Loading the three Dataset Fact-Dimension: Fact Sales, Dim Customers e Dim Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45447203-5cbf-40c1-8feb-00fcc4450c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1️⃣ LOAD DATA (já vem do seu imputed_dfs)\n",
    "# =====================================================\n",
    "fact = imputed_dfs['fact_sales_dataset'].copy()\n",
    "customers = imputed_dfs['dim_customers_dataset'].copy()\n",
    "products = imputed_dfs['dim_products_dataset'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514d980-cafd-48cc-b677-0493c13ba92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2️⃣ NORMALIZAÇÃO DE COLUNAS (ENGINEERING STEP)\n",
    "# =====================================================\n",
    "def normalize_columns(df):\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "fact = normalize_columns(fact)\n",
    "customers = normalize_columns(customers)\n",
    "products = normalize_columns(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8ba3f-88ee-4837-8857-73ca572c63c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3️⃣ AUTO-DETECT KEYS\n",
    "# =====================================================\n",
    "def find_key(df, keyword):\n",
    "    for col in df.columns:\n",
    "        if keyword in col:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "customer_key_fact = find_key(fact, \"customer\")\n",
    "customer_key_dim = find_key(customers, \"customer\")\n",
    "\n",
    "product_key_fact = find_key(fact, \"product\")\n",
    "product_key_dim = find_key(products, \"product\")\n",
    "\n",
    "date_column = find_key(fact, \"date\")\n",
    "\n",
    "print(\"🔎 Chaves detectadas:\")\n",
    "print(\"Customer Key:\", customer_key_fact)\n",
    "print(\"Product Key:\", product_key_fact)\n",
    "print(\"Date Column:\", date_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b5f53-f64d-41a8-8957-dcc562ea1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4️⃣ VALIDATION LAYER\n",
    "# =====================================================\n",
    "if not customer_key_fact or not customer_key_dim:\n",
    "    raise ValueError(\"Customer key não encontrada.\")\n",
    "\n",
    "if not product_key_fact or not product_key_dim:\n",
    "    raise ValueError(\"Product key não encontrada.\")\n",
    "\n",
    "if not date_column:\n",
    "    raise ValueError(\"Coluna de data não encontrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78783bef-a122-4e4c-a16b-944775dd364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 5️⃣ TYPE CAST\n",
    "# =====================================================\n",
    "fact[date_column] = pd.to_datetime(fact[date_column], errors=\"coerce\")\n",
    "\n",
    "if \"create_date\" in customers.columns:\n",
    "    customers[\"create_date\"] = pd.to_datetime(customers[\"create_date\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde88fb-24ff-4c06-8dad-136b204a029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 6️⃣ STAR SCHEMA MERGE\n",
    "# =====================================================\n",
    "df = (\n",
    "    fact\n",
    "    .merge(customers, left_on=customer_key_fact, right_on=customer_key_dim, how=\"left\")\n",
    "    .merge(products, left_on=product_key_fact, right_on=product_key_dim, how=\"left\")\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Merge executado com sucesso.\")\n",
    "print(\"Shape final:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06271a85-334e-4052-9d8a-f690ccf67ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# 7️⃣ TEMPO ATÉ PRIMEIRA COMPRA\n",
    "# =====================================================\n",
    "first_purchase = (\n",
    "    fact\n",
    "    .groupby(customer_key_fact)[date_column]\n",
    "    .min()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "if \"create_date\" in customers.columns:\n",
    "\n",
    "    first_purchase = first_purchase.merge(\n",
    "        customers[[customer_key_dim, \"create_date\"]],\n",
    "        left_on=customer_key_fact,\n",
    "        right_on=customer_key_dim,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    first_purchase[\"days_to_first_purchase\"] = (\n",
    "        first_purchase[date_column] - first_purchase[\"create_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    print(\"\\n📌 Tempo médio até primeira compra (dias):\",\n",
    "          first_purchase[\"days_to_first_purchase\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee44ae-e9b5-4d91-a186-48d5a4976723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# 8️⃣ COHORT ANALYSIS\n",
    "# =====================================================\n",
    "fact[\"order_month\"] = fact[date_column].dt.to_period(\"M\")\n",
    "\n",
    "cohort = (\n",
    "    fact.groupby(customer_key_fact)[\"order_month\"]\n",
    "    .min()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "cohort.columns = [customer_key_fact, \"cohort_month\"]\n",
    "\n",
    "fact_cohort = fact.merge(cohort, on=customer_key_fact)\n",
    "\n",
    "fact_cohort[\"period_number\"] = (\n",
    "    fact_cohort[\"order_month\"] - fact_cohort[\"cohort_month\"]\n",
    ").apply(lambda x: x.n)\n",
    "\n",
    "cohort_data = (\n",
    "    fact_cohort\n",
    "    .groupby([\"cohort_month\", \"period_number\"])[customer_key_fact]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "cohort_pivot = cohort_data.pivot_table(\n",
    "    index=\"cohort_month\",\n",
    "    columns=\"period_number\",\n",
    "    values=customer_key_fact\n",
    ")\n",
    "\n",
    "cohort_size = cohort_pivot.iloc[:, 0]\n",
    "retention_matrix = cohort_pivot.divide(cohort_size, axis=0)\n",
    "\n",
    "print(\"\\n📌 Matriz de Retenção:\")\n",
    "print(retention_matrix)\n",
    "\n",
    "plt.figure()\n",
    "retention_matrix.T.plot()\n",
    "plt.title(\"Retention Curve by Cohort\")\n",
    "plt.xlabel(\"Meses desde aquisição\")\n",
    "plt.ylabel(\"Taxa de Retenção\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca3866-a09b-42b0-aba7-d07dd390ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# 9️⃣ CHURN ESTRUTURAL\n",
    "# =====================================================\n",
    "latest_date = fact[date_column].max()\n",
    "\n",
    "last_purchase = (\n",
    "    fact.groupby(customer_key_fact)[date_column]\n",
    "    .max()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "last_purchase[\"months_since_last_purchase\"] = (\n",
    "    (latest_date - last_purchase[date_column]).dt.days / 30\n",
    ")\n",
    "\n",
    "churn_threshold = 6\n",
    "churn_rate = (\n",
    "    last_purchase[\"months_since_last_purchase\"] > churn_threshold\n",
    ").mean()\n",
    "\n",
    "print(f\"\\n📌 Churn estrutural (> {churn_threshold} meses): {churn_rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d0ebe-57b4-4689-ab6d-4ec46ab47304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 🔟 RECEITA E CRESCIMENTO\n",
    "# =====================================================\n",
    "quantity_col = find_key(fact, \"quantity\")\n",
    "price_col = find_key(fact, \"price\")\n",
    "\n",
    "if quantity_col and price_col:\n",
    "    fact[\"revenue\"] = fact[quantity_col] * fact[price_col]\n",
    "\n",
    "    monthly_revenue = (\n",
    "        fact.groupby(\"order_month\")[\"revenue\"]\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "    growth_rate = monthly_revenue.pct_change().mean()\n",
    "\n",
    "    print(f\"\\n📌 Crescimento médio mensal da receita: {growth_rate:.4f}\")\n",
    "\n",
    "    plt.figure()\n",
    "    monthly_revenue.plot()\n",
    "    plt.title(\"Receita Mensal\")\n",
    "    plt.xlabel(\"Mês\")\n",
    "    plt.ylabel(\"Receita\")\n",
    "    plt.show()\n",
    "\n",
    "    ltv_medio = fact.groupby(customer_key_fact)[\"revenue\"].sum().mean()\n",
    "\n",
    "    print(f\"\\n📌 LTV médio: {ltv_medio:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\n🚀 Pipeline completo executado com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580ff86-ea31-4ead-bbc3-8cf589365622",
   "metadata": {},
   "source": [
    "📂 4. ARQUIVO: 08.sales_extra_variables_extra_data_dataset.csv\n",
    "🗂 DataFrame: SALES_EXTRA_VARIABLES_EXTRA_DATA\n",
    "🕒 Coluna datetime: date\n",
    "❓ Questionário\n",
    "\n",
    "Variáveis extras impactam receita ao longo do tempo?\n",
    "\n",
    "Existe correlação temporal com fatores externos?\n",
    "\n",
    "Existe sazonalidade influenciada por variáveis adicionais?\n",
    "\n",
    "Existem mudanças estruturais?\n",
    "\n",
    "Existe aceleração recente?\n",
    "\n",
    "Variáveis extras aumentam previsibilidade?\n",
    "\n",
    "Existe cluster temporal?\n",
    "\n",
    "Eventos externos impactam desempenho?\n",
    "\n",
    "Existe efeito calendário?\n",
    "\n",
    "Variáveis extras reduzem volatilidade?\n",
    "\n",
    "📊 Métricas\n",
    "\n",
    "Receita por variável\n",
    "\n",
    "Correlação temporal\n",
    "\n",
    "Rolling mean\n",
    "\n",
    "Rolling std\n",
    "\n",
    "Índice de sazonalidade\n",
    "\n",
    "Tendência ajustada\n",
    "\n",
    "Elasticidade temporal\n",
    "\n",
    "Desvio padrão ajustado\n",
    "\n",
    "Pico ajustado\n",
    "\n",
    "Drawdown ajustado\n",
    "\n",
    "📈 KPIs\n",
    "\n",
    "Receita Ajustada\n",
    "\n",
    "Elasticidade Temporal\n",
    "\n",
    "Volatilidade Ajustada\n",
    "\n",
    "Tendência Estrutural\n",
    "\n",
    "Impacto de Variáveis Extras\n",
    "\n",
    "Crescimento Ajustado\n",
    "\n",
    "Pico Ajustado\n",
    "\n",
    "Índice de Estabilidade\n",
    "\n",
    "Concentração Temporal\n",
    "\n",
    "Performance Ajustada\n",
    "\n",
    "💡 Insights\n",
    "\n",
    "Variáveis externas podem explicar sazonalidade\n",
    "\n",
    "Ajuste reduz ruído estatístico\n",
    "\n",
    "Forte correlação indica dependência externa\n",
    "\n",
    "Mudança estrutural indica novo regime\n",
    "\n",
    "Elasticidade alta indica sensibilidade\n",
    "\n",
    "Variáveis extras melhoram previsão\n",
    "\n",
    "Volatilidade reduzida indica estabilidade\n",
    "\n",
    "Eventos externos impactam receita\n",
    "\n",
    "Ajuste revela tendência real\n",
    "\n",
    "Estrutura temporal mais clara após controle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994bba6-7d96-4897-bb37-4caf5a0aaa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e2935be-8302-4746-939e-600964cbe889",
   "metadata": {},
   "source": [
    "# MEASURE EXPLORATION\n",
    "- Calculate the key metric of the Business (Big numbers)\n",
    "- Highest level of aggregation | lowerst level of details\n",
    "- SUM(Measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd9c73-0450-4623-85f8-c2eaec8714fd",
   "metadata": {},
   "source": [
    "### BIG NUMBERS (Top-Level KPIs)\r\n",
    "### Revenue & Volume\r\n",
    "\r\n",
    "- What is the total revenue generated?\r\n",
    "- What is the total quantity sold?\r\n",
    "- What is the average selling price?\r\n",
    "- What is the total number of orders?\r\n",
    "- What is the total number of active customers?\r\n",
    "- What percentage of customers actually placed orders?\r\n",
    "- What is revenue per order?\r\n",
    "- What is revenue per customer?\r\n",
    "- Is revenue growing faster th\n",
    "### DRILL DOWN (Aggregation by Dimension)\n",
    "### Geography\n",
    "\n",
    "- Which country generates the highest revenue?\n",
    "- Which country has the highest average order value?\n",
    "- Which country has the highest customer concentration?\n",
    "- Are we over-dependent on a single country?\n",
    "\n",
    "### Product & Category\n",
    "\n",
    "- Which product generates the most revenue?\n",
    "- Which category generates the highest revenue?\n",
    "- Which subcategory has the highest margin potential?\n",
    "- Do a small number of products drive most of the revenue (Pareto 80/20)?\n",
    "\n",
    "### Sales Team\n",
    "\n",
    "- Which sales manager generates the most revenue?\n",
    "- Which sales rep closes the most deals?\n",
    "- Who generates the highest revenue per order?\n",
    "- Is performance evenly distributed across the sales team?\n",
    "\n",
    "### Channel / Device\n",
    "\n",
    "- Which device generates the highest revenue?\n",
    "- Which device has the highest average order value?\n",
    "- Is mobile revenue growing faster than desktop?\n",
    "\n",
    "### COST, PROFIT & MARGIN ANALYSIS\n",
    "\n",
    "- What is total product cost?\n",
    "- What is gross revenue vs total cost?\n",
    "- What is total refund amount?\n",
    "- What percentage of revenue is lost due to refunds?\n",
    "- Which products have the highest cost-to-revenue ratio?\n",
    "- Which categories have the best margin potential?\n",
    "- Are we selling high volume but low margin products?\n",
    "\n",
    "### CUSTOMER VALUE & CONCENTRATION\n",
    "\n",
    "- What is revenue per customer?\n",
    "- What is the revenue distribution across customers?\n",
    "- Do the top 10% of customers generate most revenue?\n",
    "- Is revenue highly concentrated in a few customers?\n",
    "- Are older customers more valuable?\n",
    "- Which demographic group generates the highest revenue?\n",
    "- What is the average order frequency per customer?\n",
    "\n",
    "### PRODUCT PERFORMANCE & PORTFOLIO STRATEGY\n",
    "\n",
    "- Which product line sustains the business?\n",
    "- Do new products outperform old ones?\n",
    "- Which product has the highest revenue per unit sold?\n",
    "- Are we carrying low-performing products?\n",
    "- Which products should be discontinued?\n",
    "- Which product lines drive cross-selling opportunities?\n",
    "\n",
    "### RISK & DEPENDENCY ANALYSIS\n",
    "\n",
    "- Is revenue concentrated in one product line?\n",
    "- Is revenue dependent on one country?\n",
    "- Is revenue dependent on one sales manager?\n",
    "- What happens if the top 5 customers churn?\n",
    "- Are refunds concentrated in specific products?\n",
    "- Are high-priced products more likely to be refunded?\n",
    "\n",
    "### STRATEGIC EXECUTIVE QUESTIONS (Board Level)\n",
    "\n",
    "- What is the compound annual growth rate (CAGR)?\n",
    "- Are we scaling revenue sustainably?\n",
    "- Is growth driven by price increase or volume increase?\n",
    "- Are we maximizing margin or focusing only on sales volume?\n",
    "- What is the long-term customer value potential?\n",
    "- Where should we allocate investment: geography, product, or sales team?\n",
    "- Are we operationally efficient relative to revenue growth?\n",
    "- What are the biggest drivers of revenue volatility?\n",
    "\n",
    "### Advanced Measure Exploration Mindset\n",
    "\n",
    "Instead of only asking:\n",
    "\n",
    "\"What is the total revenue?\"\n",
    "\n",
    "We also ask:\n",
    "\n",
    "- What drives revenue?\n",
    "- Where is revenue concentrated?\n",
    "- What is the risk exposure?\n",
    "- What is the profitability structure?\n",
    "- What is scalable?\n",
    "- What is fragile? is the risk exposure?\r\n",
    "- What is the profitability structure?\r\n",
    "- What is scalable?\r\n",
    "- What is fragile?\r\n",
    "What is the risk exposure?\r\n",
    "What is the profitability structure?\r\n",
    "What is scalable?\r\n",
    "What is fragile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8227cdf-c85e-4a86-a526-4db24820b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = dfs_imputed[\"fact_sales\"]\n",
    "df_products = dfs_imputed[\"dim_products\"]\n",
    "df_customers = dfs_imputed[\"dim_customers\"]\n",
    "\n",
    "result = pd.DataFrame({\n",
    "\n",
    "    # Total Sales\n",
    "    \"total_sales\": [df_fact[\"sales_amount\"].sum()],\n",
    "    \n",
    "    # Total Quantity Sold\n",
    "    \"total_quantity\": [df_fact[\"quantity\"].sum()],\n",
    "    \n",
    "    # Average Selling Price\n",
    "    \"avg_price\": [df_fact[\"price\"].mean()],\n",
    "    \n",
    "    # Total number of orders\n",
    "    \"total_orders\": [df_fact[\"order_number\"].count()],\n",
    "    \n",
    "    # Total distinct orders\n",
    "    \"total_distinct_orders\": [df_fact[\"order_number\"].nunique()],\n",
    "    \n",
    "    # Total number of products\n",
    "    \"total_products\": [df_products[\"product_name\"].count()],\n",
    "    \n",
    "    # Total number of customers\n",
    "    \"total_customers\": [df_customers[\"customer_key\"].count()],\n",
    "    \n",
    "    # Customers who placed at least one order\n",
    "    \"customers_with_orders\": [df_fact[\"customer_key\"].nunique()]\n",
    "\n",
    "})\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc77b3f-e018-4644-80e5-085ad49e304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for dataset_name, df in dfs_imputed.items():\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        is_datetime = pd.api.types.is_datetime64_any_dtype(df[col])\n",
    "        has_date_in_name = \"date\" in col.lower()\n",
    "        \n",
    "        # condição negativa\n",
    "        if not is_datetime and not has_date_in_name:\n",
    "            results.append({\n",
    "                \"dataframe\": dataset_name,\n",
    "                \"column\": col,\n",
    "                \"dtype\": df[col].dtype\n",
    "            })\n",
    "\n",
    "result_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ae4a9-daf4-4342-85e6-a774a648cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"../data/files/arquivo.txt\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d610b4-fbc6-4bb5-b58f-313398bc1854",
   "metadata": {},
   "source": [
    "# Magnitude Analsis\n",
    "- Compare the measure values by categories\n",
    "- It heps us understand the importanc eof different categories\n",
    "- Example:\n",
    "- sum(Measure) By (Dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dd29b-60be-4768-801d-a39413867845",
   "metadata": {},
   "source": [
    "### Revenue Magnitude Analysis\r\n",
    "Using fact_sales.sales_amount\r\n",
    "\r\n",
    "- What is the total revenue by product_key?\r\n",
    "- What is the total revenue by customer_key?\r\n",
    "- What is the total revenue by country (via join with dim_customers)?\r\n",
    "- What is the total revenue by product category (via join with dim_products.category)?\r\n",
    "- What is the total revenue by product line?\r\n",
    "- What is the total revenue by subcategory?\r\n",
    "- What is the total revenue by sales_manager (via sales dataset)?\r\n",
    "- What is the total revenue by sales_rep?\r\n",
    "- What is the total revenue by device_type?\r\n",
    "- What is the total revenue by gender?\r\n",
    "\r\n",
    "### Quantity Magnitude Analysis\r\n",
    "Using fact_sales.quantity\r\n",
    "\r\n",
    "- What is the total quantity sold by product?\r\n",
    "- What is the total quantity sold by category?\r\n",
    "- What is the total quantity sold by country?\r\n",
    "- What is the total quantity sold by sales_rep?\r\n",
    "- What is the total quantity sold by product_line?\r\n",
    "- Which category drives the highest sales volume?\r\n",
    "\r\n",
    "### Price Magnitude Analysis\r\n",
    "Using fact_sales.price\r\n",
    "\r\n",
    "- What is the total revenue impact by price segment?\r\n",
    "- What is the total price contribution by category?\r\n",
    "- What is the total price contribution by country?\r\n",
    "\r\n",
    "### Refund Magnitude Analysis\r\n",
    "Using extra_data.refund\r\n",
    "\r\n",
    "- What is the total refund amount by country?\r\n",
    "- What is the total refund amount by category?\r\n",
    "- What is the total refund amount by sales_rep?\r\n",
    "- Which device type generates the highest refund value?\r\n",
    "\r\n",
    "Are refunds concentrated in specific product categories?\r\n",
    "\r\n",
    "### Cost Magnitude Analysis\r\n",
    "Using dim_products.cost or extra_data.cost\r\n",
    "\r\n",
    "- What is the total cost by category?\r\n",
    "- What is the total cost by product_line?\r\n",
    "- What is the total cost by subcategory?\r\n",
    "- Which category has the highest total cost structure?\r\n",
    "\r\n",
    "### Customer Magnitude Analysis\r\n",
    "Using fact_sales.sales_amount\r\n",
    "\r\n",
    "- What is the total revenue by marital_status?\r\n",
    "- What is the total revenue by marital_status?\r\n",
    "- What is the total revenue by gender?\r\n",
    "- What is the total revenue by country (customer dimension)?\r\n",
    "- Which demographic segment contributes most to total revenue?\r\n",
    "\r\n",
    "### Clinical Dataset (sample dataset)\r\n",
    "\r\n",
    "Although not business-focused, magnitude can still be applied:\r\n",
    "Using sample.mean area or sample.worst area\r\n",
    "- What is the total mean area by target (benign vs malignant)?\r\n",
    "- What is the total worst perimeter by target?\r\n",
    "- What is the total concavity by target?\r\n",
    "\r\n",
    "### Strategic Magnitude Questions\r\n",
    "\r\n",
    "- Is revenue concentrated in one product category?\r\n",
    "- Is volume concentrated in one country?\r\n",
    "- Is refund magnitude higher in specific segments?\r\n",
    "- Which dimension explains the largest revenue magnitude?\r\n",
    "- Do a small number of customers generate most of the revenue?\r\n",
    "- Which product line sustains the business financially?\r\n",
    "\r\n",
    "### Why Magnitude Analysis Matters\r\n",
    "\r\n",
    "- Magnitude Analysis helps:\r\n",
    "- Identify dominant categories\r\n",
    "- Detect revenue concentration risk\r\n",
    "- Understand structural dependency\r\n",
    "- Prioritize strategic focus areas\r\n",
    "- Detect imbalance in business structureendency\r\n",
    "Prioritize strategic focus areas\r\n",
    "Detect imbalance in business structureexposure?\r\n",
    "- What is the profitability structure?\r\n",
    "- What is scalable?\r\n",
    "- What is fragile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9c9af-2887-4bda-86d5-02fa07df45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = dfs_imputed[\"dim_customers\"]\n",
    "\n",
    "total_customers_country = (\n",
    "    df_customers\n",
    "    .groupby(\"country\")[\"customer_key\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"total_customers\")\n",
    "    .sort_values(by=\"total_customers\", ascending=False)\n",
    ")\n",
    "\n",
    "total_customers_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de3cab-d742-43e4-a0c2-661035bacc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_customers_gender = (\n",
    "    df_customers\n",
    "    .groupby(\"gender\")[\"customer_key\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"total_customers\")\n",
    "    .sort_values(by=\"total_customers\", ascending=False)\n",
    ")\n",
    "\n",
    "total_customers_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21ac07-9078-4bb0-ae69-92f3a772327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = dfs_imputed[\"dim_products\"]\n",
    "\n",
    "total_products_category = (\n",
    "    df_products\n",
    "    .groupby(\"category\")[\"product_key\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"total_products\")\n",
    "    .sort_values(by=\"total_products\", ascending=False)\n",
    ")\n",
    "\n",
    "total_products_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec5f18-286d-4893-8313-045fd2a659e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cost_category = (\n",
    "    df_products\n",
    "    .groupby(\"category\")[\"cost\"]\n",
    "    .mean()\n",
    "    .reset_index(name=\"avg_cost\")\n",
    "    .sort_values(by=\"avg_cost\", ascending=False)\n",
    ")\n",
    "\n",
    "avg_cost_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee641b-5865-41dd-854d-6d39b558459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = dfs_imputed[\"fact_sales\"]\n",
    "\n",
    "revenue_by_category = (\n",
    "    df_fact\n",
    "    .merge(df_products, on=\"product_key\", how=\"left\")\n",
    "    .groupby(\"category\")[\"sales_amount\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"total_revenue\")\n",
    "    .sort_values(by=\"total_revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "revenue_by_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49636f13-5dad-47fb-8b18-10ca6123d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_by_customer = (\n",
    "    df_fact\n",
    "    .merge(df_customers, on=\"customer_key\", how=\"left\")\n",
    "    .groupby([\"customer_key\", \"first_name\", \"last_name\"])[\"sales_amount\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"total_revenue\")\n",
    "    .sort_values(by=\"total_revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "revenue_by_customer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0ac6e-aa91-431e-9fff-2284a1a382c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_items_by_country = (\n",
    "    df_fact\n",
    "    .merge(df_customers, on=\"customer_key\", how=\"left\")\n",
    "    .groupby(\"country\")[\"quantity\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"total_sold_items\")\n",
    "    .sort_values(by=\"total_sold_items\", ascending=False)\n",
    ")\n",
    "\n",
    "sold_items_by_country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe981a8-cea6-447f-a93d-dcfc48e561eb",
   "metadata": {},
   "source": [
    "# Ranking Analysis\n",
    "- Order the values of dimensions by measure.\n",
    "- Top N performers | Bottom N Performers\n",
    "- Example\n",
    "- Rank (Dimension) By Sum(Measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9dd8e-64a6-42b0-9ea4-19710ea0ea52",
   "metadata": {},
   "source": [
    "### Revenue Ranking (Core Business Ranking)\r\n",
    "Using fact_sales.sales_amount\r\n",
    "\r\n",
    "- What are the Top 10 products ranked by total revenue?\r\n",
    "- What are the Bottom 10 products ranked by total revenue?\r\n",
    "- Which Top 5 product categories generate the highest revenue?\r\n",
    "- Which categories are the lowest revenue contributors?\r\n",
    "- What are the Top 10 customers ranked by total revenue?\r\n",
    "- Which customers generate the least revenue?\r\n",
    "- Which Top 5 countries generate the most revenue?\r\n",
    "- Which countries contribute the least to total revenue?\r\n",
    "- Who are the Top 5 sales reps ranked by revenue?\r\n",
    "- Who are the lowest-performing sales reps?\r\n",
    "\r\n",
    "### Volume Ranking (Sales Quantity Perspective)\r\n",
    "Using fact_sales.quantity\r\n",
    "\r\n",
    "- Which products are the Top 10 best-selling by quantity?\r\n",
    "- Which products are the least sold by quantity?\r\n",
    "- Which country ranks highest in total units sold?\r\n",
    "- Which category ranks highest in sales volume?\r\n",
    "- Which sales rep sells the most units?\r\n",
    "- Which product line drives the highest volume?\r\n",
    "\r\n",
    "### Refund Ranking (Risk & Loss Analysis)\r\n",
    "Using extra_data.refund\r\n",
    "\r\n",
    "- Which categories have the highest total refund amounts?\r\n",
    "- Which products generate the most refunds?\r\n",
    "- Which sales rep is associated with the highest refund value?\r\n",
    "- Which country has the highest refund magnitude?\r\n",
    "- Which device type leads in total refund value?\r\n",
    "- What are the bottom 5 categories with the lowest refund impact?\r\n",
    "\r\n",
    "### Cost Ranking (Cost Structure Analysis)\r\n",
    "Using dim_products.cost\r\n",
    "\r\n",
    "- Which categories have the highest total cost?\r\n",
    "- Which product line has the highest cost structure?\r\n",
    "- Which products have the highest unit cost?\r\n",
    "- Which categories have the lowest cost structure?\r\n",
    " \r\n",
    "### Customer Ranking (Customer Value Analysis)\r\n",
    "Using fact_sales.sales_amount\r\n",
    "\r\n",
    "- Rank customers by total revenue contribution.\r\n",
    "- Who are the top 10% customers by revenue?\r\n",
    "- Who are the bottom 10% customers?\r\n",
    "- Which gender ranks highest in total revenue?\r\n",
    "- Which marital status group generates the highest revenue?\r\n",
    "- Which country ranks highest in number of customers?\r\n",
    "\r\n",
    "### Product Portfolio Ranking\r\n",
    "Using sales_amount + dim_products.category\r\n",
    "\r\n",
    "- Which product lines sustain the business financially?\r\n",
    "- Which subcategories underperform?\r\n",
    "- Are there products generating revenue but at very low ranking?\r\n",
    "- Which products dominate their category ranking?\r\n",
    "\r\n",
    "### Concentration & Dependency Ranking\r\n",
    "\r\n",
    "- Does the Top 5 products represent more than 50% of revenue?\r\n",
    "- Does the Top 10 customers represent the majority of revenue?\r\n",
    "- Is revenue concentrated in one country?\r\n",
    "- Are refunds concentrated in specific products?\r\n",
    "- Is volume concentrated in low-margin products?\r\n",
    "\r\n",
    "### Clinical Dataset Ranking (sample dataset)\r\n",
    "\r\n",
    "- Although non-business, ranking still applies:\r\n",
    "- Rank tumors by worst area.\r\n",
    "- Rank cases by mean concavity.\r\n",
    "- Rank patients by worst perimeter.\r\n",
    "- Compare top 5 highest concave points between malignant vs benign.\r\n",
    "\r\n",
    "### Strategic Business Value of Ranking Analysis\r\n",
    "\r\n",
    "- Ranking helps you:\r\n",
    "- Identify performance leaders\r\n",
    "- Detect underperformers\r\n",
    "- Reveal revenue concentration risk\r\n",
    "- Prioritize investment decisions\r\n",
    "- Optimize portfolio strategy\r\n",
    "- Improve sales team performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43884c1f-511b-45f9-822a-b8b0d7e03ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = dfs_imputed[\"fact_sales\"]\n",
    "df_products = dfs_imputed[\"dim_products\"]\n",
    "\n",
    "top_5_products = (\n",
    "    df_fact\n",
    "    .merge(df_products, on=\"product_key\", how=\"left\")\n",
    "    .groupby(\"product_name\")[\"sales_amount\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"total_revenue\")\n",
    "    .sort_values(by=\"total_revenue\", ascending=False)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "top_5_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683729b3-c2ca-4078-9321-9d44f5d4f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_products = (\n",
    "    df_fact\n",
    "    .merge(df_products, on=\"product_key\", how=\"left\")\n",
    "    .groupby(\"product_name\")[\"sales_amount\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"total_revenue\")\n",
    ")\n",
    "\n",
    "ranked_products[\"rank_products\"] = (\n",
    "    ranked_products[\"total_revenue\"]\n",
    "    .rank(method=\"dense\", ascending=False)\n",
    ")\n",
    "\n",
    "top_5_ranked = ranked_products[ranked_products[\"rank_products\"] <= 5] \\\n",
    "    .sort_values(by=\"rank_products\")\n",
    "\n",
    "top_5_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97e46e-23b2-4f93-8879-e0262a5e5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_5_products = (\n",
    "    df_fact\n",
    "    .merge(df_products, on=\"product_key\", how=\"left\")\n",
    "    .groupby(\"product_name\")[\"sales_amount\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"total_revenue\")\n",
    "    .sort_values(by=\"total_revenue\", ascending=True)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "worst_5_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75c4f2-5923-4abe-80e0-381f404ef80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = dfs_imputed[\"dim_customers\"]\n",
    "\n",
    "top_10_customers = (\n",
    "    df_fact\n",
    "    .merge(df_customers, on=\"customer_key\", how=\"left\")\n",
    "    .groupby([\"customer_key\", \"first_name\", \"last_name\"])[\"sales_amount\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"total_revenue\")\n",
    "    .sort_values(by=\"total_revenue\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "top_10_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f3d6b-ed2a-47a3-9b35-92a6c52416a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_3_customers_orders = (\n",
    "    df_fact\n",
    "    .merge(df_customers, on=\"customer_key\", how=\"left\")\n",
    "    .groupby([\"customer_key\", \"first_name\", \"last_name\"])[\"order_number\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"total_orders\")\n",
    "    .sort_values(by=\"total_orders\", ascending=True)\n",
    "    .head(3)\n",
    ")\n",
    "\n",
    "bottom_3_customers_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73cd90-dc96-456d-a0b1-9c5b93d36df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_imputed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77d757-1dac-4f2f-a765-f5c00cb15f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the primary country of residence for the majority of our customers?\n",
    "\n",
    "country_frequencies = dfs_imputed['sales_extra_variables_extra_data']['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86df967-9e99-41e9-86cf-57417dbb8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa99ff6-96a7-4468-b5b8-3918a6fb87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for the frequencies\n",
    "plt.figure(figsize=(8, 5))\n",
    "country_frequencies.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('The primary country of residence for the majority of our customers')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels if needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb49fc1-7fed-47f1-bcff-1a5bdd2ff462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which product category has the highest frequency of sales?\n",
    "\n",
    "productcat_frequencies = dfs_imputed['sales_extra_variables_extra_data']['category'].value_counts()\n",
    "productcat_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0694e9-47f9-4271-a08c-6e413177c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "productcat_frequencies.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('The product category has the highest frequency of sales')\n",
    "plt.xlabel('Products category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels if needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35089ae5-dea5-4dad-9867-ad6c6c698344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the top 5 customers with the most repeat purchases.\n",
    "\n",
    "customers_frequencies = dfs_imputed['sales_extra_variables_extra_data']['customer_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c495f-e3af-40c1-bfe9-b4c4936baab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee304b-52d0-4f36-b1b0-c1ddd8fa90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Who among the sales managers has achieved the highest number of sales?\n",
    "\n",
    "salesman_frequencies = dfs_imputed['sales_extra_variables_extra_data']['sales_manager'].value_counts()\n",
    "salesman_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f57a7-c919-4829-b9dd-061aa164bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Who are the top 5 sales representatives with the highest number of sales?\n",
    "\n",
    "salesrep_frequencies = dfs_imputed['sales_extra_variables_extra_data']['sales_rep'].value_counts()\n",
    "salesrep_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb520569-d346-4029-ba40-f4c33c244619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which device is predominantly used for making product purchases?\n",
    "\n",
    "devtype_frequencies = dfs_imputed['sales_extra_variables_extra_data']['device_type'].value_counts()\n",
    "devtype_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d974f7-a628-4b7f-8e23-56caaf997b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative frequencies (percentages)\n",
    "devtype_percentages = (devtype_frequencies / devtype_frequencies.sum()) * 100\n",
    "\n",
    "# Create a pie chart for the percentages\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(devtype_percentages, labels=devtype_percentages.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('The device is predominantly used for making product purchases')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d69eb7-42e8-4dd5-858a-c99f919e8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the top 3 product categories based on both order value and cost.\n",
    "\n",
    "median_order_value = dfs_imputed['sales_extra_variables_extra_data'].groupby('category')['order_value_EUR'].median()\n",
    "median_order_value.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67684955-2631-4b9b-9a57-f041a038c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_cost = dfs_imputed['sales_extra_variables_extra_data'].groupby('category')['cost'].median()\n",
    "median_cost.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f515a94-3fae-49a3-9ba2-0d5b118c350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the top 3 customers who contribute the most to profitability and have the highest expenses.\n",
    "\n",
    "median_customers = dfs_imputed['sales_extra_variables_extra_data'].groupby('customer_name')['order_value_EUR'].median()\n",
    "median_customers.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5e66b-1339-4547-a38a-3bb9235cfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_customers_cost = dfs_imputed['sales_extra_variables_extra_data'].groupby('customer_name')['cost'].median()\n",
    "median_customers_cost.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446856e6-5d5c-4e03-ba1e-02f00d74fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which sales representative's transactions resulted in the highest amount of refunds to customers?\n",
    "\n",
    "median_sales_rep = dfs_imputed['sales_extra_variables_extra_data'].groupby('sales_rep')['refund'].median()\n",
    "median_sales_rep.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59a707-61e6-417c-9ef4-6ef1f0230c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Who are the most loyal customers of your superstore?\n",
    "\n",
    "# Create a pivot table based on median sales, cost, and counts for each product category\n",
    "pivot_table = dfs_imputed['sales_extra_variables_extra_data'].pivot_table(index='customer_name',\n",
    "                             values=['order_value_EUR', 'cost'],\n",
    "                             aggfunc={'order_value_EUR': 'median', 'cost': 'median', 'customer_name': 'count'})\n",
    "\n",
    "# Rename the columns for clarity\n",
    "pivot_table = pivot_table.rename(columns={'order_value_EUR': 'Median_Sales',\n",
    "                                          'cost': 'Median_Cost',\n",
    "                                          'customer_name': 'Count'})\n",
    "\n",
    "# Print the pivot table\n",
    "sorted_pivot_table = pivot_table.sort_values(by = 'Count', ascending = False)\n",
    "top_10 = sorted_pivot_table[0:10]\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f879f-ac86-4f16-b815-68fa813cac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, ax1 = plt.subplots(figsize=(30, 10))\n",
    "\n",
    "# Plot the count in a line plot\n",
    "sns.lineplot(data=top_10['Count'], marker='o', ax=ax1, color='tab:blue', label='Count')\n",
    "ax1.set_xlabel('Customers Names')\n",
    "ax1.set_ylabel('Count', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Create a second y-axis for the bar plots\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the median sales and median cost in bar plots\n",
    "bar_plot = top_10[['Median_Sales', 'Median_Cost']].plot(kind='bar', ax=ax2, width=0.3, color=['tab:orange', 'tab:green'], alpha=0.7)\n",
    "ax2.set_ylabel('Values', color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "bar_plot.set_xticklabels(top_10.index, rotation=90, ha='center')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Median Sales, Cost, and Product Purchase by Customers')\n",
    "ax2.legend(loc='upper left', labels=['Median Sales', 'Median Cost'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def2147-f33c-42da-98d8-4a900c446885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Under which sales manager, which product category has the highest sales volume?\n",
    "\n",
    "crosstab = pd.crosstab(dfs_imputed['sales_extra_variables_extra_data']['sales_manager'], dfs_imputed['sales_extra_variables_extra_data']['category'])\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75621d-397c-4574-ab3c-65743a567800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the crosstabulation as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(crosstab, annot=True, cmap=\"YlGnBu\", fmt='d', cbar=True)\n",
    "plt.title('Crosstabulation Heatmap')\n",
    "plt.xlabel('Product category')\n",
    "plt.ylabel('Sales Managers')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03604f-d4c5-45b5-bf45-e0c6713110ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In which country did a particular sales representative achieve the highest sales volume?\n",
    "\n",
    "crosstab_sales_rep = pd.crosstab(dfs_imputed['sales_extra_variables_extra_data']['sales_rep'], dfs_imputed['sales_extra_variables_extra_data']['country'])\n",
    "crosstab_sales_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327961c2-1485-418d-b8b6-a6407966cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the crosstabulation as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(crosstab_sales_rep, annot=True, cmap=\"YlGnBu\", fmt='d', cbar=True)\n",
    "plt.title('Crosstabulation Heatmap')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Sales Reps')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02748e7-b4d9-41f7-98b9-0b945ef335e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the relationship between order value, cost and refund amount.\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = dfs_imputed['sales_extra_variables_extra_data'][['order_value_EUR', 'cost', 'refund']].corr()\n",
    "\n",
    "# Visualize the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ca49c-c2fd-4128-81e9-9ecab498b9cf",
   "metadata": {},
   "source": [
    "# Change over time analysis\n",
    "- Analyze how a measure evolves over time.\n",
    "- Helps track rends and identify seasonality in your data.\n",
    "- Sum(Measure) By (Date Dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0eba4-afdf-44ce-ad38-30d11487ce58",
   "metadata": {},
   "source": [
    "### Revenue Over Time (Core Business Trend)\r\n",
    "DataFrame: fact_sales\r\n",
    "Measure: sales_amount\r\n",
    "Date: order_date\r\n",
    "\r\n",
    "- How has total revenue evolved over time (monthly/yearly)?\r\n",
    "- Is revenue growing consistently year over year?\r\n",
    "- Are there seasonal peaks in revenue?\r\n",
    "- Which months generate the highest revenue?\r\n",
    "- Is revenue volatility increasing over time?\r\n",
    "- Are there declining revenue periods?\r\n",
    "\r\n",
    "### Sales Volume Over Time\r\n",
    "DataFrame: fact_sales\r\n",
    "Measure: quantity\r\n",
    "Date: order_date\r\n",
    "\r\n",
    "- How does total quantity sold evolve over time?\r\n",
    "- Is growth driven by volume or price?\r\n",
    "- Are there seasonal spikes in quantity?\r\n",
    "- Is demand becoming more stable over time?\r\n",
    "\r\n",
    "### Refund Trend Over Time\r\n",
    "DataFrame: extra_data\r\n",
    "Measure: refund\r\n",
    "Date: date\r\n",
    "\r\n",
    "- Are refunds increasing over time?\r\n",
    "- Is there seasonality in refund behavior?\r\n",
    "- Do refund spikes follow sales spikes?\r\n",
    "- Is refund growth proportional to revenue growth?\r\n",
    "\r\n",
    "### Cost Evolution Over Time\r\n",
    "DataFrame: extra_data\r\n",
    "Measure: cost\r\n",
    "Date: date\r\n",
    "\r\n",
    "- Is operational cost increasing over time?\r\n",
    "- Is cost growth aligned with revenue growth?\r\n",
    "- Are there periods where cost grows faster than revenue?\r\n",
    "\r\n",
    "### Customer Acquisition Trend\r\n",
    "DataFrame: dim_customers\r\n",
    "Measure: customer_key (count)\r\n",
    "Date: create_date\r\n",
    "\r\n",
    "- How many customers are acquired each month?\r\n",
    "- Is customer acquisition accelerating?\r\n",
    "- Are there seasonal patterns in new registrations?\r\n",
    "- Is customer growth slowing down?\r\n",
    "\r\n",
    "### Product Launch & Performance Over Time\r\n",
    "DataFrame: dim_products + fact_sales\r\n",
    "Measure: sales_amount\r\n",
    "Date: start_date or order_date\r\n",
    "\r\n",
    "- Do newly launched products generate increasing revenue over time?\r\n",
    "- How long does a product take to gain traction?\r\n",
    "- Do older products lose revenue momentum?\r\n",
    "- Is product lifecycle shortening?\r\n",
    "\r\n",
    "### Order Processing & Logistics Trend\r\n",
    "DataFrame: fact_sales\r\n",
    "Measures:\r\n",
    "\r\n",
    "- Order processing time (shipping_date - order_date)\r\n",
    "- Delivery delay (due_date - shipping_date)\r\n",
    "- Is delivery time improving over time?\r\n",
    "- Are shipping delays increasing?\r\n",
    "- Is logistics efficiency correlated with revenue growth?\r\n",
    "- Do delays spike during high-sales periods?\r\n",
    "\r\n",
    "### Revenue by Category Over Time\r\n",
    "DataFrame: fact_sales + dim_products\r\n",
    "Measure: sales_amount\r\n",
    "Date: order_date\r\n",
    "Dimension: category\r\n",
    "\r\n",
    "- How does revenue evolve by category over time?\r\n",
    "- Are some categories growing faster than others?\r\n",
    "- Is category dominance changing?\r\n",
    "- Are emerging categories gaining share?\r\n",
    "\r\n",
    "### Revenue by Country Over Time\r\n",
    "DataFrame: fact_sales + dim_customers\r\n",
    "Measure: sales_amount\r\n",
    "Date: order_date\r\n",
    "Dimension: country\r\n",
    "\r\n",
    "- Which countries are growing fastest?\r\n",
    "- Is revenue diversification improving?\r\n",
    "- Are we becoming dependent on fewer countries?\r\n",
    "\r\n",
    "### Clinical Dataset (sample)\r\n",
    "\r\n",
    "Even though not business-focused:\r\n",
    "\r\n",
    "Measure: mean area or worst area\r\n",
    "Compare by target over time (if time existed)\r\n",
    "\r\n",
    "- Are tumor severity measures trending differently across target classes?\r\n",
    "- Is there pattern difference between malignant vs benign cases?\r\n",
    "\r\n",
    "### Strategic Business Questions from Change Over Time\r\n",
    "\r\n",
    "- Is growth sustainable?\r\n",
    "- Are we scaling efficiently?\r\n",
    "- Is seasonality predictable?\r\n",
    "- Are we facing structural decline?\r\n",
    "- Is customer acquisition supporting revenue growth?\r\n",
    "- Is refund growth a warning signal?\r\n",
    "- Are costs eroding margin over time?\r\n",
    "\r\n",
    "### Why Change Over Time Analysis Matters\r\n",
    "\r\n",
    "It helps:\r\n",
    "\r\n",
    "- Identify growth phases\r\n",
    "- Detect decline early\r\n",
    "- Reveal seasonality\r\n",
    "- Evaluate business health\r\n",
    "- Support forecasting\r\n",
    "- Inform strategic planningphases\r\n",
    "Detect decline early\r\n",
    "Reveal seasonality\r\n",
    "Evaluate business health\r\n",
    "Support forecasting\r\n",
    "Inform strategic planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a3fcf-3b17-444a-ac7b-3aa75b6ee8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfs_imputed[\"fact_sales\"].copy()\n",
    "\n",
    "# Garantir formato datetime\n",
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"], errors=\"coerce\")\n",
    "\n",
    "# Remover nulos (equivalente ao WHERE order_date IS NOT NULL)\n",
    "df = df[df[\"order_date\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210e378-27aa-431c-9ce0-eb702366d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_month_analysis = (\n",
    "    df\n",
    "    .assign(\n",
    "        order_year=df[\"order_date\"].dt.year,\n",
    "        order_month=df[\"order_date\"].dt.month\n",
    "    )\n",
    "    .groupby([\"order_year\", \"order_month\"])\n",
    "    .agg(\n",
    "        total_sales=(\"sales_amount\", \"sum\"),\n",
    "        total_customers=(\"customer_key\", \"nunique\"),\n",
    "        total_quantity=(\"quantity\", \"sum\")\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"order_year\", \"order_month\"])\n",
    ")\n",
    "\n",
    "year_month_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e314b-787c-4641-a7a9-169ded80148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_analysis = (\n",
    "    df\n",
    "    .assign(order_date=df[\"order_date\"].dt.to_period(\"M\").dt.to_timestamp())\n",
    "    .groupby(\"order_date\")\n",
    "    .agg(\n",
    "        total_sales=(\"sales_amount\", \"sum\"),\n",
    "        total_customers=(\"customer_key\", \"nunique\"),\n",
    "        total_quantity=(\"quantity\", \"sum\")\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"order_date\")\n",
    ")\n",
    "\n",
    "monthly_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d66dd9-25cf-4fe9-8798-d873ceccf894",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_analysis = (\n",
    "    df\n",
    "    .assign(order_date=df[\"order_date\"].dt.strftime(\"%Y-%b\"))\n",
    "    .groupby(\"order_date\")\n",
    "    .agg(\n",
    "        total_sales=(\"sales_amount\", \"sum\"),\n",
    "        total_customers=(\"customer_key\", \"nunique\"),\n",
    "        total_quantity=(\"quantity\", \"sum\")\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"order_date\")\n",
    ")\n",
    "\n",
    "formatted_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccb44d-b086-4308-b5c2-ac442d46809c",
   "metadata": {},
   "source": [
    "# Cumulative analysis\n",
    "- Aggregate the data progressively ove time.\n",
    "- Helps o understand whether our business is growing or decining.\n",
    "- Example\n",
    "- Sum(Cumulative Measure) by (Date dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a3274-5fbe-41a1-8ad3-d955de6a1aed",
   "metadata": {},
   "source": [
    "### Cumulative Revenue Growth\n",
    "DataFrame: fact_sales\n",
    "Measure: sales_amount\n",
    "Date: order_date\n",
    "\n",
    "- How does cumulative revenue evolve month over month?\n",
    "- Is cumulative revenue accelerating or flattening?\n",
    "- Does cumulative revenue show consistent growth?\n",
    "- Are there inflection points in cumulative sales?\n",
    "- Is business growth linear or exponential?\n",
    "\n",
    "### Cumulative Sales Volume\n",
    "DataFrame: fact_sales\n",
    "Measure: quantity\n",
    "Date: order_date\n",
    "\n",
    "- How does cumulative quantity sold grow over time?\n",
    "- Is volume growth consistent?\n",
    "- Does cumulative volume growth match revenue growth?\n",
    "- Are we growing through higher volume or higher prices?\n",
    "\n",
    "### Cumulative Customer Acquisition\n",
    "DataFrame: dim_customers\n",
    "Measure: COUNT(customer_key)\n",
    "Date: create_date\n",
    "\n",
    "- How does cumulative customer acquisition evolve?\n",
    "- Is customer base growth accelerating?\n",
    "- Are we reaching a saturation point?\n",
    "- Is revenue growth aligned with customer growth?\n",
    "\n",
    "### Cumulative Revenue by Category\n",
    "DataFrame: fact_sales + dim_products\n",
    "Measure: sales_amount\n",
    "Date: order_date\n",
    "Dimension: category\n",
    "\n",
    "- Which category drives cumulative revenue growth?\n",
    "- Are some categories growing faster cumulatively?\n",
    "- Is category dominance changing over time?\n",
    "- Do new categories gain cumulative share?\n",
    "\n",
    "### Cumulative Revenue by Country\n",
    "DataFrame: fact_sales + dim_customers\n",
    "Measure: sales_amount\n",
    "Date: order_date\n",
    "Dimension: country\n",
    "\n",
    "- Which countries contribute most to cumulative growth?\n",
    "- Is growth geographically diversified?\n",
    "- Are we becoming dependent on one country?\n",
    "\n",
    "### Cumulative Refund Impact\n",
    "DataFrame: extra_data\n",
    "Measure: refund\n",
    "Date: date\n",
    "\n",
    "- How does cumulative refund amount evolve?\n",
    "- Is cumulative refund growth proportional to revenue growth?\n",
    "- Are refunds increasing faster than sales?\n",
    "- Is refund trend a warning signal?\n",
    "\n",
    "### Cumulative Cost Growth\n",
    "DataFrame: extra_data\n",
    "Measure: cost\n",
    "Date: date\n",
    "\n",
    "- Is cumulative cost increasing faster than cumulative revenue?\n",
    "- Are margins improving or shrinking over time?\n",
    "- Is cost growth sustainable?\n",
    "\n",
    "### Product Lifecycle Cumulative Analysis\n",
    "DataFrame: fact_sales + dim_products\n",
    "Measure: sales_amount\n",
    "Date: order_date\n",
    "Dimension: product_line\n",
    "\n",
    "- Which product lines show the strongest cumulative growth?\n",
    "- Do newly launched products show accelerating cumulative revenue?\n",
    "- Do older products plateau over time?\n",
    "\n",
    "### Cumulative Revenue per Customer\n",
    "DataFrame: fact_sales\n",
    "Measure: sales_amount\n",
    "Date: order_date\n",
    "Dimension: customer_key\n",
    "\n",
    "- How does cumulative revenue per customer evolve?\n",
    "- Are high-value customers increasing over time?\n",
    "- Is customer lifetime value increasing?\n",
    "\n",
    "### Operational Efficiency (Cumulative Orders)\n",
    "DataFrame: fact_sales\n",
    "Measure: COUNT(order_number)\n",
    "Date: order_date\n",
    "\n",
    "- How does cumulative number of orders grow?\n",
    "- Is order growth accelerating?\n",
    "- Are we improving conversion over time?\n",
    "\n",
    "### Strategic Business Insights from Cumulative Analysis\n",
    "\n",
    "Cumulative analysis helps answer:\n",
    "\n",
    "- Is the business truly growing?\n",
    "- Is growth stable or volatile?\n",
    "- Is customer acquisition sustainable?\n",
    "- Are refunds eroding long-term growth?\n",
    "- Is margin improving over time?\n",
    "- Are we scaling efficiently?\n",
    "- Are we hitting maturity?\n",
    "\n",
    "### Executive-Level Questions\n",
    "\n",
    "Is cumulative revenue outpacing cumulative cost?\n",
    "- Is cumulative revenue outpacing cumulative cost?\n",
    "- Is customer growth supporting revenue growth?\n",
    "- Are we compounding growth year over year?\n",
    "- Is the business entering a plateau phase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb4490-a136-4a8e-9129-9689d171818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar dataframe\n",
    "df = dfs_imputed[\"fact_sales\"].copy()\n",
    "\n",
    "# Garantir datetime\n",
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"], errors=\"coerce\")\n",
    "\n",
    "# Remover datas nulas\n",
    "df = df[df[\"order_date\"].notna()]\n",
    "\n",
    "# Equivalente ao subquery (DATETRUNC(year))\n",
    "yearly = (\n",
    "    df\n",
    "    .assign(order_date=df[\"order_date\"].dt.to_period(\"Y\").dt.to_timestamp())\n",
    "    .groupby(\"order_date\")\n",
    "    .agg(\n",
    "        total_sales=(\"sales_amount\", \"sum\"),\n",
    "        avg_price=(\"price\", \"mean\")\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"order_date\")\n",
    ")\n",
    "\n",
    "# Running Total (SUM() OVER ORDER BY)\n",
    "yearly[\"running_total_sales\"] = yearly[\"total_sales\"].cumsum()\n",
    "\n",
    "# Moving Average (AVG() OVER ORDER BY)\n",
    "yearly[\"moving_average_price\"] = yearly[\"avg_price\"].expanding().mean()\n",
    "\n",
    "yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d3c94-c19f-46dc-82b5-ec7d4ff8379e",
   "metadata": {},
   "source": [
    "# Performance analysis\n",
    "- Comparing the current value to a target value.\n",
    "- Helps measure success and compare performance.\n",
    "- Example:\n",
    "- Current(Measure) - Target(Measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d647e1-749d-4e93-94eb-43200679a096",
   "metadata": {},
   "source": [
    "### Revenue Performance\r\n",
    "DataFrame: fact_sales\r\n",
    "Measure: sales_amount\r\n",
    "\r\n",
    "- Is current revenue meeting the monthly target?\r\n",
    "- What is the revenue gap vs target by year?\r\n",
    "- Which product category exceeded its revenue target?\r\n",
    "- Which country is underperforming against sales goals?\r\n",
    "- Are we achieving expected growth rates?\r\n",
    "- What is the revenue variance vs forecast?\r\n",
    "\r\n",
    "Business insight:\r\n",
    "Identifica áreas que precisam de ação imediata.\r\n",
    "\r\n",
    "### Sales Volume Performance\r\n",
    "DataFrame: fact_sales\r\n",
    "Measure: quantity\r\n",
    "\r\n",
    "- Is current sales volume aligned with projections?\r\n",
    "- Which products are below expected volume?\r\n",
    "- Are high-cost products underperforming in units sold?\r\n",
    "- Are we compensating lower volume with higher price?\r\n",
    "\r\n",
    "### Price Performance\r\n",
    "DataFrame: fact_sales\r\n",
    "Measure: price\r\n",
    "\r\n",
    "- Is average selling price above or below target?\r\n",
    "- Are discounts reducing margin?\r\n",
    "- Are premium products achieving expected pricing?\r\n",
    "- Is pricing strategy aligned with revenue goals?\r\n",
    "\r\n",
    "### Customer Acquisition Performance\r\n",
    "DataFrame: dim_customers\r\n",
    "Measure: COUNT(customer_key)\r\n",
    "Date: create_date\r\n",
    "\r\n",
    "- Are we acquiring customers at the planned rate?\r\n",
    "- Is customer growth below forecast?\r\n",
    "- Which country is missing acquisition targets?\r\n",
    "- Is acquisition accelerating or slowing?\r\n",
    "\r\n",
    "### Customer Revenue Performance\r\n",
    "DataFrame: fact_sales\r\n",
    "\r\n",
    "- Are top customers meeting expected revenue contribution?\r\n",
    "- Is revenue per customer above benchmark?\r\n",
    "- Are high-value customers increasing?\r\n",
    "- Is churn affecting revenue performance?\r\n",
    "\r\n",
    "### Product Performance vs Target\r\n",
    "DataFrame: fact_sales + dim_products\r\n",
    "\r\n",
    "- Which product lines are below sales targets?\r\n",
    "- Are newly launched products reaching expected performance?\r\n",
    "- Are low-cost products meeting margin expectations?\r\n",
    "- Which category is exceeding expectations?\r\n",
    "\r\n",
    "### Cost & Margin Performance\r\n",
    "DataFrame: dim_products + extra_data\r\n",
    "\r\n",
    "- Is cost growth exceeding planned budget?\r\n",
    "- Is margin meeting strategic targets?\r\n",
    "- Are refunds reducing expected profitability?\r\n",
    "- Is operational cost aligned with revenue growth?\r\n",
    "\r\n",
    "### Refund Performance\r\n",
    "DataFrame: extra_data\r\n",
    "\r\n",
    "- Is refund rate above acceptable threshold?\r\n",
    "- Which category exceeds refund tolerance?\r\n",
    "- Are refund trends worsening?\r\n",
    "- Is refund cost impacting profitability target?\r\n",
    "\r\n",
    "### Sales Team Performance\r\n",
    "DataFrame: sales\r\n",
    "\r\n",
    "- Are sales reps meeting revenue quotas?\r\n",
    "- Which manager is exceeding targets?\r\n",
    "- Who is underperforming against quota?\r\n",
    "- Is performance evenly distributed across team?\r\n",
    "\r\n",
    "### Country Performance\r\n",
    "DataFrame: sales + fact_sales\r\n",
    "\r\n",
    "- Which countries are below growth target?\r\n",
    "- Is international expansion meeting expectations?\r\n",
    "- Are certain regions declining?\r\n",
    "\r\n",
    "### Portfolio Efficiency Performance\r\n",
    "DataFrame: dim_products\r\n",
    "\r\n",
    "- Are high-cost products generating expected return?\r\n",
    "- Are some product lines failing to meet ROI targets?\r\n",
    "- Should underperforming products be discontinued?\r\n",
    "\r\n",
    "### Strategic Executive-Level Questions\r\n",
    "\r\n",
    "- Is overall business performance aligned with strategic goals?\r\n",
    "- Are we overachieving revenue but underachieving margin?\r\n",
    "- Is growth sustainable relative to cost structure?\r\n",
    "- Are performance gaps structural or temporary?\r\n",
    "- Where should leadership intervene?\r\n",
    "\r\n",
    "### Performance Analysis Mindset\r\n",
    "\r\n",
    "Perguntas-chave sempre seguem estrutura:\r\n",
    "\r\n",
    "- What is the current value?\r\n",
    "- What was the target?\r\n",
    "- What is the variance?\r\n",
    "- Why is there a gap?\r\n",
    "- What action should be taken?\r\n",
    "\r\n",
    "### Example\r\n",
    "\r\n",
    "- Revenue Gap = Current Revenue – Target Revenue\r\n",
    "- Refund Variance = Actual Refund – Acceptable Refund Threshold\r\n",
    "- Customer Growth Gap = Actual New Customers – Forecasted New Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d2d8d-afc9-4145-a41d-35061af1248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataframes\n",
    "df_fact = dfs_imputed[\"fact_sales\"].copy()\n",
    "df_products = dfs_imputed[\"dim_products\"].copy()\n",
    "\n",
    "# Garantir datetime\n",
    "df_fact[\"order_date\"] = pd.to_datetime(df_fact[\"order_date\"], errors=\"coerce\")\n",
    "\n",
    "# Remover nulos\n",
    "df_fact = df_fact[df_fact[\"order_date\"].notna()]\n",
    "\n",
    "# 1 Yearly product sales (equivalente à CTE)\n",
    "yearly_product_sales = (\n",
    "    df_fact\n",
    "    .merge(df_products[[\"product_key\", \"product_name\"]], \n",
    "           on=\"product_key\", how=\"left\")\n",
    "    .assign(order_year=df_fact[\"order_date\"].dt.year)\n",
    "    .groupby([\"order_year\", \"product_name\"])[\"sales_amount\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"current_sales\")\n",
    ")\n",
    "\n",
    "# 2 Average sales per product (PARTITION BY product_name)\n",
    "yearly_product_sales[\"avg_sales\"] = (\n",
    "    yearly_product_sales\n",
    "    .groupby(\"product_name\")[\"current_sales\"]\n",
    "    .transform(\"mean\")\n",
    ")\n",
    "\n",
    "# Difference vs average\n",
    "yearly_product_sales[\"diff_avg\"] = (\n",
    "    yearly_product_sales[\"current_sales\"] \n",
    "    - yearly_product_sales[\"avg_sales\"]\n",
    ")\n",
    "\n",
    "# Above / Below Avg\n",
    "yearly_product_sales[\"avg_change\"] = yearly_product_sales[\"diff_avg\"].apply(\n",
    "    lambda x: \"Above Avg\" if x > 0 \n",
    "    else (\"Below Avg\" if x < 0 else \"Avg\")\n",
    ")\n",
    "\n",
    "# 3 Year-over-Year (LAG equivalent)\n",
    "yearly_product_sales = yearly_product_sales.sort_values(\n",
    "    [\"product_name\", \"order_year\"]\n",
    ")\n",
    "\n",
    "yearly_product_sales[\"py_sales\"] = (\n",
    "    yearly_product_sales\n",
    "    .groupby(\"product_name\")[\"current_sales\"]\n",
    "    .shift(1)\n",
    ")\n",
    "\n",
    "yearly_product_sales[\"diff_py\"] = (\n",
    "    yearly_product_sales[\"current_sales\"] \n",
    "    - yearly_product_sales[\"py_sales\"]\n",
    ")\n",
    "\n",
    "yearly_product_sales[\"py_change\"] = yearly_product_sales[\"diff_py\"].apply(\n",
    "    lambda x: \"Increase\" if x > 0 \n",
    "    else (\"Decrease\" if x < 0 else \"No Change\")\n",
    ")\n",
    "\n",
    "# Ordenar resultado final\n",
    "final_result = yearly_product_sales.sort_values(\n",
    "    [\"product_name\", \"order_year\"]\n",
    ")\n",
    "\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6251cd-3611-473a-8ef8-8cbbf08d3867",
   "metadata": {},
   "source": [
    "# Part-to-Whole\n",
    "- Analyze how an individual aprt is performing comapred to the overrall, allowing us to understand which category has the greatest impact on the business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2008df0-653a-4dfe-82ea-73461b00f004",
   "metadata": {},
   "source": [
    "### Revenue Contribution Analysis\n",
    "DataFrame: fact_sales\n",
    "Measure: sales_amount\n",
    "\n",
    "- What percentage of total revenue comes from each product?\n",
    "- Which category contributes the highest share of total revenue?\n",
    "- What is the revenue share by product line?\n",
    "- What percentage of total revenue comes from the top 10 products?\n",
    "- Do 20% of products generate 80% of revenue (Pareto principle)?\n",
    "- What is each country's share of total revenue?\n",
    "- What percentage of revenue comes from the top 5 customers?\n",
    "- How much revenue is generated by each sales rep as a share of total revenue?\n",
    "\n",
    "Business insight:\n",
    "Identifica concentração de receita e risco estrutural.\n",
    "\n",
    "### Volume Contribution (Sales Quantity)\n",
    "DataFrame: fact_sales\n",
    "Measure: quantity\n",
    "\n",
    "- What percentage of total units sold comes from each category?\n",
    "- Which product contributes most to total volume?\n",
    "- Does high volume correspond to high revenue share?\n",
    "- Which country contributes most to total sales volume?\n",
    "\n",
    "### Refund Impact Share\n",
    "DataFrame: extra_data\n",
    "Measure: refund\n",
    "\n",
    "- What percentage of total refunds comes from each category?\n",
    "- Which product accounts for the highest refund share?\n",
    "- Which country contributes most to refund losses?\n",
    "- What share of total revenue is lost to refunds?\n",
    "\n",
    "Business insight:\n",
    "Identifica categorias que prejudicam margem.\n",
    "\n",
    "### Cost Structure Contribution\n",
    "DataFrame: dim_products\n",
    "Measure: cost\n",
    "\n",
    "- What percentage of total cost is driven by each product line?\n",
    "- Which category accounts for most of the cost base?\n",
    "- Is cost concentration aligned with revenue concentration?\n",
    "- Which products contribute heavily to cost but little to revenue?\n",
    "\n",
    "### Customer Contribution Analysis\n",
    "DataFrame: fact_sales + dim_customers\n",
    "\n",
    "- What percentage of revenue comes from each customer segment (gender, marital status)?\n",
    "- What share of total revenue comes from top 5% customers?\n",
    "- Is revenue evenly distributed or concentrated?\n",
    "- Which country contributes the largest share of active customers?\n",
    "\n",
    "### Portfolio Impact Analysis\n",
    "DataFrame: fact_sales + dim_products\n",
    "\n",
    "- What share of total revenue comes from each product line?\n",
    "- Do a few subcategories dominate the portfolio?\n",
    "- Are new products increasing their share of total revenue?\n",
    "- Which categories have declining share over time?\n",
    "\n",
    "### Sales Team Contribution\n",
    "DataFrame: sales\n",
    "\n",
    "- What percentage of total sales is generated by each sales manager?\n",
    "- Is revenue concentrated in a few sales reps?\n",
    "- Which rep contributes the largest share of company revenue?\n",
    "\n",
    "### Device Contribution Analysis\n",
    "DataFrame: sales\n",
    "\n",
    "- What percentage of revenue comes from mobile vs desktop?\n",
    "- Is mobile share increasing over time?\n",
    "- Does one channel dominate revenue generation?\n",
    "\n",
    "### Geographic Contribution\n",
    "DataFrame: sales or dim_customers\n",
    "\n",
    "- What percentage of total revenue comes from each country?\n",
    "- Are we dependent on one primary market?\n",
    "- Is geographic diversification improving?\n",
    "\n",
    "### Clinical Dataset (sample)\n",
    "\n",
    "- Even though not business-oriented:\n",
    "- What percentage of cases are malignant vs benign?\n",
    "- Which morphological measures contribute most to classification separation?\n",
    "\n",
    "### Strategic Executive Questions\n",
    "\n",
    "- Is revenue highly concentrated?\n",
    "- What part of the business sustains most of the value?\n",
    "- Are we exposed to concentration risk?\n",
    "- Which categories have disproportionate impact?\n",
    "- Where should we allocate resources?\n",
    "- Which areas should be optimized or reduced?\n",
    "\n",
    "### Why Part-to-Whole Analysis Matters\n",
    "\n",
    "It helps to:\n",
    "\n",
    "- Detect excessive dependency\n",
    "- Identify the real business drivers\n",
    "- Prioritize investments\n",
    "- Reduce structural risk\n",
    "- Apply Pareto analysis (80/20)\n",
    "- Understand the true impact of each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4bce5-aaf6-4bf1-a3ef-cbd90b439462",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_sales = (\n",
    "    df_fact\n",
    "    .merge(df_products[[\"product_key\", \"category\"]],\n",
    "           on=\"product_key\",\n",
    "           how=\"left\")\n",
    "    .groupby(\"category\", as_index=False)[\"sales_amount\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"sales_amount\": \"total_sales\"})\n",
    ")\n",
    "\n",
    "overall = category_sales[\"total_sales\"].sum()\n",
    "\n",
    "category_sales = (\n",
    "    category_sales\n",
    "    .assign(\n",
    "        overall_sales=overall,\n",
    "        percentage_of_total=lambda x: \n",
    "            (x[\"total_sales\"] / overall * 100).round(2)\n",
    "    )\n",
    "    .sort_values(\"total_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "category_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bddb882-458c-4f97-9215-7f3d6e5a2820",
   "metadata": {},
   "source": [
    "# Data Segmentation\n",
    "- Group the data absed on a specific range\n",
    "- helps understand the correlation between two measures\n",
    "- Example\n",
    "- (Measure) By (Measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df901b6-634e-4a43-aa7c-804fa711afa5",
   "metadata": {},
   "source": [
    "### Revenue Segmentation by Price Range\r\n",
    "DataFrame: fact_sales\r\n",
    "Measures: sales_amount, price\r\n",
    "\r\n",
    "- How does total revenue vary across price ranges?\r\n",
    "- Do higher-priced products generate proportionally more revenue?\r\n",
    "- Is revenue concentrated in low, mid, or high price bands?\r\n",
    "-Does price range influence quantity sold?\r\n",
    "\r\n",
    "- segmentation example:\r\n",
    "\r\n",
    "- Low price (< 50)\r\n",
    "\r\n",
    "- Mid price (50–200)\r\n",
    "\r\n",
    "- High price (> 200)\r\n",
    "\r\n",
    "### Revenue by Quantity Segments\r\n",
    "DataFrame: fact_sales\r\n",
    "Measures: sales_amount, quantity\r\n",
    "\r\n",
    "- How does revenue vary by order size?\r\n",
    "- Do large-quantity orders contribute disproportionately to revenue?\r\n",
    "- Is revenue driven by high volume or high price?\r\n",
    "- segmentation example:\r\n",
    "- Small orders (1–5 units)\r\n",
    "- Medium orders (6–20)\r\n",
    "- Large orders (> 20)\r\n",
    "\r\n",
    "### Refund by Revenue Segments\r\n",
    "DataFrame: extra_data\r\n",
    "Measures: refund, order_value_EUR\r\n",
    "\r\n",
    "- Are higher-value orders more likely to be refunded?\r\n",
    "- Does refund percentage increase with order value?\r\n",
    "- Which revenue segment generates most refund losses?\r\n",
    "\r\n",
    "### Customer Revenue Segmentation\r\n",
    "DataFrame: fact_sales\r\n",
    "Measures: sales_amount (aggregated per customer)\r\n",
    "\r\n",
    "- How is total revenue distributed across customer value segments?\r\n",
    "- What percentage of customers fall into low, medium, and high-value tiers?\r\n",
    "- Do high-value customers grow faster over time?\r\n",
    "\r\n",
    "- example segmentation:\r\n",
    "\r\n",
    "- Low-value customers (< $1,000)\r\n",
    "- Mid-value customers ($1,000–$10,000)\r\n",
    "- High-value customers (> $10,000)\r\n",
    "\r\n",
    "### Cost vs Revenue Segmentation\r\n",
    "DataFrame: dim_products + fact_sales\r\n",
    "Measures: cost, sales_amount\r\n",
    "\r\n",
    "- How does revenue behave across different cost segments?\r\n",
    "- Are high-cost products generating proportional revenue?\r\n",
    "- Is margin higher in specific cost bands?\r\n",
    "\r\n",
    "### Category Revenue vs Quantity Segmentation\r\n",
    "DataFrame: fact_sales + dim_products\r\n",
    "Measures: sales_amount, quantity\r\n",
    "\r\n",
    "- Do high-volume categories generate high revenue?\r\n",
    "- Are some categories high-volume but low-revenue?\r\n",
    "- Is there correlation between volume and revenue?\r\n",
    "\r\n",
    "### Sales Performance by Customer Demographics\r\n",
    "DataFrame: fact_sales + dim_customers\r\n",
    "Measures: sales_amount, quantity\r\n",
    "Segments: gender, marital_status\r\n",
    "\r\n",
    "- Does revenue differ significantly by gender?\r\n",
    "- Do married customers spend more than single customers?\r\n",
    "- Is order size different across demographic groups?\r\n",
    "\r\n",
    "### Country Revenue Segmentation\r\n",
    "DataFrame: fact_sales + dim_customers\r\n",
    "\r\n",
    "- Are high-revenue countries also high-volume countries?\r\n",
    "- Is average order value higher in specific regions?\r\n",
    "- Does refund behavior differ by country revenue tier?\r\n",
    "\r\n",
    "### Device Revenue Segmentation\r\n",
    "DataFrame: sales\r\n",
    "\r\n",
    "- Does mobile generate smaller but more frequent purchases?\r\n",
    "- Is desktop associated with higher average order value?\r\n",
    "- Which device segment drives higher revenue per transaction?\r\n",
    "\r\n",
    "### Product Performance Segmentation\r\n",
    "DataFrame: fact_sales + dim_products\r\n",
    "\r\n",
    "- Do premium products generate higher cumulative revenue?\r\n",
    "- Are some subcategories high price but low volume?\r\n",
    "- Which product segments show strongest growth?\r\n",
    "\r\n",
    "### Clinical Dataset Segmentation (sample)\r\n",
    "Measures: mean area, worst concavity\r\n",
    "Segment: target\r\n",
    "\r\n",
    "- Do malignant cases have significantly higher concavity values?\r\n",
    "- Is area strongly correlated with tumor classification?\r\n",
    "- Which morphological measures show strongest separation?\r\n",
    "\r\n",
    "### Strategic Business Questions from Segmentation\r\n",
    "\r\n",
    "- Is revenue driven by price or volume?\r\n",
    "- Are refunds correlated with high-value orders?\r\n",
    "- Which segment is most profitable?\r\n",
    "- Are we targeting the right customer tier?\r\n",
    "- Which cost band yields highest return?\r\n",
    "- Where should we focus marketing efforts?\r\n",
    "\r\n",
    "### Why Data Segmentation Matters\r\n",
    "\r\n",
    "It helps to:\r\n",
    "\r\n",
    "- Identify hidden patterns\r\n",
    "- Understand correlations between metrics\r\n",
    "- Detect optimization opportunities\r\n",
    "- Adjust pricing strategy\r\n",
    "- Refine customer segmentation\r\n",
    "- Improve resource allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e6233-4e98-45f2-b91b-ac16860f50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = dfs_imputed[\"dim_products\"].copy()\n",
    "\n",
    "# Garantir que cost é float\n",
    "df_products[\"cost\"] = df_products[\"cost\"].astype(float)\n",
    "\n",
    "# Criar segmentação\n",
    "df_products[\"cost_range\"] = pd.cut(\n",
    "    df_products[\"cost\"],\n",
    "    bins=[-float(\"inf\"), 100, 500, 1000, float(\"inf\")],\n",
    "    labels=[\"Below 100\", \"100-500\", \"500-1000\", \"Above 1000\"]\n",
    ")\n",
    "\n",
    "# Contar produtos por segmento\n",
    "product_segments = (\n",
    "    df_products\n",
    "    .groupby(\"cost_range\")[\"product_key\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"total_products\")\n",
    "    .sort_values(\"total_products\", ascending=False)\n",
    ")\n",
    "\n",
    "product_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2f62f-7635-4b9f-955f-9b82ee572559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = dfs_imputed[\"fact_sales\"].copy()\n",
    "df_customers = dfs_imputed[\"dim_customers\"].copy()\n",
    "\n",
    "# Garantir datetime\n",
    "df_fact[\"order_date\"] = pd.to_datetime(df_fact[\"order_date\"], errors=\"coerce\")\n",
    "\n",
    "# Customer spending summary (equivalente à CTE)\n",
    "customer_spending = (\n",
    "    df_fact\n",
    "    .groupby(\"customer_key\")\n",
    "    .agg(\n",
    "        total_spending=(\"sales_amount\", \"sum\"),\n",
    "        first_order=(\"order_date\", \"min\"),\n",
    "        last_order=(\"order_date\", \"max\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calcular lifespan em meses\n",
    "customer_spending[\"lifespan\"] = (\n",
    "    (customer_spending[\"last_order\"].dt.year - customer_spending[\"first_order\"].dt.year) * 12 +\n",
    "    (customer_spending[\"last_order\"].dt.month - customer_spending[\"first_order\"].dt.month)\n",
    ")\n",
    "\n",
    "# Criar segmentação\n",
    "def segment_customer(row):\n",
    "    if row[\"lifespan\"] >= 12 and row[\"total_spending\"] > 5000:\n",
    "        return \"VIP\"\n",
    "    elif row[\"lifespan\"] >= 12 and row[\"total_spending\"] <= 5000:\n",
    "        return \"Regular\"\n",
    "    else:\n",
    "        return \"New\"\n",
    "\n",
    "customer_spending[\"customer_segment\"] = customer_spending.apply(\n",
    "    segment_customer, axis=1\n",
    ")\n",
    "\n",
    "# Contar clientes por segmento\n",
    "customer_segments = (\n",
    "    customer_spending\n",
    "    .groupby(\"customer_segment\")[\"customer_key\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"total_customers\")\n",
    "    .sort_values(\"total_customers\", ascending=False)\n",
    ")\n",
    "\n",
    "customer_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e081f6-36cd-44ea-8d87-2e264fc8c8dd",
   "metadata": {},
   "source": [
    "# BUILD CUSTOMR REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccfb6a-3c55-476d-8ee8-33a548d99f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1️⃣ Base Query\n",
    "# ============================\n",
    "\n",
    "df_fact = dfs_imputed[\"fact_sales\"].copy()\n",
    "df_customers = dfs_imputed[\"dim_customers\"].copy()\n",
    "\n",
    "# Garantir datetime\n",
    "df_fact[\"order_date\"] = pd.to_datetime(df_fact[\"order_date\"], errors=\"coerce\")\n",
    "df_customers[\"birthdate\"] = pd.to_datetime(df_customers[\"birthdate\"], errors=\"coerce\")\n",
    "\n",
    "# Remover pedidos sem data\n",
    "df_fact = df_fact[df_fact[\"order_date\"].notna()]\n",
    "\n",
    "# Join\n",
    "base_query = (\n",
    "    df_fact\n",
    "    .merge(df_customers, on=\"customer_key\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Criar nome completo\n",
    "base_query[\"customer_name\"] = (\n",
    "    base_query[\"first_name\"] + \" \" + base_query[\"last_name\"]\n",
    ")\n",
    "\n",
    "# Calcular idade\n",
    "today = pd.Timestamp.today()\n",
    "\n",
    "base_query[\"age\"] = (\n",
    "    (today - base_query[\"birthdate\"]).dt.days // 365\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 2️⃣ Customer Aggregation\n",
    "# ============================\n",
    "\n",
    "customer_aggregation = (\n",
    "    base_query\n",
    "    .groupby([\n",
    "        \"customer_key\",\n",
    "        \"customer_number\",\n",
    "        \"customer_name\",\n",
    "        \"age\"\n",
    "    ])\n",
    "    .agg(\n",
    "        total_orders=(\"order_number\", \"nunique\"),\n",
    "        total_sales=(\"sales_amount\", \"sum\"),\n",
    "        total_quantity=(\"quantity\", \"sum\"),\n",
    "        total_products=(\"product_key\", \"nunique\"),\n",
    "        last_order_date=(\"order_date\", \"max\"),\n",
    "        first_order_date=(\"order_date\", \"min\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Lifespan em meses\n",
    "customer_aggregation[\"lifespan\"] = (\n",
    "    (customer_aggregation[\"last_order_date\"].dt.year -\n",
    "     customer_aggregation[\"first_order_date\"].dt.year) * 12 +\n",
    "    (customer_aggregation[\"last_order_date\"].dt.month -\n",
    "     customer_aggregation[\"first_order_date\"].dt.month)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3️⃣ Age Group\n",
    "# ============================\n",
    "\n",
    "def age_group(age):\n",
    "    if age < 20:\n",
    "        return \"Under 20\"\n",
    "    elif 20 <= age <= 29:\n",
    "        return \"20-29\"\n",
    "    elif 30 <= age <= 39:\n",
    "        return \"30-39\"\n",
    "    elif 40 <= age <= 49:\n",
    "        return \"40-49\"\n",
    "    else:\n",
    "        return \"50 and above\"\n",
    "\n",
    "customer_aggregation[\"age_group\"] = customer_aggregation[\"age\"].apply(age_group)\n",
    "\n",
    "# ============================\n",
    "# 4️⃣ Customer Segment\n",
    "# ============================\n",
    "\n",
    "def segment(row):\n",
    "    if row[\"lifespan\"] >= 12 and row[\"total_sales\"] > 5000:\n",
    "        return \"VIP\"\n",
    "    elif row[\"lifespan\"] >= 12 and row[\"total_sales\"] <= 5000:\n",
    "        return \"Regular\"\n",
    "    else:\n",
    "        return \"New\"\n",
    "\n",
    "customer_aggregation[\"customer_segment\"] = customer_aggregation.apply(segment, axis=1)\n",
    "\n",
    "# ============================\n",
    "# 5️⃣ Recency\n",
    "# ============================\n",
    "\n",
    "customer_aggregation[\"recency\"] = (\n",
    "    (today.year - customer_aggregation[\"last_order_date\"].dt.year) * 12 +\n",
    "    (today.month - customer_aggregation[\"last_order_date\"].dt.month)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 6️⃣ KPIs\n",
    "# ============================\n",
    "\n",
    "# Average Order Value\n",
    "customer_aggregation[\"avg_order_value\"] = (\n",
    "    customer_aggregation[\"total_sales\"] /\n",
    "    customer_aggregation[\"total_orders\"]\n",
    ").fillna(0)\n",
    "\n",
    "# Average Monthly Spend\n",
    "customer_aggregation[\"avg_monthly_spend\"] = (\n",
    "    customer_aggregation.apply(\n",
    "        lambda row: row[\"total_sales\"]\n",
    "        if row[\"lifespan\"] == 0\n",
    "        else row[\"total_sales\"] / row[\"lifespan\"],\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Final Report\n",
    "# ============================\n",
    "\n",
    "report_customers = customer_aggregation[[\n",
    "    \"customer_key\",\n",
    "    \"customer_number\",\n",
    "    \"customer_name\",\n",
    "    \"age\",\n",
    "    \"age_group\",\n",
    "    \"customer_segment\",\n",
    "    \"last_order_date\",\n",
    "    \"recency\",\n",
    "    \"total_orders\",\n",
    "    \"total_sales\",\n",
    "    \"total_quantity\",\n",
    "    \"total_products\",\n",
    "    \"lifespan\",\n",
    "    \"avg_order_value\",\n",
    "    \"avg_monthly_spend\"\n",
    "]]\n",
    "\n",
    "report_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8adbe8-2b0c-4d51-9e14-af8aac5413ae",
   "metadata": {},
   "source": [
    "# BUILD PRODUCTS REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a541be7-4c8e-405f-afa3-1ed64ad5697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1️⃣ Base Query\n",
    "# ============================\n",
    "\n",
    "df_fact = dfs_imputed[\"fact_sales\"].copy()\n",
    "df_products = dfs_imputed[\"dim_products\"].copy()\n",
    "\n",
    "# Garantir datetime\n",
    "df_fact[\"order_date\"] = pd.to_datetime(df_fact[\"order_date\"], errors=\"coerce\")\n",
    "\n",
    "# Remover vendas sem data válida\n",
    "df_fact = df_fact[df_fact[\"order_date\"].notna()]\n",
    "\n",
    "# Join fact + products\n",
    "base_query = (\n",
    "    df_fact\n",
    "    .merge(df_products, on=\"product_key\", how=\"left\")\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 2️⃣ Product Aggregations\n",
    "# ============================\n",
    "\n",
    "product_aggregations = (\n",
    "    base_query\n",
    "    .groupby([\n",
    "        \"product_key\",\n",
    "        \"product_name\",\n",
    "        \"category\",\n",
    "        \"subcategory\",\n",
    "        \"cost\"\n",
    "    ])\n",
    "    .agg(\n",
    "        first_sale_date=(\"order_date\", \"min\"),\n",
    "        last_sale_date=(\"order_date\", \"max\"),\n",
    "        total_orders=(\"order_number\", \"nunique\"),\n",
    "        total_customers=(\"customer_key\", \"nunique\"),\n",
    "        total_sales=(\"sales_amount\", \"sum\"),\n",
    "        total_quantity=(\"quantity\", \"sum\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Lifespan (months)\n",
    "product_aggregations[\"lifespan\"] = (\n",
    "    (product_aggregations[\"last_sale_date\"].dt.year -\n",
    "     product_aggregations[\"first_sale_date\"].dt.year) * 12 +\n",
    "    (product_aggregations[\"last_sale_date\"].dt.month -\n",
    "     product_aggregations[\"first_sale_date\"].dt.month)\n",
    ")\n",
    "\n",
    "# Average selling price\n",
    "product_aggregations[\"avg_selling_price\"] = (\n",
    "    (product_aggregations[\"total_sales\"] /\n",
    "     product_aggregations[\"total_quantity\"])\n",
    ").round(1)\n",
    "\n",
    "# ============================\n",
    "# 3️⃣ Recency\n",
    "# ============================\n",
    "\n",
    "today = pd.Timestamp.today()\n",
    "\n",
    "product_aggregations[\"recency_in_months\"] = (\n",
    "    (today.year - product_aggregations[\"last_sale_date\"].dt.year) * 12 +\n",
    "    (today.month - product_aggregations[\"last_sale_date\"].dt.month)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 4️⃣ Product Segmentation\n",
    "# ============================\n",
    "\n",
    "def product_segment(total_sales):\n",
    "    if total_sales > 50000:\n",
    "        return \"High-Performer\"\n",
    "    elif total_sales >= 10000:\n",
    "        return \"Mid-Range\"\n",
    "    else:\n",
    "        return \"Low-Performer\"\n",
    "\n",
    "product_aggregations[\"product_segment\"] = (\n",
    "    product_aggregations[\"total_sales\"].apply(product_segment)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 5️⃣ KPIs\n",
    "# ============================\n",
    "\n",
    "# Average Order Revenue (AOR)\n",
    "product_aggregations[\"avg_order_revenue\"] = (\n",
    "    product_aggregations.apply(\n",
    "        lambda row: 0\n",
    "        if row[\"total_orders\"] == 0\n",
    "        else row[\"total_sales\"] / row[\"total_orders\"],\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Average Monthly Revenue\n",
    "product_aggregations[\"avg_monthly_revenue\"] = (\n",
    "    product_aggregations.apply(\n",
    "        lambda row: row[\"total_sales\"]\n",
    "        if row[\"lifespan\"] == 0\n",
    "        else row[\"total_sales\"] / row[\"lifespan\"],\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Final Report\n",
    "# ============================\n",
    "\n",
    "report_products = product_aggregations[[\n",
    "    \"product_key\",\n",
    "    \"product_name\",\n",
    "    \"category\",\n",
    "    \"subcategory\",\n",
    "    \"cost\",\n",
    "    \"last_sale_date\",\n",
    "    \"recency_in_months\",\n",
    "    \"product_segment\",\n",
    "    \"lifespan\",\n",
    "    \"total_orders\",\n",
    "    \"total_sales\",\n",
    "    \"total_quantity\",\n",
    "    \"total_customers\",\n",
    "    \"avg_selling_price\",\n",
    "    \"avg_order_revenue\",\n",
    "    \"avg_monthly_revenue\"\n",
    "]]\n",
    "\n",
    "report_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18dadad-62d7-4cf9-bdf7-49e811ac6f98",
   "metadata": {},
   "source": [
    "# Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d96994-3692-4435-b8f0-2ab58135222e",
   "metadata": {},
   "source": [
    "# DESCRIPTIVE ANALYSIS – QUESTIONS BY DATAFRAME\n",
    "* Measures of central tendency\n",
    "    * mean: the average sum of all samples over the total number of samples\n",
    "    * median: middle number of the ordered  sequence of samples (ascending)\n",
    "    * mode: frequentl occurring number\n",
    "    * min\n",
    "    * max\n",
    "* Measures of dispersion\n",
    "    * range: he difference beteen highest and lowest values in the data set\n",
    "    * standard deviation (compare the visuaizations between sd)\n",
    "    * percentiles: 80% are shortes than you\n",
    "        * that means you are at 90th percentile\n",
    "        * if your height is 5'11'', then it i the 80th percentile height\n",
    "### DataFrame: sample (Clinical / Numerical Features Dataset)\n",
    "General Distribution\n",
    "\n",
    "- What are the mean, median, minimum, and maximum of mean radius?\n",
    "- How is mean area distributed?\n",
    "- What is the standard deviation of mean perimeter?\n",
    "- Which variable shows the highest variability?\n",
    "- How is the variable target distributed?\n",
    "- What is the proportion of each class in target?\n",
    "- Is there a difference in the average mean concavity by target class?\n",
    "- How is worst radius distributed?\n",
    "- What is the average mean smoothness per class?\n",
    "- Which feature has the largest range (max - min)?\n",
    "\n",
    "Descriptive Comparisons\n",
    "\n",
    "- Which variables have higher averages when target = 1?\n",
    "- What is the average of area error?\n",
    "- How is worst concave points distributed?\n",
    "- What is the descriptive correlation between mean area and mean perimeter?\n",
    "- Which variables show the highest skewness?\n",
    "\n",
    "### DataFrame: sales\n",
    "Revenue and Costs\n",
    "\n",
    "- What is the total order_value_EUR?\n",
    "- What is the total cost?\n",
    "- What is the total gross margin (order_value_EUR - cost)?\n",
    "- What is the average order value?\n",
    "- Which country generates the highest total revenue?\n",
    "- Which country generates the lowest revenue?\n",
    "- How are sales distributed by category?\n",
    "- Which device_type generates the highest revenue?\n",
    "- How many unique orders exist (order_id)?\n",
    "- What is the average order value per country?\n",
    "\n",
    "Commercial Performance\n",
    "\n",
    "- Which sales_manager has the highest total sales?\n",
    "- Which sales_rep has the highest accumulated revenue?\n",
    "- What is the average sales amount per representative?\n",
    "- How are sales distributed across categories?\n",
    "- What is the average margin per category?\n",
    "\n",
    "### DataFrame: extra_variable / extra_data\n",
    "Refund Analysis\n",
    "\n",
    "- What is the total value of refund?\n",
    "- What is the refund rate (refund / total sales)?\n",
    "- Which country has the highest refund amount?\n",
    "- Which category has the highest refund volume?\n",
    "- How many orders have refund > 0?\n",
    "- What is the average refund value?\n",
    "- How are refunds distributed by device_type?\n",
    "- Which sales_rep has the highest refund volume?\n",
    "- What percentage of orders include refunds?\n",
    "- Which country has the lowest refund rate?\n",
    "\n",
    "### DataFrame: fact_sales\n",
    "Operational Metrics\n",
    "\n",
    "- What is the total sales_amount?\n",
    "- What is the total quantity sold (quantity)?\n",
    "- What is the average price?\n",
    "- What is the average sales amount per order?\n",
    "- What is the average quantity per order?\n",
    "- Which product_key has the highest sales volume?\n",
    "- Which customer_key has the highest purchase volume?\n",
    "- How is price distributed?\n",
    "- What is the standard deviation of price?\n",
    "- What is the average revenue per product?\n",
    "\n",
    "### DataFrame: dim_customers\n",
    "Customer Profile\n",
    "\n",
    "- How many customers exist?\n",
    "- What is the distribution of customers by country?\n",
    "- What is the proportion by gender?\n",
    "- What is the proportion by marital status?\n",
    "- Which country has the highest number of customers?\n",
    "- Is there a gender predominance by country?\n",
    "- How many customers exist per country + gender combination?\n",
    "- What is the distribution of customers by marital status?\n",
    "- Which country has the highest customer diversity?\n",
    "- What percentage of customers are married?\n",
    "\n",
    "### DataFrame: dim_products\n",
    "- Product Portfolio\n",
    "\n",
    "- How many products exist?\n",
    "- How many categories exist?\n",
    "- How many subcategories exist?\n",
    "- Which category has the largest number of products?\n",
    "- Which subcategory has the largest number of products?\n",
    "- What is the average cost per category?\n",
    "- Which category has the highest average cost?\n",
    "- Which product_line contains the most products?\n",
    "- What are the minimum and maximum product costs?\n",
    "- How are products distributed by maintenance?\n",
    "\n",
    "### Integrated Descriptive Analysis (Star Schema)\n",
    "- Revenue by Dimension\n",
    "\n",
    "- What is the total revenue per category?\n",
    "- What is the total revenue per subcategory?\n",
    "- Which country generates the highest revenue?\n",
    "- Which gender generates the highest revenue?\n",
    "- Which marital status generates the highest revenue?\n",
    "- Which product generates the highest revenue?\n",
    "- Which customer generates the highest revenue?\n",
    "- What is the average revenue per customer?\n",
    "- What is the average revenue per product?\n",
    "- How is revenue distributed by product_line?\n",
    "\n",
    "### Distribution and Structure Analysis\n",
    "\n",
    "- Which variable has the highest variance?\n",
    "- Which variable shows the highest concentration of values?\n",
    "- Which categories represent the largest percentage of total revenue?\n",
    "- What are the top 10 products by revenue?\n",
    "- What are the top 10 customers by revenue?\n",
    "- Which country represents the highest revenue share?\n",
    "- Which device_type represents the highest sales volume?\n",
    "- What percentage of total sales comes from each category?\n",
    "- What percentage of total sales comes from each sales_manager?\n",
    "- How is quantity distributed across price ranges?\n",
    "\n",
    "### Descriptive Efficiency Metrics\n",
    "\n",
    "- What is the average margin per product?\n",
    "- What is the average margin per category?\n",
    "- Which country has the highest average margin?\n",
    "- Which sales_rep has the highest average margin?\n",
    "- Which device_type has the highest margin?\n",
    "- Which category has the highest refund volume?\n",
    "- What is the average refund per category?\n",
    "- Which product has the highest average cost?\n",
    "- Which customer has the highest average ticket value?\n",
    "- What is the overall distribution of revenue, cost, and margin?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc7f67-86e9-46dc-9dc4-c22ad5656b8c",
   "metadata": {},
   "source": [
    "# Dataframe Describe Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d01502-9a7a-4cc5-8118-decac5f0bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dfs_imputed.items():\n",
    "    print(f\"\\n===== Dataset: {name} =====\")\n",
    "    display(df.describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b29e3-f8f7-4527-89ef-bb3414d0226d",
   "metadata": {},
   "source": [
    "# Dataframes's Objects Columns Summarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d7189-77c4-4ff8-954c-cce2a632903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dfs_imputed.items():\n",
    "    \n",
    "    object_cols = df.select_dtypes(include=['object'])\n",
    "    \n",
    "    if not object_cols.empty:\n",
    "        print(f\"\\n===== Dataset: {name} =====\")\n",
    "        display(object_cols.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59add70-2560-463f-b5e6-b7dedf1e5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the descriptives of order value, cost and refund.\n",
    "\n",
    "numeric_columns = ['order_value_EUR', 'cost', 'refund']\n",
    "\n",
    "# Perform descriptive analysis for the specific numeric variables\n",
    "numeric_summary = dfs_imputed['sales_extra_variables_extra_data'][numeric_columns].describe()\n",
    "numeric_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9ef73-b8e4-4c68-951d-e67026632356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with three subplots for the numeric variables\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot the distribution of 'Age' in the first subplot\n",
    "plt.subplot(131)  # 1 row, 3 columns, first subplot\n",
    "sns.histplot(dfs_imputed['sales_extra_variables_extra_data']['order_value_EUR'], kde=True)\n",
    "plt.title('Order value Distribution')\n",
    "\n",
    "# Plot the distribution of 'Salary' in the second subplot\n",
    "plt.subplot(132)  # 1 row, 3 columns, second subplot\n",
    "sns.histplot(dfs_imputed['sales_extra_variables_extra_data']['cost'], kde=True)\n",
    "plt.title('Cost Distribution')\n",
    "\n",
    "# Plot the distribution of 'Experience' in the third subplot\n",
    "plt.subplot(133)  # 1 row, 3 columns, third subplot\n",
    "sns.histplot(dfs_imputed['sales_extra_variables_extra_data']['refund'], kde=True)\n",
    "plt.title('Refund Distribution')\n",
    "\n",
    "# Adjust layout and show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038fe9d-51ae-43e1-89e7-e2430748a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with three subplots for the boxplots\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot the boxplot for 'Age' in the first subplot\n",
    "plt.subplot(131)  # 1 row, 3 columns, first subplot\n",
    "sns.boxplot(data=dfs_imputed['sales_extra_variables_extra_data']['order_value_EUR'])\n",
    "plt.title('Order value Boxplot')\n",
    "\n",
    "# Plot the boxplot for 'Salary' in the second subplot\n",
    "plt.subplot(132)  # 1 row, 3 columns, second subplot\n",
    "sns.boxplot(data=dfs_imputed['sales_extra_variables_extra_data']['cost'])\n",
    "plt.title('Cost Boxplot')\n",
    "\n",
    "# Plot the boxplot for 'Experience' in the third subplot\n",
    "plt.subplot(133)  # 1 row, 3 columns, third subplot\n",
    "sns.boxplot(data=dfs_imputed['sales_extra_variables_extra_data']['refund'])\n",
    "plt.title('Refund Boxplot')\n",
    "\n",
    "# Adjust layout and show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459fb9e-ea59-45c8-8b4f-72d689edf98b",
   "metadata": {},
   "source": [
    "# Model\n",
    "- regression\n",
    "    - linear\n",
    "    - polynomial\n",
    "    - OLS > use one variabale independent to fiit model > find slope\n",
    "- time series forecasting\n",
    "    - ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8498da-90ed-47c8-bebb-7916d598239f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107b013-4fad-4a86-b75b-eb0032e1e84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
